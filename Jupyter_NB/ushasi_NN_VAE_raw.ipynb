{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "import numpy as np\r\n",
                "import tensorflow as tf\r\n",
                "from tensorflow import keras\r\n",
                "from tensorflow.keras import layers\r\n",
                "from sklearn.model_selection import train_test_split\r\n",
                "from sklearn import preprocessing"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "img_size=7936\r\n",
                "np.random.seed(1234567)\r\n",
                "#loading data and fitting follows:\r\n",
                "X_train=np.loadtxt('training_data/Xtrain_ush_short.csv',delimiter=',')\r\n",
                "Y_train=np.loadtxt('training_data/Ytrain_ush_short.csv',delimiter=',')\r\n",
                "#Xtrain=Xtrain[:2000]\r\n",
                "X_train=np.transpose(X_train)\r\n",
                "print(X_train.shape)\r\n",
                "\r\n",
                "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_train, Y_train, test_size=0.33)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(3447, 7936)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "#defining a sampling function: returns a random sample from a mean and variance\r\n",
                "#that is input to it\r\n",
                "print(Xtrain.shape,Xtest.shape)\r\n",
                "def sampling(mu_log_variance):\r\n",
                "    mu, log_variance = mu_log_variance\r\n",
                "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mu), mean=0.0, stddev=1.0)\r\n",
                "    random_sample = mu + keras.backend.exp(log_variance/2) * epsilon\r\n",
                "    return random_sample"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(2309, 7936, 1) (1138, 7936)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "#model defined here\r\n",
                "inp = keras.layers.Input(shape=(img_size, 1), name=\"encoder_input\")\r\n",
                "#leaky reLU has been added as a separate layer than define as an activation\r\n",
                "x1 = keras.layers.Conv1D(filters=1, kernel_size=5, padding=\"same\", strides=2,name=\"encoder_conv_1\")(inp)\r\n",
                "x1 = keras.layers.BatchNormalization(name=\"encoder_norm_1\")(x1)\r\n",
                "x1 = keras.layers.LeakyReLU(name=\"encoder_leakyrelu_1\")(x1)\r\n",
                "\r\n",
                "x1 = keras.layers.Conv1D(filters=16, kernel_size=5, padding=\"same\", strides=2, name=\"encoder_conv_2\")(x1)\r\n",
                "x1 = keras.layers.BatchNormalization(name=\"encoder_norm_2\")(x1)\r\n",
                "x1 = keras.layers.LeakyReLU(name=\"encoder_leakyrelu_2\")(x1)\r\n",
                "\r\n",
                "x1 = keras.layers.Conv1D(filters=32, kernel_size=5, padding=\"same\", strides=2, name=\"encoder_conv_3\")(x1)\r\n",
                "x1 = keras.layers.BatchNormalization(name=\"encoder_norm_3\")(x1)\r\n",
                "bp_lay_1 = keras.layers.LeakyReLU(name=\"encoder_leakyrelu_3\")(x1)\r\n",
                "#flatten the layers in encoder\r\n",
                "shape_before_flatten = keras.backend.int_shape(bp_lay_1)[1:]\r\n",
                "x2 = keras.layers.Flatten(name=\"flat_1\")(bp_lay_1)\r\n",
                "\r\n",
                "latent_space_dim = 4\r\n",
                "#declare a mean and variance for the distribution\r\n",
                "encoder_mu = keras.layers.Dense(units=latent_space_dim, name=\"encoder_mu\")(x2)\r\n",
                "encoder_log_variance = keras.layers.Dense(units=latent_space_dim, name=\"encoder_log_variance\")(x2)\r\n",
                "encoder_op = keras.layers.Lambda(sampling, name=\"encoder_output\")([encoder_mu, encoder_log_variance])\r\n",
                "\r\n",
                "encoder = keras.models.Model(inp, encoder_op, name=\"encoder_model\")\r\n",
                "encoder.summary()\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Model: \"encoder_model\"\n",
                        "__________________________________________________________________________________________________\n",
                        "Layer (type)                    Output Shape         Param #     Connected to                     \n",
                        "==================================================================================================\n",
                        "encoder_input (InputLayer)      [(None, 7936, 1)]    0                                            \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_conv_1 (Conv1D)         (None, 3968, 1)      6           encoder_input[0][0]              \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_norm_1 (BatchNormalizat (None, 3968, 1)      4           encoder_conv_1[0][0]             \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_leakyrelu_1 (LeakyReLU) (None, 3968, 1)      0           encoder_norm_1[0][0]             \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_conv_2 (Conv1D)         (None, 1984, 16)     96          encoder_leakyrelu_1[0][0]        \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_norm_2 (BatchNormalizat (None, 1984, 16)     64          encoder_conv_2[0][0]             \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_leakyrelu_2 (LeakyReLU) (None, 1984, 16)     0           encoder_norm_2[0][0]             \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_conv_3 (Conv1D)         (None, 992, 32)      2592        encoder_leakyrelu_2[0][0]        \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_norm_3 (BatchNormalizat (None, 992, 32)      128         encoder_conv_3[0][0]             \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_leakyrelu_3 (LeakyReLU) (None, 992, 32)      0           encoder_norm_3[0][0]             \n",
                        "__________________________________________________________________________________________________\n",
                        "flat_1 (Flatten)                (None, 31744)        0           encoder_leakyrelu_3[0][0]        \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_mu (Dense)              (None, 4)            126980      flat_1[0][0]                     \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_log_variance (Dense)    (None, 4)            126980      flat_1[0][0]                     \n",
                        "__________________________________________________________________________________________________\n",
                        "encoder_output (Lambda)         (None, 4)            0           encoder_mu[0][0]                 \n",
                        "                                                                 encoder_log_variance[0][0]       \n",
                        "==================================================================================================\n",
                        "Total params: 256,850\n",
                        "Trainable params: 256,752\n",
                        "Non-trainable params: 98\n",
                        "__________________________________________________________________________________________________\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "#decoder_input = keras.layers.Input(shape=(shape_before_flatten), name=\"decoder_input\")\r\n",
                "decoder_input = keras.layers.Input(shape=(latent_space_dim), name=\"decoder_input\")\r\n",
                "x3 = keras.layers.Dense(units=np.prod(shape_before_flatten), name=\"decoder_dense_1\")(decoder_input)\r\n",
                "x3 = keras.layers.Reshape(target_shape=shape_before_flatten)(x3)\r\n",
                "\r\n",
                "x3 = keras.layers.Conv1DTranspose(filters=32, kernel_size=5, padding=\"same\", strides=2, name=\"decoder_conv_tran_1\")(x3)\r\n",
                "x3 = keras.layers.BatchNormalization(name=\"decoder_norm_1\")(x3)\r\n",
                "x3 = keras.layers.LeakyReLU(name=\"decoder_leakyrelu_1\")(x3)\r\n",
                "\r\n",
                "x3 = keras.layers.Conv1DTranspose(filters=16, kernel_size=5, padding=\"same\", strides=2, name=\"decoder_conv_tran_2\")(x3)\r\n",
                "x3 = keras.layers.BatchNormalization(name=\"decoder_norm_2\")(x3)\r\n",
                "x3 = keras.layers.LeakyReLU(name=\"decoder_leakyrelu_2\")(x3)\r\n",
                "\r\n",
                "decoder_output = keras.layers.Conv1DTranspose(filters=1, kernel_size=5, padding=\"same\", strides=2,activation='sigmoid', name=\"decoder_conv_tran_4\")(x3)\r\n",
                "#decoder_output = keras.layers.LeakyReLU(name=\"decoder_output\")(x3)\r\n",
                "\r\n",
                "decoder = keras.models.Model(decoder_input, decoder_output, name=\"decoder_model\")\r\n",
                "decoder.summary()\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Model: \"decoder_model\"\n",
                        "_________________________________________________________________\n",
                        "Layer (type)                 Output Shape              Param #   \n",
                        "=================================================================\n",
                        "decoder_input (InputLayer)   [(None, 4)]               0         \n",
                        "_________________________________________________________________\n",
                        "decoder_dense_1 (Dense)      (None, 31744)             158720    \n",
                        "_________________________________________________________________\n",
                        "reshape_6 (Reshape)          (None, 992, 32)           0         \n",
                        "_________________________________________________________________\n",
                        "decoder_conv_tran_1 (Conv1DT (None, 1984, 32)          5152      \n",
                        "_________________________________________________________________\n",
                        "decoder_norm_1 (BatchNormali (None, 1984, 32)          128       \n",
                        "_________________________________________________________________\n",
                        "decoder_leakyrelu_1 (LeakyRe (None, 1984, 32)          0         \n",
                        "_________________________________________________________________\n",
                        "decoder_conv_tran_2 (Conv1DT (None, 3968, 16)          2576      \n",
                        "_________________________________________________________________\n",
                        "decoder_norm_2 (BatchNormali (None, 3968, 16)          64        \n",
                        "_________________________________________________________________\n",
                        "decoder_leakyrelu_2 (LeakyRe (None, 3968, 16)          0         \n",
                        "_________________________________________________________________\n",
                        "decoder_conv_tran_4 (Conv1DT (None, 7936, 1)           81        \n",
                        "=================================================================\n",
                        "Total params: 166,721\n",
                        "Trainable params: 166,625\n",
                        "Non-trainable params: 96\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "from keras import backend as K\r\n",
                "def loss_func(encoder_mu, encoder_log_variance):\r\n",
                "    def vae_reconstruction_loss(y_true, y_predict):\r\n",
                "        reconstruction_loss_factor = 1000\r\n",
                "        reconstruction_loss = keras.backend.mean(keras.backend.square(y_true-y_predict), axis=[1, 2])\r\n",
                "        return reconstruction_loss_factor * reconstruction_loss\r\n",
                "\r\n",
                "    def vae_kl_loss(encoder_mu, encoder_log_variance):\r\n",
                "        kl_loss = -0.5 * keras.backend.sum(1.0 + encoder_log_variance - keras.backend.square(encoder_mu) - keras.backend.exp(encoder_log_variance), axis=1)\r\n",
                "        return kl_loss\r\n",
                "\r\n",
                "    def vae_kl_loss_metric(y_true, y_predict):\r\n",
                "        kl_loss = -0.5 * keras.backend.sum(1.0 + encoder_log_variance - keras.backend.square(encoder_mu) - keras.backend.exp(encoder_log_variance), axis=1)\r\n",
                "        return kl_loss\r\n",
                "\r\n",
                "    def vae_loss(y_true, y_predict):\r\n",
                "        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)\r\n",
                "        kl_loss = vae_kl_loss(y_true, y_predict)\r\n",
                "\r\n",
                "        loss = reconstruction_loss + kl_loss\r\n",
                "        return loss\r\n",
                "\r\n",
                "    return vae_loss\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "#model for the vae\r\n",
                "vae_input = keras.layers.Input(shape=(img_size,1), name=\"VAE_input\")\r\n",
                "vae_encoder_output = encoder(vae_input)\r\n",
                "vae_decoder_output = decoder(vae_encoder_output)\r\n",
                "vae = keras.models.Model(vae_input, vae_decoder_output, name=\"VAE\")\r\n",
                "\r\n",
                "vae.summary()\r\n",
                "#vae.compile(optimizer=keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\r\n",
                "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=loss_func(encoder_mu, encoder_log_variance))\r\n",
                "\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Model: \"VAE\"\n",
                        "_________________________________________________________________\n",
                        "Layer (type)                 Output Shape              Param #   \n",
                        "=================================================================\n",
                        "VAE_input (InputLayer)       [(None, 7936, 1)]         0         \n",
                        "_________________________________________________________________\n",
                        "encoder_model (Functional)   (None, 4)                 256850    \n",
                        "_________________________________________________________________\n",
                        "decoder_model (Functional)   (None, 7936, 1)           166721    \n",
                        "=================================================================\n",
                        "Total params: 423,571\n",
                        "Trainable params: 423,377\n",
                        "Non-trainable params: 194\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "#Xtrain_N=[]\r\n",
                "#[Xtrain_N.append(Xtrain[i]/np.abs(Xtrain[i][np.argmin(Xtrain[i])])) for i in range(0,3231)]\r\n",
                "#Xtrain=np.array(Xtrain_N).reshape(3231,12000,1)\r\n",
                "#arr=np.arange(0,4500,1)\r\n",
                "#np.random.shuffle(arr)\r\n",
                "#Xtrain=np.array([Xtrain[i] for i in arr])\r\n",
                "#Xtrain=np.array(Xtrain).reshape(4500,2000,1)\r\n",
                "#Xtrain=Xtrain[0:4500]\r\n",
                "#print(Xtrain.shape)\r\n",
                "Xtrain=[preprocessing.normalize(Xtrain[i]) for i in range(0,len(Xtrain))]\r\n",
                "Xtrain=np.array(Xtrain).reshape(2309, 7936, 1)\r\n",
                "print(Xtrain.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(2309, 7936, 1)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "\r\n",
                "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\r\n",
                "history=vae.fit(Xtrain, Xtrain, epochs=30, batch_size=64 ,verbose=1, validation_split=0.2)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 1/30\n",
                        "29/29 [==============================] - 56s 2s/step - loss: 5759.8721 - val_loss: 5731.1870\n",
                        "Epoch 2/30\n",
                        "29/29 [==============================] - 52s 2s/step - loss: 5649.2168 - val_loss: 5650.8779\n",
                        "Epoch 3/30\n",
                        "29/29 [==============================] - 52s 2s/step - loss: 5566.2344 - val_loss: 5564.6528\n",
                        "Epoch 4/30\n",
                        "29/29 [==============================] - 50s 2s/step - loss: 5486.0527 - val_loss: 5480.1572\n",
                        "Epoch 5/30\n",
                        "29/29 [==============================] - 51s 2s/step - loss: 5416.5684 - val_loss: 5405.3818\n",
                        "Epoch 6/30\n",
                        "29/29 [==============================] - 51s 2s/step - loss: 5340.0444 - val_loss: 5332.9409\n",
                        "Epoch 7/30\n",
                        "29/29 [==============================] - 51s 2s/step - loss: 5253.9062 - val_loss: 5257.4712\n",
                        "Epoch 8/30\n",
                        "29/29 [==============================] - 49s 2s/step - loss: 5183.1685 - val_loss: 5183.1533\n",
                        "Epoch 9/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 5134.2026 - val_loss: 5126.6172\n",
                        "Epoch 10/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 5093.4199 - val_loss: 5085.2710\n",
                        "Epoch 11/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 5057.5352 - val_loss: 5049.2871\n",
                        "Epoch 12/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 5036.2598 - val_loss: 5024.2866\n",
                        "Epoch 13/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 5019.5195 - val_loss: 5009.2300\n",
                        "Epoch 14/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 5009.4561 - val_loss: 4999.7842\n",
                        "Epoch 15/30\n",
                        "29/29 [==============================] - 44s 2s/step - loss: 5001.7622 - val_loss: 4992.9688\n",
                        "Epoch 16/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4995.6538 - val_loss: 4987.9360\n",
                        "Epoch 17/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4991.5527 - val_loss: 4984.4907\n",
                        "Epoch 18/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4988.3252 - val_loss: 4982.4834\n",
                        "Epoch 19/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 4985.4424 - val_loss: 4980.3120\n",
                        "Epoch 20/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4983.1812 - val_loss: 4978.8081\n",
                        "Epoch 21/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 4981.5649 - val_loss: 4977.5854\n",
                        "Epoch 22/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 4980.7568 - val_loss: 4976.7129\n",
                        "Epoch 23/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 4979.0859 - val_loss: 4975.8535\n",
                        "Epoch 24/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4978.0078 - val_loss: 4975.1050\n",
                        "Epoch 25/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4977.0962 - val_loss: 4974.3252\n",
                        "Epoch 26/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4975.9673 - val_loss: 4973.5762\n",
                        "Epoch 27/30\n",
                        "29/29 [==============================] - 42s 1s/step - loss: 4975.1606 - val_loss: 4973.0381\n",
                        "Epoch 28/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 4974.5781 - val_loss: 4972.5859\n",
                        "Epoch 29/30\n",
                        "29/29 [==============================] - 44s 2s/step - loss: 4974.0430 - val_loss: 4972.2056\n",
                        "Epoch 30/30\n",
                        "29/29 [==============================] - 43s 1s/step - loss: 4973.6274 - val_loss: 4971.8979\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "vae.save_weights('autoencoder_raw_7936.h5')\r\n",
                "encoder.save_weights('encoding_raw_7936.h5')\r\n",
                "decoder.save_weights('decoding_simpler_raw_7936.h5')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to improve the NN\n",
    "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from keras import backend as K\n",
    "import GetLightcurves as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _parse_tfr_element(element):\n",
    "  desc = {\n",
    "        'input':tf.io.FixedLenFeature([], tf.string),\n",
    "        'map':tf.io.FixedLenFeature([], tf.string),\n",
    "        'counts': tf.io.FixedLenFeature([], tf.string),\n",
    "        'id': tf.io.FixedLenFeature([], tf.string),  \n",
    "    }\n",
    "  example_message = tf.io.parse_single_example(element, desc)\n",
    "\n",
    "  #return(example_message['counts'])\n",
    "  binp = example_message['input'] # get byte \n",
    "  bmap = example_message['map'] # get byte string\n",
    "  bcts = example_message['counts'] # get byte string\n",
    "  bid = example_message['id'] # get byte string\n",
    "  print(binp.shape,bmap.shape,bcts.shape)\n",
    "  inp = tf.io.parse_tensor(binp, out_type=tf.float32) # restore 2D array from byte string\n",
    "  map = tf.io.parse_tensor(bmap, out_type=tf.bool)\n",
    "  cts = tf.io.parse_tensor(bcts, out_type=tf.int8)\n",
    "  id = tf.io.parse_tensor(bid, out_type=tf.string)\n",
    "  return (inp,map,cts,id)\n",
    "\n",
    "\n",
    "tfr_dataset = tf.data.TFRecordDataset(['../../training_data/seg_mask_training_av_bal']) \n",
    "tfr_testdata = tf.data.TFRecordDataset(['../../training_data/seg_mask_test_av_bal']) \n",
    "#for serialized_instance in tfr_dataset:\n",
    "#  print(serialized_instance) # print serialized example messages\n",
    "#zprint(len(tfr_dataset))\n",
    "dataset = tfr_dataset.map(_parse_tfr_element)\n",
    "testdata = tfr_testdata.map(_parse_tfr_element)\n",
    "\n",
    "Xtrain=[]\n",
    "Ytrain=[]\n",
    "\n",
    "for instance in dataset:\n",
    "  Xtrain.append(instance[0])\n",
    "  Ytrain.append(instance[1])\n",
    "\n",
    "Xtest=[]\n",
    "Ytest=[]\n",
    "TestID=[]\n",
    "\n",
    "for instance in testdata:\n",
    "  Xtest.append(instance[0])\n",
    "  Ytest.append(instance[1])\n",
    "  TestID.append(instance[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain=np.asarray(Ytrain).reshape(len(Xtrain),4000,3)\n",
    "Ytest=np.asarray(Ytest).reshape(len(Xtest),4000,3)\n",
    "print(Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtrain=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
    "Xtest=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = np.asarray(Ytrain, dtype = 'float32')\n",
    "Ytest = np.asarray(Ytest, dtype = 'float32')\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "fig,ax=plt.subplots(10,2,figsize=(10,20))\n",
    "for i in range(0,10):\n",
    "    ax[i][0].plot(Xtrain[vararr[i]])\n",
    "    #ax[i][1].plot(Ytrain[vararr[i],:,2])\n",
    "    ax[i][1].plot(Ytrain[vararr[i],:,0])\n",
    "    ax[i][1].plot(Ytrain[vararr[i],:,1])\n",
    "    #ax[i][1].set_xlim(2600,3000)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def log_cosh_dice_loss(y_true, y_pred):\n",
    "        x = generalized_dice_coeff(y_true, y_pred)\n",
    "        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)*0.5 + log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    #w = 1 - w\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def loss_wrap(alpha=1,gamma=2,comb=1):\n",
    "    def focal_dice_loss(y_true, y_pred):\n",
    "        loss = FocalLoss(y_true, y_pred, alpha, gamma) + comb*log_cosh_dice_loss(y_true, y_pred)\n",
    "        return loss\n",
    "    return focal_dice_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    hp_f = hp.Int('units1', min_value=8, max_value=32, step=8)\n",
    "    hp_bottle = hp.Int('units4', min_value=256, max_value=1024, step=256)\n",
    "    hp_alp = hp.Float('alp', min_value=0.4, max_value=0.8, step=0.2)\n",
    "    hp_gamma = hp.Choice('gamma', values=[1, 2, 3, 4])\n",
    "    hp_comb = hp.Choice('lincomb', values=[1.0, 1.5, 2.0, 2.5])\n",
    "\n",
    "    IMG_SIZE=2000\n",
    "\n",
    "    conv_ip = keras.layers.Input(shape=(IMG_SIZE,),name='Input')\n",
    "    xi=keras.layers.Reshape((IMG_SIZE, 1), input_shape=(IMG_SIZE,),name='reshape_1')(conv_ip)\n",
    "    xi=keras.layers.BatchNormalization()(xi)\n",
    "\n",
    "    x1=keras.layers.SeparableConv1D(hp_f,kernel_size=3,strides=1,activation='tanh',name='1st16_5')(xi)  #3998, 32\n",
    "    c1=keras.layers.SeparableConv1D(hp_f,kernel_size=3,strides=1,activation='tanh',name='2nd16_3')(x1)  #3996, 32\n",
    "\n",
    "    x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "    x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1')(x2)  #1998, 64\n",
    "    x2=keras.layers.SeparableConv1D(hp_f*2,kernel_size=3,strides=1,activation='tanh',name='1st32_5')(x2) #1996, 64\n",
    "    c2=keras.layers.SeparableConv1D(hp_f*2,kernel_size=5,strides=1,activation='tanh',name='2nd32_3')(x2) #1992, 64\n",
    "\n",
    "    x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "    x3=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_2')(x3)  #996, 64\n",
    "    x3=keras.layers.SeparableConv1D(hp_f*4,kernel_size=3,strides=1,activation='tanh',name='1st64_5')(x3) #994, 128\n",
    "    c3=keras.layers.SeparableConv1D(hp_f*4,kernel_size=3,strides=1,activation='tanh',name='2nd64_3')(x3) #992, 128\n",
    "\n",
    "    x4=keras.layers.BatchNormalization(name='bn3')(c3)\n",
    "    x4=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_3')(x4)  #496, 128\n",
    "    x4=keras.layers.SeparableConv1D(hp_f*8,kernel_size=3,strides=1,activation='tanh',name='1st128_5')(x4)  #494, 256\n",
    "    c4=keras.layers.SeparableConv1D(hp_f*8,kernel_size=3,strides=1,activation='tanh',name='2nd128_5')(x4) #492, 256\n",
    "\n",
    "    x5=keras.layers.BatchNormalization(name='bn4')(c4) \n",
    "    x5=keras.layers.MaxPool1D(2,strides=2,data_format='channels_last',name='maxpool_4')(x5)  #246, 256  \n",
    "    x5=keras.layers.SeparableConv1D(hp_bottle,kernel_size=3,strides=1,activation='tanh',name='1st512_5')(x5)  #244, 256\n",
    "    x5=keras.layers.SeparableConv1D(hp_bottle,kernel_size=3,strides=1,activation='tanh',name='2nd512_5')(x5)  #242, 256\n",
    "\n",
    "    x5 = keras.layers.Conv1DTranspose(hp_f*8, kernel_size=4, activation='relu', strides=2, name=\"T1st128_5\")(x5)  #486, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(hp_f*8, kernel_size=5, activation='relu', strides=1, name=\"T2nd128_5\")(x5)  #490, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(hp_f*8, kernel_size=3, activation='relu', strides=1, name=\"T3rd128_5\")(x5)  #492, 256\n",
    "    x5 =keras.layers.BatchNormalization(name='bn5')(x5) \n",
    "\n",
    "    x6 = keras.layers.Concatenate(axis=2, name='cn1')([c4,x5])  #492, 512\n",
    "    x6 = keras.layers.Conv1DTranspose(hp_f*8,kernel_size=3,strides=1,activation='relu',name='3rd128_3')(x6) #494, 256\n",
    "    x6 = keras.layers.Conv1DTranspose(hp_f*4, kernel_size=4, activation='relu', strides=2, name=\"T1st64_3\")(x6) #990, 128\n",
    "    x6 = keras.layers.Conv1DTranspose(hp_f*4, kernel_size=3, activation='relu', strides=1, name=\"T2nd64_3\")(x6) #992, 128\n",
    "    x6 = keras.layers.BatchNormalization(name='bn6')(x6)  \n",
    "\n",
    "    x7 = keras.layers.Concatenate(axis=2, name='cn2')([c3,x6]) #992, 256\n",
    "    x7 = keras.layers.Conv1DTranspose(hp_f*4,kernel_size=3,strides=1,activation='relu',name='3rd64_3')(x7) #994, 128\n",
    "    x7 = keras.layers.Conv1DTranspose(hp_f*2, kernel_size=4, activation='relu', strides=2, name=\"T1st32_3\")(x7) #1990, 64\n",
    "    x7 = keras.layers.Conv1DTranspose(hp_f*2, kernel_size=3, activation='relu', strides=1, name=\"T2nd32_3\")(x7) #1992, 64\n",
    "    x7 = keras.layers.BatchNormalization(name='bn7')(x7)  \n",
    "\n",
    "    x8 = keras.layers.Concatenate(axis=2, name='cn3')([c2,x7])  #1992, 128\n",
    "    x8 = keras.layers.Conv1DTranspose(hp_f*2,kernel_size=3,strides=1,activation='relu',name='3rd32_3')(x8)  #1994, 64\n",
    "    x8 = keras.layers.Conv1DTranspose(hp_f,kernel_size=4,strides=2,activation='relu',name='T1st16_3')(x8) #3990, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(hp_f,kernel_size=5,strides=1,activation='relu',name='T2nd16_3')(x8) #3994, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(hp_f,kernel_size=3,strides=1,activation='relu',name='T4rth16_3')(x8) #3996, 32\n",
    "    x8 = keras.layers.BatchNormalization(name='bn8')(x8)  \n",
    "\n",
    "    x9 = keras.layers.Concatenate(axis=2, name='cn4')([c1,x8])  #3996, 64\n",
    "    x9 = keras.layers.Conv1DTranspose(hp_f,kernel_size=3,strides=1,activation='relu',name='3rd16_3')(x9) #3998, 32\n",
    "    x9 = keras.layers.Conv1DTranspose(hp_f,kernel_size=3,strides=1,activation='relu',name='T3rd16_3')(x9) #4000, 32\n",
    "    x9 = keras.layers.BatchNormalization(name='bn9')(x9)\n",
    "\n",
    "    conv_op = keras.layers.Conv1D(3,kernel_size=3,strides=1,name='semiop',padding='same',activation='softmax')(x9) # (4000, 3)\n",
    "\n",
    "    #keras.backend.clear_session()\n",
    "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss=loss_wrap(alpha=hp_alp,gamma=hp_gamma,comb=hp_comb), metrics=[tf.keras.metrics.MeanIoU(num_classes=3)])\n",
    "    return convNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=10,\n",
    "    directory='tune4800',\n",
    "    project_name='keras_tuner_demo'\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "tuner.search(np.asarray(Xtrain[:,1000:3000]), np.asarray(Ytrain[:,1000:3000,:]), epochs=10, batch_size=16,validation_split=0.2,callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=1)\n",
    "bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(bestHP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

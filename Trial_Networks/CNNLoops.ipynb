{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to improve the NN\n",
    "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=np.loadtxt('training_data/Xtrain_av_raw.csv',delimiter=',')\n",
    "temp2=np.loadtxt('training_data/Xtrain_av_raw_v2.csv',delimiter=',')\n",
    "temp3=np.loadtxt('training_data/Xtrain_av_raw_v3.csv',delimiter=',')\n",
    "temp4=np.loadtxt('training_data/Xtrain_av_raw_v4.csv',delimiter=',')\n",
    "print(temp1.shape)\n",
    "print(temp2.shape)\n",
    "print(temp3.shape)\n",
    "print(temp4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_func=[]\n",
    "val_acc_func=[]\n",
    "test_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main code:\n",
    "#Q1. ARE MORE LAYERS BETTER? We use 2 classifier Training sample to test this\n",
    "def are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,img_size,no_l,epoch):\n",
    "    #add hidden layers\n",
    "    conv_ip = keras.layers.Input(shape=(img_size,),name='Input')\n",
    "    x=keras.layers.Reshape((img_size, 1), input_shape=(img_size,),name='reshape_1')(conv_ip)\n",
    "    x=keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    for i in range(0,no_l):\n",
    "        x=keras.layers.Conv1D(16,kernel_size=3,strides=1,activation='tanh',name='conv16_3_'+str(i))(x)\n",
    "    x=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',name='maxpool_1')(x)\n",
    "    for i in range(0,no_l):\n",
    "        x=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',name='conv32_3_'+str(i))(x)\n",
    "    x=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',name='maxpool_2')(x)\n",
    "    x=keras.layers.Flatten(name='flat_1')(x)\n",
    "    x3=keras.layers.Dense(128,name='dense_layer_1',activation='relu')(x)\n",
    "    x3=keras.layers.Dense(128,name='dense_layer_3',activation='relu')(x3)\n",
    "    conv_op=keras.layers.Dense(2,name='dense_layer_4',activation='softmax')(x3)\n",
    "\n",
    "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    history=convNN.fit(np.array(Xtrain),np.array(Ytrain), batch_size=64, epochs=epoch , verbose=1 , validation_split=0.2,callbacks=[es_callback])\n",
    "\n",
    "    fig,ax=plt.subplots(1,1,figsize=(5,5))\n",
    "    ax.plot(history.history['accuracy'])\n",
    "    ax.plot(history.history['val_accuracy'])\n",
    "    ax.set_title('model Accuracy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'test'], loc='upper left')\n",
    "    #plt.savefig('loop_acc_100_3d0'+str(no_l)+'_'+str(img_size)+'.png')\n",
    "\n",
    "    test_loss, test_acc = convNN.evaluate(np.array(Xtest), np.array(Ytest))\n",
    "    print('Test accuracy:', test_acc)\n",
    "    test_accuracy.append(test_acc)\n",
    "    acc_func.append(history.history['accuracy'])\n",
    "    val_acc_func.append(history.history['val_accuracy'])\n",
    "    #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.loadtxt('training_data/Xtrain_av_raw200_2_2d5_v2.csv',delimiter=',')\n",
    "Y_train=np.loadtxt('training_data/Ytrain_av_raw200_2_2d5_v2.csv',delimiter=',')\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "print(\"shapes:\",Xtrain.shape,Ytrain.shape,Xtest.shape,Ytest.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,1,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,2,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,3,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,4,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,5,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,6,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,7,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,8,60)\n",
    "are_more_layers_better(Xtrain,Ytrain,Xtest,Ytest,200,9,60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_accuracy)\n",
    "#print(acc_func)\n",
    "#print(val_acc_func)\n",
    "new_mean=[el[-10:] for el in val_acc_func]\n",
    "print(new_mean)\n",
    "new_mean=[np.mean(el[-10:]) for el in val_acc_func]\n",
    "fig2,ax2=plt.subplots(1,1,figsize=(5,5))\n",
    "ax2.plot(np.arange(1,len(test_accuracy)+1,1),new_mean)\n",
    "ax2.set_xlabel('number of layers')\n",
    "ax2.set_ylabel('test accuracy')\n",
    "plt.savefig('test_acc_progression_2d5_200.png')\n",
    "\n",
    "fig3,ax3=plt.subplots(1,1,figsize=(7,7))\n",
    "for i in range(0,len(val_acc_func)):\n",
    "    ax3.plot(acc_func[i],label=\"train_lay\"+str(i+1))\n",
    "    ax3.plot(val_acc_func[i],label=\"test_lay\"+str(i+1))\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('epoch') \n",
    "ax3.set_ylabel('accuracy')\n",
    "\n",
    "#np.savetxt('comprehensive_cnn_acc.csv',acc_func,delimiter=',')\n",
    "plt.savefig('lc_cumulative_2d5_200.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "#from astropy.io import fits,ascii\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import GetLightcurves as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 0.00001\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection +smooth) / (tf.reduce_sum(y_true_f) +  tf.reduce_sum(y_pred_f) +smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def weightFocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA): \n",
    "    w = tf.reduce_sum(targets,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 - w\n",
    "    #w = 1 / (w  + 0.00001)\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)   \n",
    "    \n",
    "    #inputs = K.flatten(inputs)\n",
    "    #targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(w*K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def log_cosh_dice_loss(y_true, y_pred):\n",
    "        x = generalized_dice_coeff(y_true, y_pred)\n",
    "        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)*0.5 + log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    #w = 1 / (w  + 0.00001)\n",
    "    w = 1 - w\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred):\n",
    "    loss = FocalLoss(y_true, y_pred) + 1.5*dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "#print(weightFocalLoss(Ytest, np.ones((len(Ytest),4000,3),dtype='float32')))\n",
    "#check score\n",
    "def corr(y_true, y_pred):\n",
    "    #wholesome thing.... lets see if this is any better\n",
    "    corrects = 0 \n",
    "    incorrects = 0\n",
    "    cm=[[0,0],[0,0]]\n",
    "    pred_arr=[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        tmp=[0,0]\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        #newpl=np.asarray([1 if(el>np.mean(newpl)+np.std(newpl)) else 0 for el in newpl])\n",
    "        #newfps=np.asarray([1 if(el>np.mean(newfps)+np.std(newfps)) else 0 for el in newfps])\n",
    "        if(np.any(y_true[i,:,0]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,0])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,0])\n",
    "            if(val2[0,1]>val1[0,1]): \n",
    "                corrects+=1\n",
    "                cm[0][0]+=1\n",
    "                tmp[0]=1\n",
    "            else: \n",
    "                incorrects+=1\n",
    "                cm[1][0]+=1\n",
    "                tmp[1]=1\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        #fps detect\n",
    "        if(np.any(y_true[i,:,1]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,1])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,1])\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                corrects+=1\n",
    "                cm[1][1]+=1\n",
    "                tmp[1]=1\n",
    "            else: \n",
    "                cm[0][1]+=1\n",
    "                incorrects+=1\n",
    "                tmp[0]=1\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        pred_arr.append(tmp)\n",
    "        #planet detection:\n",
    "    \n",
    "    print(corrects, incorrects)\n",
    "    print(np.asarray(cm)/np.asarray(cm).sum())\n",
    "    print(corrects/(corrects+incorrects))\n",
    "\n",
    "def corrarr(y_true, y_pred):\n",
    "    checkarr=[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        if(np.any(y_true[i,:,0]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,0])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,0])\n",
    "            if(val2[0,1]>val1[0,1]):\n",
    "                checkarr.append(0)\n",
    "            else: \n",
    "                checkarr.append(1)\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        #fps detect\n",
    "        elif(np.any(y_true[i,:,1]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,1])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,1])\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                checkarr.append(0)\n",
    "            else: \n",
    "                checkarr.append(1)\n",
    "    return(checkarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anomaly(arr, frac):\n",
    "    arr = np.asarray(arr)\n",
    "    l = int(frac*np.asarray(arr>0).sum())\n",
    "    print(l)\n",
    "    rarr = np.random.uniform(1.1, 1.8, l)\n",
    "    place = np.where(arr>0)[0]\n",
    "    np.random.shuffle(place)\n",
    "    place = place[:l]\n",
    "    narr = []\n",
    "    x=0\n",
    "    for i in range(len(arr)):\n",
    "        if(np.any(place==i)): \n",
    "            narr.append(arr[i]/rarr[x])\n",
    "            x+=1\n",
    "        else: narr.append(arr[i])\n",
    "\n",
    "    return(np.asarray(narr))\n",
    "\n",
    "def get_chunks_from_record(filename, binip, binop):\n",
    "    dfn,dfy,dfc = gc.read_tfr_record(filename,\n",
    "            ['input','mask','counts'],['ar','ar','ar'], \n",
    "            [tf.float32, tf.bool, tf.int8])\n",
    "    dfy = np.reshape(np.asarray(dfy),(len(dfn),4000,3))\n",
    "    df = np.concatenate(np.asarray(dfy[:,:,0]), axis=0)\n",
    "    df = generate_anomaly(df, 0.02)\n",
    "    #df = df-np.median(df)\n",
    "    #df = df/np.abs(min(df))\n",
    "\n",
    "    xarr=np.asarray([df[i:i+binip] for i in range(0,len(df)-binip, binop)])\n",
    "    yarr=np.asarray([df[i+binip:i+binip+binop] for i in range(0,len(df)-binip, binop)])\n",
    "    print(\"shapes:\",xarr.shape, yarr.shape)\n",
    "    return (xarr, yarr)\n",
    "\n",
    "def predictor(imsz, opsize):\n",
    "    conv_ip = keras.layers.Input(shape=(imsz,),name='Input')\n",
    "    xi=keras.layers.Reshape((imsz, 1), input_shape=(imsz,),name='reshape_1')(conv_ip)\n",
    "    xi=keras.layers.BatchNormalization()(xi)\n",
    "    xi=keras.layers.GRU(32, return_sequences=True,name=\"GRU_lay_2\",activation='tanh')(xi)\n",
    "    #xi=keras.layers.Conv1D(32, kernel_size=5,name=\"GRU_lay_2\",activation='tanh')(xi)\n",
    "    xi=keras.layers.MaxPool1D(2,strides=2,data_format='channels_last',name='maxpool_1')(xi)  \n",
    "    xi=keras.layers.GRU(64, return_sequences=True,name=\"GRU_lay_3\",activation='tanh')(xi) \n",
    "    #xi=keras.layers.Conv1D(64, kernel_size=5,name=\"GRU_lay_3\",activation='tanh')(xi)\n",
    "    xi=keras.layers.MaxPool1D(2,strides=2,data_format='channels_last',name='maxpool_2')(xi)\n",
    "    #xi=keras.layers.GRU(64, return_sequences=True,name=\"GRU_lay_4\",activation='tanh')(xi) \n",
    "    xi=keras.layers.Flatten()(xi)\n",
    "    xi=keras.layers.Dense(64,activation='relu')(xi)\n",
    "    conv_op=keras.layers.Dense(opsize,activation='relu')(xi)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
    "    return(convNN)\n",
    "\n",
    "def train_model(model, xarr, yarr, start=0, count=300, random=False):\n",
    "    if(random):\n",
    "        shar=np.arange(0,len(xarr))\n",
    "        np.random.shuffle(shar)\n",
    "        xarrf=np.asarray([xarr[i] for i in shar])\n",
    "        yarrf=np.asarray([yarr[i] for i in shar])\n",
    "    else: \n",
    "        xarrf=xarr\n",
    "        yarrf=yarr\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    history=model.fit(xarrf[start:start+count], yarrf[start:start+count], batch_size=8, epochs=50, verbose=1,validation_split=0.2,\n",
    "        callbacks=[stop_early])\n",
    "    maxms = model.predict(xarrf[start:start+count])\n",
    "    mserr = max(((maxms - yarrf[start:start+count])**2).reshape(-1))\n",
    "    print(mserr)\n",
    "    return(model, history, mserr)\n",
    "\n",
    "def test_model(model, xarr, yarr):\n",
    "    PrMP=model.predict(xarr)\n",
    "    PrMP=np.concatenate(PrMP)\n",
    "    PrMP=PrMP.reshape(-1)\n",
    "    print(PrMP.shape)\n",
    "    return(PrMP, np.concatenate(yarr))\n",
    "    \n",
    "def get_anomalies(predtot, truetot, thres=2):\n",
    "    difftmp = np.abs(predtot - truetot)\n",
    "    difftmp = difftmp\n",
    "    #th = max(difftmp[:20000])\n",
    "    anarrtmp=difftmp>np.mean(difftmp)+thres*np.std(difftmp)\n",
    "    #anarrtmp=difftmp>thres\n",
    "    print(anarrtmp.sum())\n",
    "    return(anarrtmp)\n",
    "\n",
    "def plot_anomalies(arr, ind, el, path):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "    count=np.where(np.asarray(ind)>0)[0]\n",
    "    ax.plot(arr, marker='.', ls='None', label='lc')\n",
    "    ax.plot(count, arr[count], marker='.', ls='None', label='weird')\n",
    "    ax.legend()\n",
    "    ax.set_title(el)\n",
    "    ax.set_xlabel('pixel')\n",
    "    ax.set_ylabel('flux')\n",
    "    plt.savefig(path+el+'_s.png')\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholistic code:\n",
    "INPDIR='../../processed_directories/full_lc_planets/'\n",
    "OPDIR='../../processed_directories/anomalies_set4/'\n",
    "entries = os.listdir(INPDIR)\n",
    "totx=[]\n",
    "toty=[]\n",
    "totid=[]\n",
    "totloss=[]\n",
    "totvalloss=[]\n",
    "totpredy=[]\n",
    "totanom=[]\n",
    "for el in entries[100:130]:\n",
    "    x,y = get_chunks_from_record(INPDIR+el,2000,50)\n",
    "    Mdl = predictor(2000,50)\n",
    "    Mdl, history, mserr = train_model(Mdl,x,y,0,600,True)\n",
    "    prdy, ty = test_model(Mdl, x, y)\n",
    "    anarr = get_anomalies(prdy, ty, mserr/2)\n",
    "    print(len(prdy), len(ty))\n",
    "\n",
    "    plot_anomalies(ty, anarr, el, OPDIR)\n",
    "\n",
    "    totx.append(x)\n",
    "    toty.append(ty)\n",
    "    totid.append(el)\n",
    "    totloss.append(history.history['loss'])\n",
    "    totvalloss.append(history.history['val_loss'])\n",
    "    totpredy.append(prdy)\n",
    "    totanom.append(anarr)\n",
    "\n",
    "net = np.asarray([[totx[i], toty[i], totid[i], totloss[i], totvalloss[i], totpredy[i], totanom[i]] for i in range(len(totx))], dtype='object')\n",
    "gc.write_tfr_record(OPDIR+'set4_v3_2',net,['input','true_op','id','loss','val_loss','pred_op','anomaly'],['ar','ar','b','ar','ar','ar','ar'],\n",
    "    ['float32','float32','string','float32','float32','float32','bool'])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the ensemble\n",
    "INPDIR='../../processed_directories/full_lc_planets/'\n",
    "OPDIR='../../processed_directories/anomalies_set2/'\n",
    "tr_opx, anomx = gc.read_tfr_record(OPDIR+'set2',['true_op','anomaly'],['ar','ar'],['float32','bool'])\n",
    "tr_opy, anomy = gc.read_tfr_record(OPDIR+'set3',['true_op','anomaly'],['ar','ar'],['float32','bool'])\n",
    "tr_op1 = np.concatenate(np.asarray([tr_opx, tr_opy]), axis=0)\n",
    "anom1 = np.concatenate(np.asarray([anomx, anomy]), axis=0)\n",
    "\n",
    "tr_op2, anom2, id2 = gc.read_tfr_record(OPDIR+'set2and3_v2',['true_op','anomaly', 'id'],['ar','ar', 'b'],['float32','bool','string'])\n",
    "tr_op3, anom3 = gc.read_tfr_record(OPDIR+'set2and3_v3',['true_op','anomaly'],['ar','ar'],['float32','bool'])\n",
    "tr_op4, anom4 = gc.read_tfr_record(OPDIR+'set2and3_v4',['true_op','anomaly'],['ar','ar'],['float32','bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the ensemble\n",
    "INPDIR='../../processed_directories/full_lc_planets/'\n",
    "OPDIR='../../processed_directories/anomalies_set4/'\n",
    "\n",
    "\n",
    "tr_op2, anom2, id2 = gc.read_tfr_record(OPDIR+'set4_v1',['true_op','anomaly', 'id'],['ar','ar', 'b'],['float32','bool','string'])\n",
    "tr_op3, anom3 = gc.read_tfr_record(OPDIR+'set4_v2',['true_op','anomaly'],['ar','ar'],['float32','bool'])\n",
    "tr_op4, anom4 = gc.read_tfr_record(OPDIR+'set4_v3_2',['true_op','anomaly'],['ar','ar'],['float32','bool'])\n",
    "tr_op5, anom5 = gc.read_tfr_record(OPDIR+'set4_v3',['true_op','anomaly'],['ar','ar'],['float32','bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = [str(el)[2:11] for el in id2]\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netanom = np.asarray(anom2) * np.asarray(anom3) * np.asarray(anom4)\n",
    "netanom = ( np.asarray(anom2, dtype='int') + np.asarray(anom3, dtype='int') + np.asarray(anom5, dtype='int') +\n",
    "    np.asarray(anom4, dtype='int'))>2\n",
    "print(netanom[1])\n",
    "no = 1\n",
    "plt.plot(tr_op2[no])\n",
    "count = np.where(netanom[no]>0)[0]\n",
    "plt.plot(count, np.asarray(tr_op2)[no,count], marker='.', ls='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range(0,len(netanom)):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    xax = np.arange(0,len(tr_op2[x]))*29.4/1440\n",
    "    ax.plot(xax,tr_op2[x], label='lc', marker='.', ls='None')\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('flux')\n",
    "    ax.set_title(idf[x])\n",
    "    count = np.where(netanom[x]>0)[0]\n",
    "    ax.scatter(xax[count], np.asarray(tr_op2)[x,count], marker='o', ls='None', label='weird', color='black')\n",
    "    ax.legend()\n",
    "    plt.savefig(OPDIR+idf[x]+'_fin.png')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a periodic stream and an anomaly\n",
    "bit = np.concatenate(np.asarray([np.zeros(50),np.ones(10)]))\n",
    "print(bit.shape)\n",
    "inp = np.tile(bit, int(68000/60))\n",
    "print(inp.shape)\n",
    "\n",
    "inp = generate_anomaly(inp, 0.05)\n",
    "binip=2000\n",
    "binop=100\n",
    "x=np.asarray([inp[i:i+binip] for i in range(0,len(inp)-binip-binop, binop)])\n",
    "y=np.asarray([inp[i+binip:i+binip+binop] for i in range(0,len(inp)-binip-binop, binop)])\n",
    "print(\"shapes:\",x.shape, y.shape)\n",
    "plt.plot(inp)\n",
    "plt.xlim(20000,21000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholistic code gen anomaly:\n",
    "INPDIR='../../processed_directories/full_lc_planets/'\n",
    "OPDIR='../../processed_directories/'\n",
    "entries = os.listdir(INPDIR)\n",
    "el=entries[5]\n",
    "\n",
    "#x,y = get_chunks_from_record(INPDIR+el,2000,20)\n",
    "Mdl = predictor(2000,100)\n",
    "Mdl, history, mserr = train_model(Mdl,x,y,0,300,True)\n",
    "prdy, ty = test_model(Mdl, x, y)\n",
    "#mserr = 0.3\n",
    "anarr = get_anomalies(prdy, ty, mserr/3)\n",
    "print(len(prdy), len(ty))\n",
    "\n",
    "plot_anomalies(ty, anarr, el, OPDIR)\n",
    "\n",
    "net = np.asarray([x, ty, prdy, anarr], dtype='object')\n",
    "gc.write_tfr_record(OPDIR+'gentest3',net,['input','true_op','pred_op','anomaly'],['ar','ar','ar','ar'],\n",
    "    ['float32','float32','float32','bool'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter (np.arange(len(prdy)),prdy, color='green')\n",
    "plt.scatter(np.arange(len(ty)),ty, marker='.')\n",
    "plt.xlim(20000,22000)\n",
    "anarr = get_anomalies(prdy, ty, 5)\n",
    "print(len(prdy), len(ty))\n",
    "plot_anomalies(ty, anarr, el, OPDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "CV_PATH='../../training_data/cross_validation/'\n",
    "MN_PATH='../../training_data/'\n",
    "\n",
    "Xtrain, Mtrain, Otrain, OMtrain = gc.read_tfr_record(MN_PATH+'anomalies_ts2000_100',\n",
    "    ['input','map','output','opmap'],\n",
    "    ['ar','ar','ar','ar'], \n",
    "    [tf.float32,tf.float32,tf.float32,tf.float32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainf=[]\n",
    "Otrainf=[]\n",
    "Mtrainf=[]\n",
    "OMtrainf=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    if(not ((-min(Xtrain[i])+np.median(Xtrain[i]))>0) ): continue\n",
    "    if(not ((-min(Otrain[i])+np.median(Otrain[i]))>0) ): continue\n",
    "    Xtrainf.append((Xtrain[i]-np.median(Xtrain[i]))/(-min(Xtrain[i])+np.median(Xtrain[i])))\n",
    "    Otrainf.append((Otrain[i]-np.median(Otrain[i]))/(-min(Otrain[i])+np.median(Otrain[i])))\n",
    "    Mtrainf.append(Mtrain[i])\n",
    "    OMtrainf.append(OMtrain[i])\n",
    "\n",
    "Otrainf = np.asarray(Otrainf)\n",
    "Xtrainf = np.asarray(Xtrainf)\n",
    "Mtrainf = np.asarray(Mtrainf)\n",
    "OMtrainf = np.asarray(OMtrainf)\n",
    "print(Otrainf.shape, Xtrainf.shape, Mtrainf.shape, OMtrainf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN=keras.models.load_model('tests.h5',custom_objects={'focal_dice_loss': focal_dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=convNN.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=np.asarray(pred_op)\n",
    "fig,ax=plt.subplots(6,2,figsize=(15,15))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output')\n",
    "#ar=np.random.randint(len(Xtest),size=10)\n",
    "ar=[0,1,2,3,8,7]\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,6):\n",
    "    m=min(Xtest[ar[i]])\n",
    "    ax[i][0].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='rw')\n",
    "    ax[i][0].plot(m*pred_op[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][0].plot(m*pred_op[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    ax[i][1].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='rw')\n",
    "    ax[i][1].plot(m*Ytest[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][1].plot(m*Ytest[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    ax[i][0].legend('flux')\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "ax[5][0].set_xlabel('Phase')\n",
    "ax[5][1].set_xlabel('Phase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ts(map,binip,binop):\n",
    "    xarr=np.asarray([map[i:i+binip] for i in range(0,len(map)-binip, binop)])\n",
    "    yarr=np.asarray([map[i+binip:i+binip+binop] for i in range(0,len(map)-binip, binop)])\n",
    "    return (xarr, yarr)\n",
    "\n",
    "def remove_nan(red_flux):\n",
    "    for i in range(0,len(red_flux)):\n",
    "        if np.isnan(red_flux[i]):\n",
    "            red_flux[i]=0\n",
    "\n",
    "'''def open_the_file_and_chunk(filepath, hdu_no, dil_rate):\n",
    "    hdu = fits.open(filepath)\n",
    "    flux=hdu[hdu_no].data['LC_DETREND']\n",
    "    #flux=hdu[len(hdu)-1].data['RESIDUAL_LC']\n",
    "    tdurs = [hdu[n].header['TPERIOD'] for n in range(1,len(hdu)-1)]\n",
    "    phase = hdu[hdu_no].data['PHASE']\n",
    "    \n",
    "    remove_nan(flux)\n",
    "    inc=4000*dil_rate\n",
    "    chunks = np.asarray([flux[np.arange(i,i+inc,dil_rate)] for i in range(0,len(flux)-inc, inc)])\n",
    "    return(chunks, tdurs, phase)'''\n",
    "\n",
    "def plot_a_map(input, map):\n",
    "    totip = np.concatenate(input)\n",
    "    totop = np.concatenate(map, axis=0)\n",
    "\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in totop])\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "\n",
    "    m=min(totip)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Total Segmentation Map')\n",
    "    plt.xlabel('time (days)')\n",
    "    plt.ylabel('flux')\n",
    "    #plt.plot(np.arange(0,len(totip))*29.4/1440,totop[:,0])\n",
    "    #plt.plot(np.arange(0,len(totip))*29.4/1440,totop[:,1])\n",
    "    plt.plot(np.asarray(bkg)*29.4/1440,totip[bkg], color='#a4b3b6', marker='.', ls='none', label='bkg')\n",
    "    plt.plot(np.asarray(fps)*29.4/1440,totip[fps], color='#44318d', marker='.', ls='none', label='fps')\n",
    "    plt.plot(np.asarray(pl)*29.4/1440,totip[pl], color='#d83f87', marker='.', ls='none', label='pl')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = 'E:\\Masters_Project_Data\\\\alienworlds_fps\\\\'\n",
    "#kplr004852528-20160128150956_dvt.fits\n",
    "#kplr011442793-20160128150956_dvt.fits\n",
    "#kplr008480285-20160128150956_dvt.fits\n",
    "#kplr011619964-20160128150956_dvt.fits\n",
    "NAME = 'kplr011619964-20160128150956_dvt.fits'\n",
    "\n",
    "chunks, tdur, pharr = open_the_file_and_chunk(FILEPATH+NAME,1,1)\n",
    "print(chunks.shape)\n",
    "x = min(chunks.reshape(-1))\n",
    "chunks=np.asarray([(row-np.median(chunks))/(-x+np.median(chunks)) for row in chunks])\n",
    "#opchunk = convNN.predict(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_map(chunks, opchunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredM = predictor(2000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XPtr1, YPtr1 = gen_ts(np.concatenate(chunks,axis=0),2000,500)\n",
    "print(XPtr1.shape, YPtr1.shape)\n",
    "#XPtr2, YPtr2 = gen_ts(np.concatenate(opchunk,axis=0)[:,1],2000,10)\n",
    "#print(XPtr2.shape, YPtr2.shape)\n",
    "\n",
    "#XPtr=np.concatenate(np.asarray([XPtr1,XPtr2]))\n",
    "#YPtr=np.concatenate(np.asarray([YPtr1,YPtr2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.asarray(Mtrain).shape, np.asarray(OMtrain).shape)\n",
    "Xtr, Xtst, Otr, Otst = train_test_split(XPtr1, YPtr1, test_size=0.6)\n",
    "print(Xtr.shape, Otr.shape, Xtst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=PredM.fit(XPtr1[:600], YPtr1[:600], batch_size=8, epochs=30, verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('fprez_segment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_op=PredM.predict(XPtr1[:600])\n",
    "thresmax = max(np.asarray((test_op-YPtr1[:600])**2).reshape(-1))\n",
    "print(thresmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,1,figsize=(20,10))\n",
    "for i in range(0,5):\n",
    "    ax[i].plot(Otst[i+10])\n",
    "    ax[i].plot(1*test_op[i+10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredM.load_weights('anomaly_take3_maptrain_wts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XPtrtmp, YPtrtmp = gen_ts(np.concatenate(chunks), 2000, 50)\n",
    "\n",
    "PrMP2=PredM.predict(XPtrtmp)\n",
    "#PrMP1=PredM.predict(XPtr1)\n",
    "#PrMP1=np.concatenate(PrMP1)\n",
    "PrMP2=np.concatenate(PrMP2)\n",
    "PrMP2=PrMP2.reshape(-1)\n",
    "print(PrMP2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(pred_op[1,1000:,1])\n",
    "plt.plot(np.concatenate(chunks,axis=0)[2000:,])\n",
    "plt.plot(1*PrMP2)\n",
    "#plt.xlim(1000,10000)\n",
    "#plt.plot(Ytest[1,1000:,1])\n",
    "#print((np.concatenate(chunks,axis=0)[2000:,]).shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff2=np.concatenate(opchunk,axis=0)[2000:,1]-PrMP2\n",
    "#diff2=diff2**2\n",
    "#diff1=np.concatenate(opchunk,axis=0)[2000:,0]-PrMP1\n",
    "#diff1=diff1**2\n",
    "\n",
    "difftmp=np.concatenate(chunks,axis=0)[2000:]-1*PrMP2\n",
    "difftmp = difftmp**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print((diff1>np.mean(diff1)+2*np.std(diff1)).sum())\n",
    "print((diff2>np.mean(diff2)+2*np.std(diff2)).sum())\n",
    "anarr1=diff1>np.mean(diff1)+2*np.std(diff1)\n",
    "anarr2=diff2>np.mean(diff2)+2*np.std(diff2)'''\n",
    "\n",
    "anarrtmp=difftmp>np.mean(difftmp)+2*np.std(difftmp)\n",
    "\n",
    "print(anarrtmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opraw=np.concatenate(chunks)[2000:]\n",
    "plt.plot(opraw,marker='.', ls='None')\n",
    "#count1=np.where(anarr1>0)[0]\n",
    "#count2=np.where(anarr2>0)[0]\n",
    "count=np.where(anarrtmp>0)[0]\n",
    "#count=np.intersect1d(count1, count2)\n",
    "#plt.plot(count1,opraw[count1],marker='.', ls='None')\n",
    "plt.plot(count,opraw[count],marker='.', ls='None')\n",
    "#plt.xlim(35075,35125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tdur) \n",
    "phase=pharr[2000:68000]\n",
    "plt.plot(phase, opraw, marker='.', ls='None')\n",
    "plt.plot(phase[count],opraw[count],marker='.', ls='None')\n",
    "#plt.xlim(4.5,5)\n",
    "#plt.xlim(-0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks,lombscargle\n",
    "nt = opraw*anarrtmp < -0.5\n",
    "p = np.linspace(0.01,300,5000)\n",
    "print(nt)\n",
    "f = 2*np.pi /p\n",
    "predp = lombscargle(np.arange(0,len(nt))*29.4/1440, nt,f, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opraw*anarrtmp, marker='.', ls='None')\n",
    "#plt.plot(p,predp)\n",
    "#plt.xlim(0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredM.save('anomaly_take3_maptrain')\n",
    "PredM.save_weights('anomaly_take3_maptrain_wts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using random forest or k means and all\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

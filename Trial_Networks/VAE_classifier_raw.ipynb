{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#we initially put classifier and the other thing in different files... better to find a way to train it together...\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn import preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#loading data and fitting follows:\n",
                "img_size=7936\n",
                "np.random.seed(1234567)\n",
                "#loading data and fitting follows:\n",
                "X_train=np.loadtxt('training_data/Xtrain_ush_short_nolap.csv',delimiter=',')\n",
                "Y_train=np.loadtxt('training_data/Ytrain_ush_short_nolap.csv',delimiter=',')\n",
                "#Xtrain=Xtrain[:2000]\n",
                "X_train=np.transpose(X_train)\n",
                "print(X_train.shape)\n",
                "\n",
                "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_train, Y_train, test_size=0.33)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sampling(mu_log_variance):\n",
                "    mu, log_variance = mu_log_variance\n",
                "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mu), mean=0.0, stddev=1.0)\n",
                "    random_sample = mu + keras.backend.exp(log_variance/2) * epsilon\n",
                "    return random_sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#model defined here\n",
                "#leaky reLU has been added as a separate layer than define as an activation\n",
                "inp = keras.layers.Input(shape=(img_size, 1), name=\"encoder_input\")\n",
                "x1 = keras.layers.Conv1D(filters=1, kernel_size=5, padding=\"same\", strides=2,name=\"encoder_conv_1\")(inp)\n",
                "x1 = keras.layers.BatchNormalization(name=\"encoder_norm_1\")(x1)\n",
                "x1 = keras.layers.LeakyReLU(name=\"encoder_leakyrelu_1\")(x1)\n",
                "\n",
                "x1 = keras.layers.Conv1D(filters=16, kernel_size=5, padding=\"same\", strides=2, name=\"encoder_conv_2\")(x1)\n",
                "x1 = keras.layers.BatchNormalization(name=\"encoder_norm_2\")(x1)\n",
                "x1 = keras.layers.LeakyReLU(name=\"encoder_leakyrelu_2\")(x1)\n",
                "\n",
                "x1 = keras.layers.Conv1D(filters=32, kernel_size=5, padding=\"same\", strides=2, name=\"encoder_conv_3\")(x1)\n",
                "x1 = keras.layers.BatchNormalization(name=\"encoder_norm_3\")(x1)\n",
                "bp_lay_1 = keras.layers.LeakyReLU(name=\"encoder_leakyrelu_3\")(x1)\n",
                "\n",
                "#flatten the layers in encoder\n",
                "shape_before_flatten = keras.backend.int_shape(bp_lay_1)[1:]\n",
                "x2 = keras.layers.Flatten(name=\"flat_1\")(bp_lay_1)\n",
                "\n",
                "latent_space_dim = 4\n",
                "#declare a mean and variance for the distribution\n",
                "encoder_mu = keras.layers.Dense(units=latent_space_dim, name=\"encoder_mu\")(x2)\n",
                "encoder_log_variance = keras.layers.Dense(units=latent_space_dim, name=\"encoder_log_variance\")(x2)\n",
                "encoder_op = keras.layers.Lambda(sampling, name=\"encoder_output\")([encoder_mu, encoder_log_variance])\n",
                "\n",
                "encoder = keras.models.Model(inp, encoder_op, name=\"encoder_model\")\n",
                "encoder.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#inp2 = keras.layers.Input(shape=shape_before_flatten, name=\"encoder_input\")\n",
                "#x3 = keras.layers.AvgPool1D(2,strides=2,name='red_comp')(bp_lay_1)\n",
                "#x3 = keras.layers.Flatten(name=\"flat_2\")(bp_lay_1)\n",
                "x3 = keras.layers.concatenate([encoder_mu, encoder_log_variance])\n",
                "#x3 = keras.layers.Dense(128, activation='relu')(x3)\n",
                "#x3 = keras.layers.Dropout(0.1)(x3)\n",
                "#x3 = keras.layers.Dense(16, activation='relu')(x3)\n",
                "#x3 = keras.layers.Dropout(0.1)(x3)\n",
                "x3 = keras.layers.Dense(8, activation='relu')(x3)\n",
                "x3 = keras.layers.Dense(4, activation='relu')(x3)\n",
                "#x3 = keras.Model(inputs=inp, outputs=x3)\n",
                "#x3 = keras.layers.concatenate([encoder_mu, encoder_log_variance, x3.output])\n",
                "out = keras.layers.Dense(2, activation='sigmoid')(x3)\n",
                "\n",
                "full_model = keras.models.Model(inp,out, name=\"classifier\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder.load_weights('encoding_raw_7936_d8.h5')\n",
                "for l1,l2 in zip(full_model.layers[:11],encoder.layers[0:11]):\n",
                "    l1.set_weights(l2.get_weights())\n",
                "\n",
                "print(full_model.get_weights()[0])\n",
                "print(encoder.get_weights()[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for layer in full_model.layers[0:11]:\n",
                "    layer.trainable = False\n",
                "\n",
                "full_model.summary()\n",
                "full_model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(lr=0.005),metrics=['accuracy'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(np.array(Xtrain).shape)\n",
                "Xtrain=preprocessing.normalize(Xtrain)\n",
                "Xtrain=np.array(Xtrain).reshape(2309, 7936, 1)\n",
                "print(np.array(Xtrain).shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
                "classify_train = full_model.fit(Xtrain, Ytrain, batch_size=64,epochs=100,verbose=1,validation_split=0.3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_model.save_weights('autoencoder_classification.h5')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "test_loss, test_acc = full_model.evaluate(np.array(Xtrain), np.array(Ytrain))\n",
                "print('Test accuracy:', test_acc)\n",
                "Ypred_raw=full_model.predict(np.array(Xtrain))\n",
                "Ypred=np.argmax(Ypred_raw, axis=1)\n",
                "Ytest_new=np.argmax(Ytrain,axis=1)\n",
                "cm = confusion_matrix(Ytest_new, Ypred)\n",
                "print(cm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "plt.plot(classify_train.history['accuracy'], label='accuracy')\n",
                "plt.plot(classify_train.history['val_accuracy'], label = 'val_accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.ylim([0.5, 1])\n",
                "plt.legend(loc='lower right')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(classify_train.history['loss'], label='accuracy')\n",
                "plt.plot(classify_train.history['val_loss'], label = 'val_accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "#plt.ylim([0.5, 1])\n",
                "plt.legend(loc='lower right')"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
        },
        "kernelspec": {
            "display_name": "Python 3.9.5 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

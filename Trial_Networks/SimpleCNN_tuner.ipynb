{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#attempt to improve the NN\n",
                "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "import keras_tuner as kt\n",
                "import pandas as pd\n",
                "from sklearn import preprocessing\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train=np.loadtxt('training_data/Xtrain_seg_mask.csv',delimiter=',')\n",
                "Y_train=np.loadtxt('training_data/Ytrain_seg_mask.csv',delimiter=',')\n",
                "#X_trainl=np.loadtxt('training_data/Xtrainl_av_gl6000.csv',delimiter=',')\n",
                "\n",
                "#X_train=X_train[:,250:550]\n",
                "arr=np.arange(0,len(X_train),1)\n",
                "np.random.shuffle(arr)\n",
                "\n",
                "#X_extra=np.loadtxt('training_data/Xextra_av_stitch.csv',delimiter=',')\n",
                "#X_train=preprocessing.normalize(X_train)\n",
                "#X_train=X_train[:,150:350]\n",
                "#scalar=preprocessing.StandardScaler()\n",
                "#scalar.fit(X_train)\n",
                "#X_train=scalar.transform(X_train)\n",
                "#X_train=X_train\n",
                "#X_train=np.array([[el[i] for i in range(0,len(el),2)] for el in X_train])\n",
                "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_train, Y_train, test_size=0.2)\n",
                "print(Xtrain.shape,Ytrain.shape,Xtest.shape,Ytest.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(11223)\n",
                "'''\n",
                "wt=[]\n",
                "for el in Xtrain:\n",
                "    med=np.median(el)\n",
                "    min=el[np.argmin(el)]\n",
                "    std=np.std(el)\n",
                "    temp=(el-med)/(med-min)\n",
                "    clean=[x for x in temp if x > med-std]\n",
                "    wt.append(1/np.sqrt(np.std(clean)))\n",
                "\n",
                "norm = np.linalg.norm(np.array(wt))\n",
                "wt= 100*(np.array(wt)/norm)\n",
                "\n",
                "print(wt.shape)\n",
                "\n",
                "kernel_size = 3\n",
                "kernel = np.ones(kernel_size) / kernel_size\n",
                "Xtrain = [np.convolve(Xtrain[i], kernel, mode='same') for i in range(len(Xtrain))]\n",
                "Xtest = [np.convolve(Xtest[i], kernel, mode='same') for i in range(len(Xtest))]\n",
                "'''\n",
                "#vararr=np.random.randint(0,len(Xtrain),size=10)\n",
                "fig,ax=plt.subplots(10,1,figsize=(10,20))\n",
                "\n",
                "#for i in range(0,10):\n",
                "#    ax[i].plot(Xtrain[vararr[i]],label=(Ytrain[vararr[i]],wt[vararr[i]]))\n",
                "#    ax[i].legend()\n",
                "#plt.show\n",
                "i=0\n",
                "tab=0\n",
                "while tab<10:\n",
                "        \n",
                "    ax[tab].plot(Xtrain[i],label=(Ytrain[i]))\n",
                "    ax[tab].legend()\n",
                "    tab+=1\n",
                "    i+=1\n",
                "    \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_model(hp):\n",
                "    IMG_SIZE=4800\n",
                "    IM_L=200\n",
                "    hp_units1 = hp.Int('units1', min_value=8, max_value=32, step=8)\n",
                "    hp_units2 = hp.Int('units2', min_value=16, max_value=64, step=16)\n",
                "    hp_units3 = hp.Int('units3', min_value=32, max_value=128, step=32)\n",
                "    hp_units4 = hp.Int('units4', min_value=64, max_value=256, step=64)\n",
                "    hp_dense = hp.Int('unitsd', min_value=128, max_value=512, step=128)\n",
                "    hp_learning_rate = hp.Choice('learning_rate', values=[0.0001, 0.00001, 0.000001])\n",
                "\n",
                "    conv_ip = keras.layers.Input(shape=(IMG_SIZE,),name='Input')\n",
                "    x=keras.layers.Reshape((IMG_SIZE, 1), input_shape=(IMG_SIZE,),name='reshape_1')(conv_ip)\n",
                "    x=keras.layers.BatchNormalization()(x)\n",
                "    x=keras.layers.Conv1D(hp_units1,kernel_size=5,strides=3,activation='tanh',padding='same',name='second_conv16_5')(x)\n",
                "    x=keras.layers.Conv1D(hp_units1,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv16_5')(x) \n",
                "\n",
                "    x=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_1')(x)  #800\n",
                "    x=keras.layers.Conv1D(hp_units2,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv32_5')(x)\n",
                "    x=keras.layers.Conv1D(hp_units2,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv32_5')(x)\n",
                "\n",
                "    x=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_2')(x)  #400\n",
                "    x=keras.layers.Conv1D(hp_units3,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv64_5')(x)\n",
                "    x=keras.layers.Conv1D(hp_units3,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv64_5')(x)\n",
                "    \n",
                "    x=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_3')(x)    #200\n",
                "    x=keras.layers.Conv1D(hp_units4,kernel_size=3,strides=1,activation='tanh',padding='same',name='first_conv128_5')(x)\n",
                "    x=keras.layers.Conv1D(hp_units4,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv128_5')(x)\n",
                "    x=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_4')(x)    #100\n",
                "\n",
                "    x = keras.layers.Flatten(name='flat_1')(x)\n",
                "\n",
                "    x3=keras.layers.Dense(hp_dense,name='dense_layer_1',activation='relu')(x)\n",
                "    x3=keras.layers.Dense(hp_dense,name='dense_layer_2',activation='relu')(x3)\n",
                "    x3=keras.layers.Dense(hp_dense,name='dense_layer_3',activation='relu')(x3)\n",
                "\n",
                "    x3=keras.layers.Dense(1,name='dense_layer_5',activation='relu')(x3)\n",
                "    conv_op=keras.layers.Dense(2,name='dense_layer_6',activation='softmax')(x3)\n",
                "\n",
                "    #convNN = keras.Model(inputs=[conv_ip,conv_ipl], outputs=conv_op,name='Convolutional_NN')\n",
                "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
                "\n",
                "    #convNN.summary()\n",
                "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='categorical_crossentropy',metrics=['accuracy'])\n",
                "\n",
                "    return convNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuner = kt.Hyperband(\n",
                "    build_model,\n",
                "    objective='val_accuracy',\n",
                "    max_epochs=20,\n",
                "    directory='tune4800',\n",
                "    project_name='keras_tuner_demo'\n",
                ")\n",
                "\n",
                "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
                "tuner.search(np.array(Xtrain), np.array(Ytrain), epochs=60, validation_split=0.2,callbacks=[stop_early])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = tuner.get_best_models(num_models=1)\n",
                "bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
                "cnn = tuner.hypermodel.build(bestHP)\n",
                "H = cnn.fit(x=Xtrain, y=Ytrain, validation_split=0.2, batch_size=64, epochs=100, verbose=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
                "#history=convNN.fit([np.array(Xtrain),np.array(Xtrainl)],np.array(Ytrain), batch_size=64, epochs=40, verbose=VERBOSE, validation_split=0.12,callbacks=[es_callback])\n",
                "#history=convNN.fit([np.array(Xtrain),np.array(Xtrainl)],np.array(Ytrain), batch_size=64, epochs=50 , verbose=1 , shuffle=True,validation_split=0.2)\n",
                "print(\" \")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(H.history['loss'])\n",
                "plt.plot(H.history['val_loss'])\n",
                "plt.title('model loss')  \n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'test'], loc='upper left')\n",
                "plt.savefig('Mloss_opt_rebin.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(H.history['accuracy'])\n",
                "plt.plot(H.history['val_accuracy'])\n",
                "plt.title('model accuracy')\n",
                "plt.ylabel('accuracy')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'test'], loc='upper left')\n",
                "plt.savefig('macc_opt_rebin.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_loss, test_acc = convNN.evaluate(np.array(Xtest), np.array(Ytest))\n",
                "print('Test accuracy:', test_acc)\n",
                "Ypred_raw=convNN.predict(np.array(Xtest))\n",
                "Ypred=np.argmax(Ypred_raw, axis=1)\n",
                "Ytest_new=np.argmax(Ytest,axis=1)\n",
                "cm = confusion_matrix(Ytest_new, Ypred)\n",
                "print(cm)\n",
                "#from sklearn.metrics import mean_squared_error\n",
                "#pred_op=convNN.predict(np.array(Xtest))\n",
                "#[print(Xextra_ts[i,1]) for i in range(len(Xextra_ts))]\n",
                "#val=mean_squared_error(pred_op, Xextra_ts[:,1], squared=False)\n",
                "#print(val)\n",
                "#[print(pred_op[i],Xextra_ts[i,1]) for i in range(0,50)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#convNN.save_weights('conv_LocalGlobal_raw.h5')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
        },
        "kernelspec": {
            "display_name": "Python 3.9.7 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

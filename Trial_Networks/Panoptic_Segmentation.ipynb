{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.signal import argrelextrema,find_peaks\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix,mean_squared_error\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.loadtxt('../../training_data/Xtrain_seg_mask_rv_sm_bal.csv',delimiter=',')\n",
    "Y_train=np.loadtxt('../../training_data/Ytrain_seg_mask_rv_sm_bal.csv',delimiter=',')\n",
    "C_train=np.loadtxt('../../training_data/Ctrain_seg_mask_rv_sm_bal.csv',delimiter=',')\n",
    "\n",
    "print(X_train.shape,Y_train.shape,C_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=Y_train.reshape(len(X_train),4000,3)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_train = np.asarray([min(el) for el in X_train])\n",
    "print(E_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest, Ctrain, Ctest, Etrain, Etest= train_test_split(X_train, Y_train, C_train, E_train, test_size=0.2)\n",
    "print(Xtrain.shape,Ytrain.shape,Ctrain.shape,Xtest.shape,Ytest.shape,Ctest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "fig,ax=plt.subplots(10,2,figsize=(10,20))\n",
    "for i in range(0,10):\n",
    "    ax[i][0].plot(Xtrain[vararr[i]])\n",
    "    #ax[i][1].plot(Ytrain[vararr[i],:,2])\n",
    "    ax[i][1].plot(Ytrain[vararr[i],:,0],label=Ctrain[vararr[i]])\n",
    "    ax[i][1].plot(Ytrain[vararr[i],:,1])\n",
    "    #ax[i][1].set_xlim(2600,3000)\n",
    "    ax[i][1].legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred) + generalized_dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    #w = (1 - w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=4000\n",
    "#add hidden layers\n",
    "conv_ip = keras.layers.Input(shape=(IMG_SIZE,),name='Input')\n",
    "extra_ip = keras.layers.Input(shape=(1,), name='lost_snr')\n",
    "xi=keras.layers.Reshape((IMG_SIZE, 1), input_shape=(IMG_SIZE,),name='reshape_1')(conv_ip)\n",
    "#xi=keras.layers.Cropping1D(cropping=(100, 100))(xi)\n",
    "xi=keras.layers.BatchNormalization()(xi)\n",
    "\n",
    "x1=keras.layers.SeparableConv1D(32,kernel_size=5,strides=2,activation='tanh',padding='same',name='second_conv32_5')(xi)\n",
    "c1=keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv32_5')(x1)    #2400,32\n",
    "\n",
    "x2=keras.layers.BatchNormalization()(c1)\n",
    "x2=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_1')(x2)   \n",
    "x2=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='first_conv64_5')(x2)\n",
    "c2=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv64_5')(x2) #size 1200,64\n",
    "\n",
    "x3=keras.layers.BatchNormalization()(c2)\n",
    "x3=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_2')(x3)   \n",
    "x3=keras.layers.SeparableConv1D(128,kernel_size=5,strides=1,activation='tanh',padding='same',name='first_conv128_5')(x3)\n",
    "c3=keras.layers.SeparableConv1D(128,kernel_size=5,strides=1,activation='tanh',padding='same',name='second_conv128_5')(x3) #size 600,128\n",
    "\n",
    "x4=keras.layers.BatchNormalization()(c3)\n",
    "x4=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_3')(x4)    \n",
    "x4=keras.layers.SeparableConv1D(256,kernel_size=5,strides=1,activation='tanh',padding='same',name='third_conv256_5')(x4)#size 300,256\n",
    "x4=keras.layers.SeparableConv1D(512,kernel_size=5,strides=1,activation='tanh',padding='same',name='fourth_conv256_5')(x4)#size 300,256\n",
    "\n",
    "b1=keras.layers.SeparableConv1D(512,kernel_size=3,strides=2,activation='tanh',padding='same',name='bypass')(x4)#size 300,256\n",
    "b1 = keras.layers.Flatten(name='prethis')(b1)\n",
    "b1 = keras.layers.Dense(64,activation='relu')(b1)\n",
    "#b1 = keras.layers.Dense(32,activation='relu')(b1)\n",
    "\n",
    "b2 = keras.layers.Conv1DTranspose(128, kernel_size=5, activation='relu', padding=\"same\", strides=2, name=\"dec_conv_tran128_5\")(x4)#600,128\n",
    "b2 = keras.layers.Conv1DTranspose(128, kernel_size=5, activation='relu', padding=\"same\", strides=1, name=\"dec_conv_tran128_5_2\")(b2)#600,128\n",
    "b2 =keras.layers.BatchNormalization()(b2)\n",
    "\n",
    "x5 = keras.layers.Concatenate(axis=2)([c3,b2])\n",
    "x5 = keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='relu',padding='same',name='dec_conv128_5')(x5)\n",
    "x5 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', padding=\"same\", strides=2, name=\"dec_conv_tran64_1\")(x5)\n",
    "x5 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', padding=\"same\", strides=1, name=\"dec_conv_tran64_2\")(x5)\n",
    "x5 = keras.layers.BatchNormalization()(x5)  #1200,64\n",
    "\n",
    "x6 = keras.layers.Concatenate(axis=2)([c2,x5]) \n",
    "x6 = keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='relu',padding='same',name='dec_conv64_2')(x6) #1200,64\n",
    "x6 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', padding=\"same\", strides=2, name=\"dec_conv_tran32_5\")(x6)\n",
    "x6 = keras.layers.BatchNormalization()(x6)  #2400,32\n",
    "\n",
    "x7 = keras.layers.Concatenate(axis=2)([c1,x6]) \n",
    "x7 = keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='relu',padding='same',name='dec_conv32_5')(x7) #1200,64\n",
    "x7 = keras.layers.Conv1DTranspose(32,kernel_size=3,strides=2,activation='relu',padding='same',name='dec_convt32_5')(x7) #1200,64\n",
    "\n",
    "#b3 = keras.layers.Conv1D(3,kernel_size=3,strides=1,padding='same',name='bef_semiop',activation='relu')(x7)\n",
    "conv_op = keras.layers.Conv1D(3,kernel_size=3,strides=1,padding='same',name='semiop',activation='softmax')(x7)\n",
    "\n",
    "#b3 = keras.layers.Flatten()()\n",
    "b3 = keras.layers.Permute((2,1),name=\"check\")(conv_op)\n",
    "b3=keras.layers.Reshape((12000,1), name='reshape_2')(b3)\n",
    "b3 = keras.layers.Cropping1D(cropping=(0,4000))(b3)\n",
    "b3 = keras.layers.Flatten()(b3)\n",
    "b4 = keras.layers.Concatenate(name='this')([b1, b3, extra_ip])\n",
    "b4 = keras.layers.Dense(32,activation='relu')(b4)\n",
    "b4 = keras.layers.Dense(32,activation='relu')(b4)\n",
    "b4 = keras.layers.Dense(32,activation='relu')(b4)\n",
    "b4 = keras.layers.Dense(32,activation='relu')(b4)\n",
    "cl_op =keras.layers.Dense(2,activation='relu',name='clop')(b4)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "#convNN = keras.Model(inputs=[conv_ip,conv_ipl], outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "convNN = keras.Model(inputs=[conv_ip, extra_ip], outputs=[conv_op,cl_op],name='Convolutional_NN')\n",
    "\n",
    "\n",
    "convNN.summary()\n",
    "convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005), \n",
    "    loss={'clop': keras.losses.Huber(), 'semiop': bce_dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN.load_weights('thisonebal.h5')\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history=convNN.fit([np.asarray(Xtrain),Etrain],[np.asarray(Ytrain),np.asarray(Ctrain)], batch_size=32, epochs=15  , verbose=1,  shuffle=True,\n",
    "     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['clop_loss'])\n",
    "plt.plot(history.history['val_clop_loss'])\n",
    "#plt.plot(history.history['semiop_loss'])\n",
    "#plt.plot(history.history['val_semiop_loss'])\n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('present_segment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op,pred_cl=convNN.predict([np.array(Xtest),Etest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convNN.load_weights('long_hard_battle_eq.h5')\n",
    "fig,ax=plt.subplots(6,2,figsize=(15,20))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output')\n",
    "ar=np.random.randint(len(Xtest),size=10)\n",
    "#ar=[0,1,2,3,8,7]\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,6):\n",
    "    #ax[i][0].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='data')\n",
    "    #ax[i][0].plot(-pred_op[ar[i],:,2],color='yellow',ls='None',marker='.',label='bkg')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    ax[i][0].set_title(str(np.asarray(np.around(pred_cl[ar[i]]))))\n",
    "\n",
    "    ax[i][1].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='data')\n",
    "    #ax[i][1].plot(-Ytest[ar[i],:,1],color='yellow',ls='None',marker='.',label='bkg')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][1].set_title(str(Ctest[ar[i]]))\n",
    "    \n",
    "    #ax[i][1].plot(pred_op_mod[ar[i]],color='black',ls='None',marker='.')\n",
    "    ax[i][0].legend('flux')\n",
    "    #ax[i][0].set_ylim(-1.05,0.1)\n",
    "    #ax[i][1].set_ylim(-1.05,0.1)\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "ax[5][0].set_xlabel('Phase')\n",
    "ax[5][1].set_xlabel('Phase')\n",
    "\n",
    "#plt.savefig('present_itsamust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN.save_weights('thisonebal.h5')\n",
    "#convNN.save('Model_long_hard_battle_av.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp=[np.around(el) for el in pred_cl]\n",
    "ntest=[el>0 for el in Ctrain]\n",
    "npred=[el>0 for el in comp]\n",
    "#print(ntest,npred)\n",
    "\n",
    "cm=confusion_matrix(np.asarray(ntest)[:,0],np.asarray(npred)[:,0])\n",
    "print(cm/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding periodicity and all\n",
    "\n",
    "note=40\n",
    "x=pred_op[note,:,1]\n",
    "x2=pred_op[note,:,0]\n",
    "y=Ytest[note,:,1]\n",
    "y2=Ytest[note,:,0]\n",
    "h=(np.median(x)+2*np.std(x))\n",
    "h2=(np.median(x2)+2*np.std(x2))\n",
    "print(h,h2)\n",
    "kernel_size = 22\n",
    "kernel = np.ones(kernel_size) / kernel_size\n",
    "#x=np.convolve(x, kernel, mode='same')\n",
    "#Xtrain = [np.convolve(Xtrain[i], kernel, mode='same') for i in range(IP)]\n",
    "#Xtest = [np.convolve(Xtest[i], kernel, mode='same') for i in range(TEST)]\n",
    "peaksf, _ = find_peaks(x, height=h,distance=20)\n",
    "peaksp, _ = find_peaks(x2, height=h2,distance=20)\n",
    "peaksft,_ = find_peaks(y, height=0)\n",
    "peakspt,_ = find_peaks(y2, height=0)\n",
    "print(len(peaksf),len(peaksft),len(peaksp),len(peakspt))\n",
    "\n",
    "plwh=np.where(y==1)[0]\n",
    "fpswh=np.where(y2==1)[0]\n",
    "\n",
    "print(\"checkalg\",np.mean(x[peaksf]),np.std(x[peaksf]),np.mean(x2[peaksp]),np.std(x2[peaksp]))\n",
    "\n",
    "plt.plot(y,color='blue')\n",
    "plt.plot(y2,color='red')\n",
    "plt.plot(x,color='green')\n",
    "plt.plot(x2,color='black')\n",
    "\n",
    "plt.plot(peaksf,x[peaksf],color=\"blue\",marker=\".\",ls='None')\n",
    "#plt.plot(peaksft,y[peaksft],color=\"blue\",marker=\".\",ls='None')\n",
    "plt.plot(peaksp,x2[peaksp],color=\"red\",marker=\".\",ls='None')\n",
    "#plt.plot(peakspt,y2[peakspt],color=\"red\",marker=\".\",ls='None')\n",
    "\n",
    "\n",
    "#plt.xlim(0,2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new code to get periodicity... idk if its gonna be worthwile but lets see\n",
    "#get test data\n",
    "class_Ytest=[]\n",
    "for el in Ytrain:\n",
    "    xpl=el[:,0]\n",
    "    xfps=el[:,1]\n",
    "    plp, _ = find_peaks(xpl, height=0)\n",
    "    pfps, _ = find_peaks(xfps, height=0)\n",
    "    class_Ytest.append([len(plp),len(pfps)])\n",
    "\n",
    "class_Ypred=[]\n",
    "pred_conf=[]\n",
    "for el in pred_op:\n",
    "    pl = el[:,0]\n",
    "    hpl = np.median(pl)+0.1*np.std(pl)\n",
    "    fps = el[:,1]\n",
    "    hfps = np.median(fps)+0.1*np.std(fps)\n",
    "    plp, _ = find_peaks(pl, height=hpl,distance=10)\n",
    "    fpsp, _ = find_peaks(fps, height=hfps,distance=10) \n",
    "    if(len(plp>0)): \n",
    "        val1=np.mean(pl[plp])\n",
    "        std1=np.std(pl[plp])\n",
    "    else: \n",
    "        val1=0\n",
    "        std1=0\n",
    "    if(len(fpsp)>0): \n",
    "        val2=np.mean(fps[fpsp])\n",
    "        std2=np.std(fps[fpsp])\n",
    "    else: \n",
    "        val2=0\n",
    "        std2=0\n",
    "          \n",
    "    class_Ypred.append([val1,val2])\n",
    "    pred_conf.append([std1,std2])\n",
    "    \n",
    "\n",
    "print(np.asarray(class_Ypred).shape,np.asarray(class_Ytest).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pred=[]\n",
    "n_test=[]\n",
    "good_preds=0\n",
    "for i in range(len(class_Ypred)):\n",
    "    #planet\n",
    "    ind=np.argmax(class_Ypred[i])\n",
    "    if(class_Ypred[i][ind]-0.5*pred_conf[i][ind] > class_Ypred[i][1-ind] + 0.5*pred_conf[i][1-ind]): good_preds+=1\n",
    "    #else: continue\n",
    "    n_pred.append(ind)\n",
    "    if(class_Ytest[i][0]>0 and class_Ytest[i][1]>0): n_test.append(np.argmax(class_Ypred[i]))\n",
    "    elif(class_Ytest[i][0]>0): n_test.append(0)\n",
    "    else:  n_test.append(1)\n",
    "\n",
    "cm=confusion_matrix(np.asarray(n_test),np.asarray(n_pred))\n",
    "print(good_preds)\n",
    "print(cm/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to improve the NN\n",
    "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import GetLightcurves as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Ctrain, TrainID = gc.read_tfr_record('../../training_data/seg_mask_training_av_spaced',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "Xtest, Ytest, Ctest, TestID = gc.read_tfr_record('../../training_data/seg_mask_test_av_spaced',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain=np.asarray(Ytrain).reshape(len(Xtrain),4000,3)\n",
    "Ytest=np.asarray(Ytest).reshape(len(Xtest),4000,3)\n",
    "print(Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainf=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
    "Xtestf=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])\n",
    "#Xtrainf=np.asarray([row-np.median(row) for row in Xtrain])\n",
    "#Xtestf=np.asarray([row-np.median(row) for row in Xtest])\n",
    "#Xtrainf=np.asarray([np.tanh(100*row) for row in Xtrainf])\n",
    "#Xtestf=np.asarray([np.tanh(100*row) for row in Xtestf])\n",
    "\n",
    "#Xtrainf=np.asarray([0.5*(np.tanh(0.1*(row - np.median(row))/np.std(row))) for row in Xtrain])\n",
    "#Xtestf=np.asarray([0.5*(np.tanh(0.1*(row - np.median(row))/np.std(row))) for row in Xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(Xtrain)):\n",
    "    if(np.any(Ytrain[i,:,0]==1) and np.any(Ytrain[i,:,1]==1)):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = np.asarray(Ytrain, dtype = 'float32')\n",
    "Ytest = np.asarray(Ytest, dtype = 'float32')\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "vararr=[5,247,3,375]\n",
    "#vararr=[375,376,396,442,456,613]\n",
    "fig,ax=plt.subplots(4,3,figsize=(12,12))\n",
    "j=0\n",
    "\n",
    "xaxis = np.arange(0,4000,1)*29.5/1440\n",
    "plt.suptitle('Segmentation Signature',size=20)\n",
    "ax[0][0].set_title('Input',size=15)\n",
    "ax[0][1].set_title('Output',size=15)\n",
    "ax[0][2].set_title('Result',size=15)\n",
    "\n",
    "ax[3][0].set_xlabel('Time(Days)',size=12)\n",
    "ax[3][1].set_xlabel('Time(Days)',size=12)\n",
    "ax[3][2].set_xlabel('Time(Days)',size=12)\n",
    "for i in range(0,4):\n",
    "    ax[i][0].plot(np.arange(0,4000,1)*29.4/1440,Xtrain[vararr[i]],color='#077b8a',marker='.',ls='None')\n",
    "    #ax[i][1].plot(Ytrain[vararr[i],:,2])\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in Ytrain[vararr[i]]])\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    #predpl = np.where(Ytest[ar[i],:,0]==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "    ax[i][0].set_ylabel('Flux',size=12)\n",
    "    ax[i][2].plot(bkg*29.4/1440,Xtrain[vararr[i]][bkg],marker='.',ls='None',color='#a2d5c6',label='bkg')\n",
    "    ax[i][2].plot(pl*29.4/1440,Xtrain[vararr[i]][pl],marker='.',ls='None',color='#5c3c92',label='pl')\n",
    "    ax[i][2].plot(fps*29.4/1440,Xtrain[vararr[i]][fps],marker='.',ls='None',color='#d72631',label='fps')\n",
    "    ax[i][1].plot(np.arange(0,4000,1)*29.4/1440,Ytrain[vararr[i],:,0],color='#5c3c92',label='pl')\n",
    "    ax[i][1].plot(np.arange(0,4000,1)*29.4/1440,Ytrain[vararr[i],:,1],color='#d72631',label='fps')\n",
    "    ax[0][1].legend()\n",
    "    ax[0][2].legend()\n",
    "    ax[i][2].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "    ax[i][1].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "    ax[i][0].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "plt.savefig('fprez_segmentation2')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 0.00001\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection +smooth) / (tf.reduce_sum(y_true_f) +  tf.reduce_sum(y_pred_f) +smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def weightFocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA): \n",
    "    w = tf.reduce_sum(targets,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 - w\n",
    "    #w = 1 / (w  + 0.00001)\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)   \n",
    "    \n",
    "    #inputs = K.flatten(inputs)\n",
    "    #targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(w*K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def log_cosh_dice_loss(y_true, y_pred):\n",
    "        x = generalized_dice_coeff(y_true, y_pred)\n",
    "        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)*0.5 + log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    #w = 1 - w\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def generalized_dice_coeff_v2(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w**2  + 0.00001)\n",
    "\n",
    "    z=tf.slice(w,[0],[2])\n",
    "    z2 = tf.slice(w,[2],[1])\n",
    "    z=tf.reduce_mean(z)\n",
    "    z2=tf.reduce_mean(z2)\n",
    "    w =  tf.stack([z,z*0.7,z2])\n",
    "\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred):\n",
    "    loss = FocalLoss(y_true, y_pred) + 1.5*generalized_dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "#print(weightFocalLoss(Ytest, np.ones((len(Ytest),4000,3),dtype='float32')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=4000\n",
    "#add hidden layers\n",
    "conv_ip = keras.layers.Input(shape=(IMG_SIZE,),name='Input')\n",
    "xi=keras.layers.Reshape((IMG_SIZE, 1), input_shape=(IMG_SIZE,),name='reshape_1')(conv_ip)\n",
    "#xi=keras.layers.Cropping1D(cropping=(100, 100))(xi)\n",
    "xi=keras.layers.BatchNormalization()(xi)\n",
    "\n",
    "x1=keras.layers.SeparableConv1D(16,kernel_size=3,activation='tanh',name='1st16_5')(xi)  #3998, 32\n",
    "c1=keras.layers.SeparableConv1D(16,kernel_size=3,strides=1,activation='tanh',name='2nd16_3')(x1)  #3996, 32\n",
    "\n",
    "x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "#x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1')(x2)  #1998, 64\n",
    "x2=keras.layers.Conv1D(16,kernel_size=1,strides=2,name='maxpool_1')(x2)  #1998, 64\n",
    "x2=keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='tanh',name='1st32_5')(x2) #1996, 64\n",
    "c2=keras.layers.SeparableConv1D(32,kernel_size=5,strides=1,activation='tanh',name='2nd32_3')(x2) #1992, 64\n",
    "\n",
    "x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "x3=keras.layers.Conv1D(32,kernel_size=1,strides=2,name='maxpool_2')(x3)  #996, 64\n",
    "x3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',name='1st64_5')(x3) #994, 128\n",
    "c3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',name='2nd64_3')(x3) #992, 128\n",
    "\n",
    "x4=keras.layers.BatchNormalization(name='bn3')(c3)\n",
    "x4=keras.layers.Conv1D(64,kernel_size=1,strides=2,name='maxpool_3')(x4)  #496, 128\n",
    "x4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='1st128_5')(x4)  #494, 256\n",
    "c4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='2nd128_5')(x4) #492, 256\n",
    "\n",
    "x5=keras.layers.BatchNormalization(name='bn4')(c4) \n",
    "x5=keras.layers.Conv1D(128,kernel_size=1,strides=2,name='maxpool_4')(x5)  #246, 256  \n",
    "x5=keras.layers.SeparableConv1D(512,kernel_size=3,strides=1,activation='tanh',name='1st512_5')(x5)  #244, 256\n",
    "x5=keras.layers.SeparableConv1D(512,kernel_size=3,strides=1,activation='tanh',name='2nd512_5')(x5)  #242, 256\n",
    "\n",
    "x5 = keras.layers.Conv1DTranspose(128, kernel_size=4, activation='relu', strides=2, name=\"T1st128_5\")(x5)  #486, 256\n",
    "x5 = keras.layers.Conv1DTranspose(128, kernel_size=5, activation='relu', strides=1, name=\"T2nd128_5\")(x5)  #490, 256\n",
    "x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T3rd128_5\")(x5)  #492, 256\n",
    "x5 =keras.layers.BatchNormalization(name='bn5')(x5) \n",
    "\n",
    "#c4 = keras.layers.Cropping1D((2,2))(c4) #488, 256\n",
    "x6 = keras.layers.Concatenate(axis=2, name='cn1')([c4,x5])  #492, 512\n",
    "x6 = keras.layers.Conv1DTranspose(128,kernel_size=3,strides=1,activation='relu',name='3rd128_3')(x6) #494, 256\n",
    "x6 = keras.layers.Conv1DTranspose(64, kernel_size=4, activation='relu', strides=2, name=\"T1st64_3\")(x6) #990, 128\n",
    "x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=1, name=\"T2nd64_3\")(x6) #992, 128\n",
    "x6 = keras.layers.BatchNormalization(name='bn6')(x6)  \n",
    "\n",
    "#c3 = keras.layers.Cropping1D((4,4))(c3) #984, 128\n",
    "x7 = keras.layers.Concatenate(axis=2, name='cn2')([c3,x6]) #992, 256\n",
    "x7 = keras.layers.Conv1DTranspose(64,kernel_size=3,strides=1,activation='relu',name='3rd64_3')(x7) #994, 128\n",
    "x7 = keras.layers.Conv1DTranspose(32, kernel_size=4, activation='relu', strides=2, name=\"T1st32_3\")(x7) #1990, 64\n",
    "x7 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', strides=1, name=\"T2nd32_3\")(x7) #1992, 64\n",
    "x7 = keras.layers.BatchNormalization(name='bn7')(x7)  \n",
    "\n",
    "x8 = keras.layers.Concatenate(axis=2, name='cn3')([c2,x7])  #1992, 128\n",
    "x8 = keras.layers.Conv1DTranspose(32,kernel_size=3,strides=1,activation='relu',name='3rd32_3')(x8)  #1994, 64\n",
    "x8 = keras.layers.Conv1DTranspose(16,kernel_size=4,strides=2,activation='relu',name='T1st16_3')(x8) #3990, 32\n",
    "x8 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T2nd16_3')(x8) #3994, 32\n",
    "x8 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='T4rth16_3')(x8) #3996, 32\n",
    "x8 = keras.layers.BatchNormalization(name='bn8')(x8)  \n",
    "\n",
    "#c1 = keras.layers.Cropping1D((1,1))(c1) #3994, 32\n",
    "x9 = keras.layers.Concatenate(axis=2, name='cn4')([c1,x8])  #3996, 64\n",
    "x9 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='3rd16_3')(x9) #3998, 32\n",
    "x9 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T3rd16_3')(x9) #4002, 32\n",
    "x9 = keras.layers.BatchNormalization(name='bn9')(x9)\n",
    "\n",
    "conv_op = keras.layers.Conv1D(3,kernel_size=3,strides=1,name='semiop',activation='softmax')(x9) # (4000, 3)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "\n",
    "convNN.summary()\n",
    "convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=focal_dice_loss ,metrics=[dice_coeff] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainf,X_valf,Y_train,Y_val = train_test_split(Xtrainf, Ytrain, test_size=0.2, shuffle=True)\n",
    "print(X_trainf.shape, X_valf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convNN.load_weights('newtests.h5')\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history=convNN.fit(np.asarray(X_trainf),np.asarray(Y_train), batch_size=16, epochs=10   , verbose=1,\n",
    "     validation_data=(X_valf, Y_val))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('fprez_segment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN.save('newtests3.h5')\n",
    "#convNN.load_weights('tests.h5')\n",
    "#convNN.save('newtests2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=convNN.predict(np.array(Xtestf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model('tests.h5',custom_objects={'focal_dice_loss': focal_dice_loss})\n",
    "#Xtestf=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])\n",
    "#pred_op = model.predict(Xtestf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestID2 = [str(TestID[i])[2:13] for i in range(0,len(TestID))]\n",
    "#print(TestID2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#convNN.load_weights('long_hard_battle_eq.h5')\n",
    "Xtest = np.asarray(Xtest)\n",
    "\n",
    "fig,ax=plt.subplots(6,2,figsize=(15,15))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output')\n",
    "ar=np.random.randint(len(Xtest),size=10)\n",
    "#ar=[0,1,2,3,8,7]\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,6):\n",
    "    #m = min(Xtestf)\n",
    "    ax[i][0].plot(Xtestf[ar[i]],color='gray',ls='None',marker='.',label='data')\n",
    "    #ax[i][0].plot(-pred_op[ar[i],:,2],color='yellow',ls='None',marker='.',label='bkg')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    #ax[i][0].plot(-pred_op[ar[i],:,2],color='gray',ls='None',marker='.',label='pl')\n",
    "\n",
    "    ax[i][1].plot(Xtestf[ar[i],:],color='gray',ls='None',marker='.',label='data')\n",
    "    #ax[i][1].plot(-Ytest[ar[i],:,1],color='yellow',ls='None',marker='.',label='bkg')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    \n",
    "    #ax[i][1].plot(pred_op_mod[ar[i]],color='black',ls='None',marker='.')\n",
    "    ax[i][0].legend('flux')\n",
    "    #ax[i][0].set_ylim(-1.05,0.1)\n",
    "    #ax[i][1].set_ylim(-1.05,0.1)\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "ax[5][0].set_xlabel('Phase')\n",
    "ax[5][1].set_xlabel('Phase')\n",
    "\n",
    "#plt.savefig('present_itsamust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholesome thing.... lets see if this is any better\n",
    "corrects = 0 \n",
    "incorrects = 0\n",
    "cm=[[0,0],[0,0]]\n",
    "pred_arr=[]\n",
    "\n",
    "\n",
    "for i in range(0, len(Xtest)):\n",
    "    tmp=[0,0]\n",
    "    newpl=np.asarray(pred_op[i,:,0])\n",
    "    newfps=np.asarray(pred_op[i,:,1])\n",
    "    #newpl=np.asarray([1 if(el>np.mean(newpl)+np.std(newpl)) else 0 for el in newpl])\n",
    "    #newfps=np.asarray([1 if(el>np.mean(newfps)+np.std(newfps)) else 0 for el in newfps])\n",
    "    if(np.any(Ytest[i,:,0]>0)):\n",
    "        val1 = np.corrcoef(newfps, Ytest[i,:,0])\n",
    "        val2 = np.corrcoef(newpl, Ytest[i,:,0])\n",
    "        if(val2[0,1]>val1[0,1]): \n",
    "            corrects+=1\n",
    "            cm[0][0]+=1\n",
    "            tmp[0]=1\n",
    "        else: \n",
    "            incorrects+=1\n",
    "            cm[1][0]+=1\n",
    "            tmp[1]=1\n",
    "        #print(val1[0,1], val2[0,1])\n",
    "    #fps detect\n",
    "    elif(np.any(Ytest[i,:,1]>0)):\n",
    "        val1 = np.corrcoef(newfps, Ytest[i,:,1])\n",
    "        val2 = np.corrcoef(newpl, Ytest[i,:,1])\n",
    "        if(val1[0,1]>val2[0,1]): \n",
    "            corrects+=1\n",
    "            cm[1][1]+=1\n",
    "            tmp[1]=1\n",
    "        else: \n",
    "            cm[0][1]+=1\n",
    "            incorrects+=1\n",
    "            tmp[0]=1\n",
    "        #print(val1[0,1], val2[0,1])\n",
    "    pred_arr.append(tmp)\n",
    "    #planet detection:\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(corrects, incorrects)\n",
    "print(np.asarray(cm)/np.asarray(cm).sum())\n",
    "print(corrects/(corrects+incorrects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dice_coeff(pred_op[:,:,0:3], np.asarray(Ytest[:,:,0:3], dtype='float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholesome thing.... lets see if this is any better\n",
    "corrects = 0 \n",
    "incorrects = 0\n",
    "cm=[[0,0],[0,0]]\n",
    "pred_arr=[]\n",
    "Ytest - np.asarray(Ytest, dtype='float32')\n",
    "\n",
    "for i in range(0, len(Xtest)):\n",
    "    tmp=[0,0]\n",
    "    newpl=pred_op[i,:,0]\n",
    "    newfps=pred_op[i,:,1]\n",
    "    if(np.any(Ytest[i,:,0]>0)):\n",
    "        val1 = dice_coeff(newfps, np.asarray(Ytest[i,:,0], dtype='float32')).numpy()\n",
    "        val2 = dice_coeff(newpl, np.asarray(Ytest[i,:,0], dtype='float32')).numpy()\n",
    "        if(val2>val1): \n",
    "            corrects+=1\n",
    "            cm[0][0]+=1\n",
    "            tmp[0]=1\n",
    "        else: \n",
    "            incorrects+=1\n",
    "            cm[1][0]+=1\n",
    "            tmp[1]=1\n",
    "            #print(val1, val2,'pl')\n",
    "    #fps detect\n",
    "    if(np.any(Ytest[i,:,1]>0)):\n",
    "        val1 = dice_coeff(newfps, np.asarray(Ytest[i,:,1], dtype='float32')).numpy()\n",
    "        val2 = dice_coeff(newpl, np.asarray(Ytest[i,:,1], dtype='float32')).numpy()\n",
    "        if(val1>val2): \n",
    "            corrects+=1\n",
    "            cm[1][1]+=1\n",
    "            tmp[1]=1\n",
    "        else: \n",
    "            cm[0][1]+=1\n",
    "            incorrects+=1\n",
    "            tmp[0]=1\n",
    "            #print(val1, val2,'fps')\n",
    "    pred_arr.append(tmp)\n",
    "    #planet detection:\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(corrects, incorrects)\n",
    "print(np.asarray(cm)/np.asarray(cm).sum())\n",
    "print(corrects/(corrects+incorrects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=np.asarray([[TestID2[i],Ytest[i].reshape(-1),pred_op[i].reshape(-1),pred_arr[i]] for i in range(len(TestID2))], dtype='object')\n",
    "\n",
    "gc.write_tfr_record('../../training_data/jointanalysis2',net, ['id','true_map','pred_map','pred_class'],\n",
    "    ['b','ar','ar','ar'],['string','bool','float32','bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.MeanIoU(num_classes=3)\n",
    "#tpred=[[[max(x[0], x[1]), x[2]] for x in el] for el in pred_op]\n",
    "#ttest=[[[max(x[0], x[1]), x[2]] for x in el] for el in Ytest]\n",
    "m.update_state(np.around(Ytest[:,:,:]), np.around(pred_op[:,:,:]))\n",
    "m.result().numpy()\n",
    "#print(pred_op[0:10,0:30])\n",
    "#print(generalized_dice_coeff(Ytest,pred_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orthodox threshold method to generate metric\n",
    "class_Ytest=[]\n",
    "class_Ypred=[]\n",
    "for el in pred_op:\n",
    "    pl=np.array(el[:,0]>0.40).sum()\n",
    "    fps=np.array(el[:,1]>0.5 ).sum()\n",
    "    if(pl>fps):class_Ypred.append(0)\n",
    "    else: class_Ypred.append(1)\n",
    "\n",
    "for el in Ytest:\n",
    "    pl=np.array(el[:,0]>0.30).sum()\n",
    "    fps=np.array(el[:,1]>0.30).sum()\n",
    "    if(pl>fps):class_Ytest.append(0)\n",
    "    else: class_Ytest.append(1)\n",
    "    #4800,2\n",
    "#[print(class_Ypred[i],class_Ytest[i]) for i in range(0,len(class_Ytest))]\n",
    "\n",
    "cm=confusion_matrix(class_Ytest,class_Ypred)\n",
    "print(cm/cm.sum())\n",
    "print((cm[0,0]+cm[1,1])/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pixel accuracy ratio\n",
    "ptest=[np.argmax(np.array([el[0],el[1],el[2]])) for el in np.reshape(pred_op,(len(pred_op)*4000,3))]\n",
    "ppred=[np.argmax(el) for el in np.reshape(Ytest,(len(Ytest)*4000,3))]\n",
    "\n",
    "cm=confusion_matrix(ptest, ppred)\n",
    "print(cm/cm.sum())\n",
    "print((cm[0,0]+cm[1,1]+cm[2,2])/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "\n",
    "fpr , tpr, thes= roc_curve(class_Ytest,class_Ypred,pos_label=1)\n",
    "print(auc(fpr,tpr))\n",
    "print(fpr.shape,tpr.shape,thes.shape)\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr,tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding periodicity and all\n",
    "from scipy.signal import argrelextrema,find_peaks\n",
    "note=40\n",
    "x=pred_op[note,:,1]\n",
    "x2=pred_op[note,:,0]\n",
    "y=Ytest[note,:,1]\n",
    "y2=Ytest[note,:,0]\n",
    "h=(np.median(x)+2*np.std(x))\n",
    "h2=(np.median(x2)+2*np.std(x2))\n",
    "print(h,h2)\n",
    "kernel_size = 22\n",
    "kernel = np.ones(kernel_size) / kernel_size\n",
    "#x=np.convolve(x, kernel, mode='same')\n",
    "#Xtrain = [np.convolve(Xtrain[i], kernel, mode='same') for i in range(IP)]\n",
    "#Xtest = [np.convolve(Xtest[i], kernel, mode='same') for i in range(TEST)]\n",
    "peaksf, _ = find_peaks(x, height=h,distance=20)\n",
    "peaksp, _ = find_peaks(x2, height=h2,distance=20)\n",
    "peaksft,_ = find_peaks(y, height=0)\n",
    "peakspt,_ = find_peaks(y2, height=0)\n",
    "print(len(peaksf),len(peaksft),len(peaksp),len(peakspt))\n",
    "\n",
    "plwh=np.where(y==1)[0]\n",
    "fpswh=np.where(y2==1)[0]\n",
    "\n",
    "print(\"checkalg\",np.mean(x[peaksf]),np.std(x[peaksf]),np.mean(x2[peaksp]),np.std(x2[peaksp]))\n",
    "\n",
    "plt.plot(y,color='blue')\n",
    "plt.plot(y2,color='red')\n",
    "plt.plot(x,color='green')\n",
    "plt.plot(x2,color='black')\n",
    "\n",
    "plt.plot(peaksf,x[peaksf],color=\"blue\",marker=\".\",ls='None')\n",
    "#plt.plot(peaksft,y[peaksft],color=\"blue\",marker=\".\",ls='None')\n",
    "plt.plot(peaksp,x2[peaksp],color=\"red\",marker=\".\",ls='None')\n",
    "#plt.plot(peakspt,y2[peakspt],color=\"red\",marker=\".\",ls='None')\n",
    "\n",
    "\n",
    "#plt.xlim(0,2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new code to get periodicity... idk if its gonna be worthwile but lets see\n",
    "#get test data\n",
    "class_Ytest=[]\n",
    "for el in Ytest:\n",
    "    xpl=el[:,0]\n",
    "    xfps=el[:,1]\n",
    "    plp, _ = find_peaks(xpl, height=0)\n",
    "    pfps, _ = find_peaks(xfps, height=0)\n",
    "    class_Ytest.append([len(plp),len(pfps)])\n",
    "\n",
    "class_Ypred=[]\n",
    "pred_conf=[]\n",
    "for el in pred_op:\n",
    "    pl = el[:,0]\n",
    "    hpl = np.median(pl)+0.5*np.std(pl)\n",
    "    fps = el[:,1]\n",
    "    hfps = np.median(fps)+0.5*np.std(fps)\n",
    "    plp, _ = find_peaks(pl, height=hpl,distance=5)\n",
    "    fpsp, _ = find_peaks(fps, height=hfps,distance=5) \n",
    "    if(len(plp>0)): \n",
    "        val1=np.mean(pl[plp])\n",
    "        std1=np.std(pl[plp])\n",
    "    else: \n",
    "        val1=0\n",
    "        std1=0\n",
    "    if(len(fpsp)>0): \n",
    "        val2=np.mean(fps[fpsp])\n",
    "        std2=np.std(fps[fpsp])\n",
    "    else: \n",
    "        val2=0\n",
    "        std2=0\n",
    "          \n",
    "    class_Ypred.append([val1,val2])\n",
    "    pred_conf.append([std1,std2])\n",
    "    \n",
    "\n",
    "print(np.asarray(class_Ypred).shape,np.asarray(class_Ytest).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pred=[]\n",
    "n_test=[]\n",
    "good_preds=0\n",
    "for i in range(len(class_Ypred)):\n",
    "    #planet\n",
    "    ind=np.argmax(class_Ypred[i])\n",
    "    if(class_Ypred[i][ind]-1*pred_conf[i][ind] > class_Ypred[i][1-ind] + 1*pred_conf[i][1-ind]): good_preds+=1\n",
    "    #else: continue\n",
    "    n_pred.append(ind)\n",
    "    if(class_Ytest[i][0]>0 and class_Ytest[i][1]>0): n_test.append(np.argmax(class_Ypred[i]))\n",
    "    elif(class_Ytest[i][0]>0): n_test.append(0)\n",
    "    else:  n_test.append(1)\n",
    "\n",
    "cm=confusion_matrix(np.asarray(n_test),np.asarray(n_pred))\n",
    "print(good_preds)\n",
    "print(cm/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some chummy plots\n",
    "#ar=[0,5,6,11,13]\n",
    "#ar=np.random.randint(0,len(Xtest),size=6)\n",
    "#ar = [5,20,10,7,15, 39]\n",
    "ar = [1,17,21,4]\n",
    "#ar=[36,37,38,39,40]\n",
    "#ar=[194,201,202,203,206]\n",
    "\n",
    "Ytest = np.asarray(Ytest, dtype='float32')\n",
    "Xtest = np.asarray(Xtest, dtype='float32')\n",
    "fig,ax=plt.subplots(4,1,figsize=(10,12))\n",
    "plt.style.use('seaborn-bright')\n",
    "ax[0].set_title('Sample segmentation map', size=20)\n",
    "ind=np.arange(0,4000)\n",
    "\n",
    "for i in range(0,4):\n",
    "    plotlab=pred_op[ar[i],:,:]\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]/1.2]) for el in plotlab])\n",
    "    pcounts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in Ytest[ar[i],:,:]])\n",
    "    #if(i==0): counts=np.asarray([np.argmax([el[0],el[1],el[2]/1.4]) for el in plotlab])\n",
    "    #elif(i==4): counts=np.asarray([np.argmax([el[0],el[1],el[2]/2]) for el in plotlab])\n",
    "\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "    ppl = np.where(pcounts==0)[0]\n",
    "    pfps = np.where(pcounts==1)[0]\n",
    "    m = min(Xtest[ar[i]])*1.5\n",
    "    mx= max(Xtest[ar[i]])*1.5\n",
    "    ax[i].plot(ind[bkg],Xtest[ar[i],bkg],color='#aaaaaa',marker='.',ls='None',label='bkg')\n",
    "    ax[i].plot(ind[fps],Xtest[ar[i],fps],color='#d2601a',marker='.',ls='None',label='fps') \n",
    "    ax[i].plot(ind[pl],Xtest[ar[i],pl],color='#1d3c45',marker='.',ls='None',label='pl')\n",
    "    if(len(ppl)>0): ax[i].vlines(x=ppl,ymin=m*2, ymax=mx*2, alpha=0.5, color='#6699CC')\n",
    "    if(len(pfps)>0): ax[i].vlines(x=pfps,ymin=m*2, ymax=mx*2, alpha=0.5, color='#e4d96f')\n",
    "    ax[i].set_ylim(m,mx)\n",
    "    ax[i].set_xlim(1000,3000)\n",
    "    #ax[i].legend()\n",
    "    ax[i].set_ylabel('flux', size=15)\n",
    "    \n",
    "    #ax[i][0].legend()\n",
    "ax[3].set_xlabel('Time', size=15)\n",
    "#ax[3].set_ylim(-0.003,0.002)\n",
    "plt.savefig('fprez_result2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another\n",
    "class_Ypred=[]\n",
    "class_Ytest=[]\n",
    "for i in range(0,len(pred_op)):\n",
    "    plotlab=pred_op[i,:,:]\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]/2]) for el in plotlab])\n",
    "    pl=(counts==0).sum()\n",
    "    fps=(counts==1).sum()\n",
    "    #bkg=(counts==2).sum()\n",
    "    if(pl==0 and fps ==0): continue\n",
    "    if(pl>fps): class_Ypred.append(0)\n",
    "    else: class_Ypred.append(1)\n",
    "\n",
    "    tpl=Ytest[i,:,0].sum()\n",
    "    tfps=Ytest[i,:,1].sum()\n",
    "    if(tpl>tfps): class_Ytest.append(0)\n",
    "    else: class_Ytest.append(1)\n",
    "\n",
    "cm=confusion_matrix(np.asarray(class_Ytest),np.asarray(class_Ypred))\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate transits by grouping:\n",
    "#convNN.load_weights('thisisagoodone.h5')\n",
    "#pred_op=convNN.predict(np.array(Xtestf))\n",
    "\n",
    "maxarr_pred = [[ np.argmax(np.asarray([x[0],x[1],x[2]])) for x in el] for el in pred_op]\n",
    "maxarr_predpl  =[[x==0 for x in el] for el in maxarr_pred]\n",
    "maxarr_predfps  =[[x==1 for x in el] for el in maxarr_pred]\n",
    "#maxarr_testpl = [[np.argmax([el[2],el[0]]) for x in el] for el in Xtest]\n",
    "#maxarr_predfps = [[np.argmax(np.asarray([x[2],x[1]])) for x in el] for el in pred_op]\n",
    "#maxarr_testfps = [[np.argmax([el[2],el[1]]) for x in el] for el in Xtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(maxarr_predpl).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema,find_peaks\n",
    "\n",
    "corrects=np.asarray([[0,0],[0,0]])\n",
    "newc = 0\n",
    "newinc = 0\n",
    "for i in range(0,len(pred_op)):\n",
    "    tick = 0\n",
    "    ntick = 0\n",
    "    plp, _ = find_peaks(maxarr_predpl[i], height=0.0,distance=20) \n",
    "    for m in plp:\n",
    "        if(Ytest[i,m,0]>0.5): \n",
    "            corrects[0,0]+=1\n",
    "            tick+=1\n",
    "        if(Ytest[i,m,1]>0.5): \n",
    "            corrects[1,0]+=1\n",
    "            ntick+=1\n",
    "        #if(Ytrain[i,m,2]>0.5): corrects[2,0]+=1\n",
    "        #else: corrects[2,0]+=1\n",
    "    fpsp, _ = find_peaks(maxarr_predfps[i], height=0.0,distance=20)\n",
    "    for m in fpsp:\n",
    "        if(Ytest[i,m,1]>0.5): \n",
    "            corrects[1,1]+=1\n",
    "            tick+=1\n",
    "        if(Ytest[i,m,0]>0.5): \n",
    "            corrects[0,1]+=1\n",
    "            ntick+=1\n",
    "        #if(Ytrain[i,m,2]>0.1): corrects[0,2]+=1\n",
    "        #else: corrects[0,2]+=1\n",
    "    if(tick>ntick): newc+=1\n",
    "    else: newinc+=1\n",
    "\n",
    "print(corrects/corrects.sum())\n",
    "print(newinc,newc, newinc/(newc+newinc))\n",
    "print((corrects[0,0]+corrects[1,1])/corrects.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import ascii\n",
    "from astropy.table import Table\n",
    "TestID2 = [TestID[i].numpy() for i in range(0,len(TestID))]\n",
    "TestID2 = [str(TestID2[i])[2:11] for i in range(0,len(TestID2))]\n",
    "print(TestID2[2:10])\n",
    "Ytest = np.asarray(Ytest, dtype = 'float32')\n",
    "\n",
    "for i in range(0,len(Xtest)):\n",
    "    data = Table()\n",
    "    data['RAW'] = Xtest[i]\n",
    "    data['PL_MAP'] = Ytest[i,:,0]\n",
    "    data['FPS_MAP'] = Ytest[i,:,1]\n",
    "    data['BKG_MAP'] = Ytest[i,:,2]\n",
    "    data['PRED_PL_MAP'] = pred_op[i,:,0]\n",
    "    data['PRED_FPS_MAP'] = pred_op[i,:,1]\n",
    "    data['PRED_BKG_MAP'] = pred_op[i,:,2]\n",
    "    ascii.write(data, '../../processed_directories/sem_seg_op/'+str(np.asarray(TestID2)[i]), overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestID2 = [str(TestID[i])[2:12] for i in range(0,len(TestID))]\n",
    "#print(TestID2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are setting up an expanded semantic segmentation problem\n",
    "import os\n",
    "from scipy.signal import find_peaks\n",
    "PATHRV = '../../processed_directories/expand_test_and_noise_rec/'\n",
    "PATHAV = '../../processed_directories/sem_seg_av_test/'\n",
    "FOUT = '../../processed_directories/expand_test_result_av/'\n",
    "\n",
    "def peak_cm(pred_op, true_op):\n",
    "  maxarr_pred = np.argmax(pred_op,axis=-1)\n",
    "  maxarr_predpl  = np.asarray(maxarr_pred==0,dtype='int')\n",
    "  maxarr_predfps  = np.asarray(maxarr_pred==1, dtype='int')\n",
    "\n",
    "  corrects=np.asarray([[0,0],[0,0]])\n",
    "  peak_pl=[]\n",
    "  peak_fps=[]\n",
    "  for i in range(0,len(pred_op)):\n",
    "      plp, _ = find_peaks(maxarr_predpl[i], height=0.3,distance=20) \n",
    "      peak_pl.append(plp)\n",
    "      for m in plp:\n",
    "          if(true_op[i,m,0]>0.5): corrects[0,0]+=1\n",
    "          #elif(true_op[i,m,1]>0.5): corrects[1,0]+=1\n",
    "          else: corrects[1,0]+=1\n",
    "      fpsp, _ = find_peaks(maxarr_predfps[i], height=0.3,distance=20)\n",
    "      peak_fps.append(fpsp)\n",
    "      for m in fpsp:\n",
    "          if(true_op[i,m,1]>0.1): corrects[1,1]+=1\n",
    "          else: corrects[0,1]+=1\n",
    "\n",
    "  #print(corrects/corrects.sum())\n",
    "  #print((corrects[0,0]+corrects[1,1])/corrects.sum())\n",
    "  #print(corrects[0,0],corrects[1,1])\n",
    "  corrects = corrects.reshape(-1)\n",
    "  return(corrects,peak_pl,peak_fps)\n",
    "\n",
    "#load model or weights...\n",
    "model = keras.models.load_model('newtests.h5',custom_objects={'focal_dice_loss': focal_dice_loss})\n",
    "#convNN.load_model('epic_one.h5')\n",
    "\n",
    "test_dir = os.listdir(PATHAV)\n",
    "for el in test_dir :\n",
    "  test1_X, test1_Y= gc.read_tfr_record(PATHAV+el[:9],['input','mask'],['ar','ar'],[tf.float32,tf.bool])\n",
    "  \n",
    "  test1_X = np.asarray(test1_X, dtype='float32')\n",
    "  test1_Y = np.asarray(test1_Y, dtype='float32')\n",
    "  test1_Y = np.reshape(test1_Y,(len(test1_X),4000,3)) \n",
    "\n",
    "  mastermed=np.median(test1_X)\n",
    "  masterstd=np.std(test1_X)\n",
    "  #test1_X=np.asarray([(row-mastermed)/(-row[np.argmin(row)]+masterstd) for row in test1_X])\n",
    "  test1_X=np.asarray([0.5*(np.tanh(0.1*(row - mastermed)/masterstd)) for row in test1_X])\n",
    "\n",
    "  test_pred = model.predict(test1_X)\n",
    "\n",
    "  print(el)\n",
    "  np.reshape(test1_Y,(len(test1_Y),12000))\n",
    "  np.reshape(test_pred,(len(test_pred),12000))\n",
    "\n",
    "  net=np.asarray([[test1_X[i],test1_Y[i],test_pred[i],mastermed,masterstd] for i in range(0,len(test1_X))])\n",
    "  gc.write_tfr_record(FOUT+el, net, ['input', 'true_map', 'pred_map', 'scale_median', 'scale_std'],['ar','ar','ar','fl','fl'],\n",
    "    ['float32','bool', 'float32','float32','float32'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

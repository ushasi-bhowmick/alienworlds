{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to improve the NN\n",
    "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.loadtxt('training_data/Xtrain_seg_mask.csv',delimiter=',')\n",
    "Y_train=np.loadtxt('training_data/Ytrain_seg_mask.csv',delimiter=',')\n",
    "\n",
    "Y_train=Y_train.reshape(len(Y_train),4800,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in X_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest= train_test_split(X_train, Y_train, test_size=0.2)\n",
    "print(Xtrain.shape,Ytrain.shape,Xtest.shape,Ytest.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "fig,ax=plt.subplots(10,2,figsize=(10,20))\n",
    "for i in range(0,10):\n",
    "    ax[i][0].plot(Xtrain[vararr[i]])\n",
    "    ax[i][1].plot(Ytrain[vararr[i],:,0])\n",
    "    ax[i][1].plot(Ytrain[vararr[i],:,1])\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    sh = tf.shape(y_true)\n",
    "    y_true_int = tf.reshape(y_true, [sh[0], sh[1]*sh[2]])\n",
    "    y_pred_int = tf.reshape(y_pred, [sh[0], sh[1]*sh[2]])\n",
    "    w = 1 - tf.reduce_sum(y_true,(1,2)) / 9600.\n",
    "    w = tf.reshape(w, [len(w),1])\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(tf.multiply(y_true_int,w), [-1])\n",
    "    y_pred_f = tf.reshape(tf.multiply(y_pred_int,w), [-1])\n",
    "    #y_pred_int_f = tf.reshape(tf.matmul(w, y_pred_int), [-1])\n",
    "    #y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f_2 = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f_2)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) +  tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred) + generalized_dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(Ytest,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    w=tf.cast(w,tf.float32)\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE=6000\n",
    "#add hidden layers\n",
    "conv_ip = keras.layers.Input(shape=(IMG_SIZE,),name='Input')\n",
    "xi=keras.layers.Reshape((IMG_SIZE, 1), input_shape=(IMG_SIZE,),name='reshape_1')(conv_ip)\n",
    "#xi=keras.layers.Cropping1D(cropping=(100, 100))(xi)\n",
    "xi=keras.layers.BatchNormalization()(xi)\n",
    "\n",
    "x1=keras.layers.Conv1D(16,kernel_size=5,strides=3,activation='tanh',padding='same',name='second_conv16_5')(xi)\n",
    "x1=keras.layers.Conv1D(16,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv16_5')(x1)\n",
    "x1=keras.layers.BatchNormalization()(x1)\n",
    "c1=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_1')(x1)   #size 1000\n",
    "\n",
    "x2=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv32_5')(c1)\n",
    "x2=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv32_5')(x2)\n",
    "x2=keras.layers.BatchNormalization()(x2)\n",
    "c2=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_2')(x2)    #size 500\n",
    "\n",
    "ao = keras.layers.Conv1DTranspose(filters=32, kernel_size=3, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_1\")(c2)\n",
    "ao = keras.layers.Conv1DTranspose(filters=32, kernel_size=3, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_2\")(ao)\n",
    "ao=keras.layers.Flatten()(ao)\n",
    "ao=keras.layers.Dense(200,activation='tanh',name='adding_this')(ao)\n",
    "\n",
    "x3=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv64_5')(c2)\n",
    "x3=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv64_5')(x3)\n",
    "x3=keras.layers.BatchNormalization()(x3)\n",
    "x3=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_3')(x3)    #size 250\n",
    "\n",
    "x3=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv128_5')(x3)\n",
    "x3=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv128_5')(x3)\n",
    "x3=keras.layers.BatchNormalization()(x3)\n",
    "x3=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_4')(x3)    #size 125\n",
    "\n",
    "\n",
    "x3 = keras.layers.Conv1DTranspose(filters=64, kernel_size=3, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_1\")(x3)\n",
    "x3 = keras.layers.Conv1DTranspose(filters=64, kernel_size=3, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_2\")(x3)\n",
    "\n",
    "x3 = keras.layers.Conv1DTranspose(filters=32, kernel_size=3, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_3\")(x3)\n",
    "x3 = keras.layers.Conv1DTranspose(filters=32, kernel_size=3, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_4\")(x3)\n",
    "tx2 = keras.layers.Concatenate(axis=1)([c2,x3]) #size=400\n",
    "tx2 = keras.layers.Conv1D(32,kernel_size=3,strides=2,activation='tanh',padding='same',name='maxpoolt_2')(tx2)\n",
    "tx2=keras.layers.BatchNormalization()(tx2)\n",
    "\n",
    "#tx2 = keras.layers.Conv1DTranspose(filters=16, kernel_size=5, activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_5\")(tx2)\n",
    "#tx2 = keras.layers.Conv1DTranspose(filters=16, kernel_size=5  , activation='tanh', padding=\"same\", strides=1, name=\"decoder_conv_tran_6\")(tx2)\n",
    "\n",
    "tx2=keras.layers.Flatten()(x3)\n",
    "tx2=keras.layers.Dense(128,activation='relu')(tx2)\n",
    "tx2=keras.layers.Dense(128,activation='relu')(tx2)\n",
    "tx2=keras.layers.Dense(1,activation='relu')(tx2)\n",
    "conv_op=keras.layers.Dense(2,activation='softmax',name='missing_this')(tx2)\n",
    "\n",
    "\n",
    "\n",
    "#convNN = keras.Model(inputs=[conv_ip,conv_ipl], outputs=conv_op,name='Convolutional_NN')\n",
    "convNN = keras.Model(inputs=conv_ip, outputs=[ao,conv_op],name='Convolutional_NN')\n",
    "\n",
    "\n",
    "convNN.summary()\n",
    "convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.000005), \n",
    "    loss={'missing_this': 'categorical_crossentropy','adding_this':keras.losses.Huber(delta=0.1)},metrics={'missing_this':'accuracy'})\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history=convNN.fit(np.array(Xtrain),[np.array(Itrain),np.array(Ytrain)], batch_size=64, epochs=70 , verbose=1 ,\n",
    "    shuffle=True,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('present_segment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#convNN.load_weights('long_hard_battle_eq.h5')\n",
    "pred_op=convNN.predict(np.array(Xtest))\n",
    "\n",
    "fig,ax=plt.subplots(6,2,figsize=(15,15))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output')\n",
    "ar=np.random.randint(len(Xtest),size=10)\n",
    "#ar=[0,1,2,3,8,7]\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,6):\n",
    "    ax[i][0].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='data')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,0],color='black',ls='None',marker='.',label='fps')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,1],color='green',ls='None',marker='.',label='pl')\n",
    "    \n",
    "    \n",
    "    ax[i][1].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='data')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,0],color='black',ls='None',marker='.',label='fps')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,1],color='green',ls='None',marker='.',label='pl')\n",
    "    \n",
    "    #ax[i][1].plot(pred_op_mod[ar[i]],color='black',ls='None',marker='.')\n",
    "    ax[i][0].legend('flux')\n",
    "    ax[i][0].set_ylim(-1.05,0.05)\n",
    "    ax[i][1].set_ylim(-1.05,0.05)\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "ax[5][0].set_xlabel('Phase')\n",
    "ax[5][1].set_xlabel('Phase')\n",
    "\n",
    "#plt.savefig('present_itsamust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_Ytest=[]\n",
    "class_Ypred=[]\n",
    "for el in pred_op:\n",
    "    pl=np.array(el[:,0]>0.20).sum()\n",
    "    fps=np.array(el[:,1]>0.35).sum()\n",
    "    if(pl>fps):class_Ypred.append(1)\n",
    "    else: class_Ypred.append(0)\n",
    "\n",
    "for el in Ytest:\n",
    "    pl=np.array(el[:,0]>0.20).sum()\n",
    "    fps=np.array(el[:,1]>0.35).sum()\n",
    "    if(pl>fps):class_Ytest.append(1)\n",
    "    else: class_Ytest.append(0)\n",
    "    #4800,2\n",
    "#[print(class_Ypred[i],class_Ytest[i]) for i in range(0,len(class_Ytest))]\n",
    "\n",
    "cm=confusion_matrix(class_Ytest,class_Ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different approach to CM\n",
    "class_Ytest=[]\n",
    "class_Ypred=[]\n",
    "for el in pred_op:\n",
    "    pl=np.array(el[:,0]>0.20).sum()\n",
    "    fps=np.array(el[:,1]>0.30).sum()\n",
    "    if(pl>3*fps):class_Ypred.append([1,0])\n",
    "    elif(fps>3*pl):class_Ypred.append([0,1])\n",
    "    else: class_Ypred.append([1,1])\n",
    "\n",
    "for el in Ytest:\n",
    "    pl=np.array(el[:,0]>0.20).sum()\n",
    "    fps=np.array(el[:,1]>0.30).sum()\n",
    "    if(pl*fps):class_Ytest.append([1,1])\n",
    "    elif(pl):class_Ytest.append([1,0])\n",
    "    else: class_Ytest.append([0,1])\n",
    "    #4800,2\n",
    "#[print(class_Ypred[i],class_Ytest[i]) for i in range(0,len(class_Ytest))]\n",
    "\n",
    "\n",
    "cm=confusion_matrix(np.asarray(class_Ytest)[:,0],np.asarray(class_Ypred)[:,0])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "\n",
    "fpr , tpr, thes= roc_curve(class_Ytest,class_Ypred,pos_label=1)\n",
    "print(auc(fpr,tpr))\n",
    "print(fpr.shape,tpr.shape,thes.shape)\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr,tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train=convNN.predict(np.array(Xtrain))\n",
    "mask_test=convNN.predict(np.array(Xtest))\n",
    "mask_train=np.asarray([[1 if max(el[0],el[1])>0.3 else 0 for el in x ] for x in mask_train])\n",
    "mask_test=np.asarray([[1 if max(el[0],el[1])>0.3 else 0 for el in x ] for x in mask_test])\n",
    "Itrain=Xtrain*mask_train\n",
    "Itest=Xtest*mask_test\n",
    "cltrain=[]\n",
    "cltest=[]\n",
    "\n",
    "for el in Ytrain:\n",
    "    pl=np.array(el[:,0]>0.5).sum()\n",
    "    fps=np.array(el[:,1]>0.5).sum()\n",
    "    if(pl>fps):cltrain.append([1,0])\n",
    "    else: cltrain.append([0,1])\n",
    "\n",
    "for el in Ytest:\n",
    "    pl=np.array(el[:,0]>0.5).sum()\n",
    "    fps=np.array(el[:,1]>0.5).sum()\n",
    "    if(pl>fps):cltest.append([1,0])\n",
    "    else: cltest.append([0,1])\n",
    "\n",
    "print(np.asarray(Itrain).shape,np.asarray(cltrain).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test=convNN.predict(np.array(Xtest))\n",
    "mask_test=np.asarray([[1 if max(el[0],el[1])>0.5 else 0 for el in x ] for x in mask_test])\n",
    "mask_test_true=np.asarray([[1 if max(el[0],el[1])>0.5 else 0 for el in x ] for x in Ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothbin(arr):\n",
    "    for m in range(5,len(arr)-5):\n",
    "        if(arr[m]==0):\n",
    "            if(arr[m-1]==-1 and arr[m+1]==-1): \n",
    "                arr[m]=-1\n",
    "            elif(arr[m-2]==-1 and arr[m+2]==-1): \n",
    "                arr[m]=-1\n",
    "            elif(arr[m-3]==-1 and arr[m+3]==-1): \n",
    "                arr[m]=-1\n",
    "            elif(arr[m-4]==-1 and arr[m+4]==-1): \n",
    "                arr[m]=-1\n",
    "            elif(arr[m-5]==-1 and arr[m+5]==-1): \n",
    "                arr[m]=-1\n",
    "    return arr\n",
    "\n",
    "fig,ax=plt.subplots(4,1,figsize=(10,10))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('sample reconstructions')\n",
    "#ar=np.random.randint(len(Xtest),size=5)\n",
    "ar=[0,7,2,3]\n",
    "\n",
    "for i in range(0,4):\n",
    "    ax[i].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='raw')\n",
    "    ax[i].plot(mask_test_true[ar[i]]*min(Xtest[ar[i]]),color='blue',ls='-',label='generated')\n",
    "    ax[i].plot(smoothbin(mask_test[ar[i]])*min(Xtest[ar[i]])*0.8,color='black',label='predict')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_ylabel('flux')\n",
    "    #ax[i][1].plot(pred_op_mod[ar[i]],color='black',ls='None',marker='.')\n",
    "    \n",
    "    #ax[i][0].legend()\n",
    "ax[3].set_xlabel('Time')\n",
    "plt.savefig('present_rec.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN.save_weights('long_hard_battle_eq.h5')\n",
    "convNN.save('Model_long_hard_battle_eq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "fig,ax=plt.subplots(10,1,figsize=(10,20))\n",
    "for i in range(0,10):\n",
    "    ax[i].plot( Itrain[vararr[i]][2000:3000],label=Ytrain[vararr[i]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE=4800\n",
    "#add hidden layers\n",
    "class_ip = keras.layers.Input(shape=(IM_SIZE,),name='Input')\n",
    "\n",
    "m=keras.layers.Reshape((IM_SIZE, 1), input_shape=(IM_SIZE,),name='reshape_1')(class_ip)\n",
    "m=keras.layers.BatchNormalization()(m)\n",
    "m=keras.layers.Conv1D(16,kernel_size=3,strides=2,activation='tanh',padding='same',name='second_conv16_5')(m)\n",
    "m=keras.layers.Conv1D(16,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv16_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_1')(m)\n",
    "m=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv32_5')(m)\n",
    "m=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv32_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_2')(m)\n",
    "m=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv64_5')(m)\n",
    "m=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv64_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_4')(m)\n",
    "m=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv128_5')(m)\n",
    "m=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv128_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_3')(m)\n",
    "m=keras.layers.Flatten(name='flat_1')(m)\n",
    "m=keras.layers.Dense(256,name='dense_layer_3',activation='relu')(m)\n",
    "m=keras.layers.Dense(256,name='dense_layer_5',activation='relu')(m)\n",
    "m=keras.layers.Dense(256,name='dense_layer_7',activation='relu')(m)\n",
    "m=keras.layers.Dense(1,name='dense_layer_4',activation='relu')(m)\n",
    "class_op=keras.layers.Dense(2,name='dense_layer_6',activation='softmax')(m)\n",
    "\n",
    "\n",
    "#convNN = keras.Model(inputs=[conv_ip,conv_ipl], outputs=conv_op,name='Convolutional_NN')\n",
    "classify = keras.Model(inputs=class_ip, outputs=class_op,name='Classifier_NN')\n",
    "\n",
    "\n",
    "classify.summary()\n",
    "classify.compile(optimizer=keras.optimizers.Adam(learning_rate=0.000005), loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "#history=convNN.fit([np.array(Xtrain),np.array(Xtrainl)],np.array(Ytrain), batch_size=64, epochs=40, verbose=VERBOSE, validation_split=0.12,callbacks=[es_callback])\n",
    "history2=classify.fit(np.asarray(Itrain),np.asarray(cltrain), batch_size=64, epochs=40 , verbose=1 , shuffle=True,validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy\\n Can the NN work with the raw chunk?')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#  plt.savefig('rec_manual_mask.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = classify.evaluate(np.array(Itest), np.array(cltest))\n",
    "print('Test accuracy:', test_acc)\n",
    "Ypred_raw=classify.predict(np.array(Itest))\n",
    "Ypred=np.argmax(Ypred_raw, axis=1)\n",
    "Ytest_new=np.argmax(cltest,axis=1)\n",
    "cm = confusion_matrix(Ytest_new, Ypred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import GetLightcurves as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 0.00001\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection +smooth) / (tf.reduce_sum(y_true_f) +  tf.reduce_sum(y_pred_f) +smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def weightFocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA): \n",
    "    w = tf.reduce_sum(targets,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 - w\n",
    "    #w = 1 / (w  + 0.00001)\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)   \n",
    "    \n",
    "    #inputs = K.flatten(inputs)\n",
    "    #targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(w*K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def log_cosh_dice_loss(y_true, y_pred):\n",
    "        x = generalized_dice_coeff(y_true, y_pred)\n",
    "        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)*0.5 + log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    #w = 1 / (w  + 0.00001)\n",
    "    w = 1 - w\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred):\n",
    "    loss = FocalLoss(y_true, y_pred) + 1.5*dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "#print(weightFocalLoss(Ytest, np.ones((len(Ytest),4000,3),dtype='float32')))\n",
    "#check score\n",
    "def corr(y_true, y_pred):\n",
    "    #wholesome thing.... lets see if this is any better\n",
    "    corrects = 0 \n",
    "    incorrects = 0\n",
    "    cm=[[0,0],[0,0]]\n",
    "    pred_arr=[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        tmp=[0,0]\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        #newpl=np.asarray([1 if(el>np.mean(newpl)+np.std(newpl)) else 0 for el in newpl])\n",
    "        #newfps=np.asarray([1 if(el>np.mean(newfps)+np.std(newfps)) else 0 for el in newfps])\n",
    "        if(np.any(y_true[i,:,0]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,0])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,0])\n",
    "            if(val2[0,1]>val1[0,1]): \n",
    "                corrects+=1\n",
    "                cm[0][0]+=1\n",
    "                tmp[0]=1\n",
    "            else: \n",
    "                incorrects+=1\n",
    "                cm[1][0]+=1\n",
    "                tmp[1]=1\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        #fps detect\n",
    "        if(np.any(y_true[i,:,1]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,1])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,1])\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                corrects+=1\n",
    "                cm[1][1]+=1\n",
    "                tmp[1]=1\n",
    "            else: \n",
    "                cm[0][1]+=1\n",
    "                incorrects+=1\n",
    "                tmp[0]=1\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        pred_arr.append(tmp)\n",
    "        #planet detection:\n",
    "    \n",
    "    print(corrects, incorrects)\n",
    "    print(np.asarray(cm)/np.asarray(cm).sum())\n",
    "    print(corrects/(corrects+incorrects))\n",
    "\n",
    "def corrarr(y_true, y_pred):\n",
    "    checkarr=[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        if(np.any(y_true[i,:,0]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,0])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,0])\n",
    "            if(val2[0,1]>val1[0,1]):\n",
    "                checkarr.append(0)\n",
    "            else: \n",
    "                checkarr.append(1)\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        #fps detect\n",
    "        elif(np.any(y_true[i,:,1]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,1])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,1])\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                checkarr.append(0)\n",
    "            else: \n",
    "                checkarr.append(1)\n",
    "    return(checkarr)\n",
    "\n",
    "def split_by_snr(X, Y, thres):\n",
    "    idscl=[i for i in range(0,len(X)) if(np.abs(np.mean(X[i])/np.std(X[i]))>thres)]\n",
    "    idsns=[i for i in range(0,len(X)) if(np.abs(np.mean(X[i])/np.std(X[i]))<thres)]\n",
    "    Xnoise=np.asarray([X[i] for i in idsns])\n",
    "    Xcl=np.asarray([X[i] for i in idscl])\n",
    "    Ynoise=np.asarray([Y[i] for i in idsns])\n",
    "    Ycl=np.asarray([Y[i] for i in idscl])\n",
    "    return(Xnoise, Xcl, Ynoise, Ycl)\n",
    "\n",
    "\n",
    "\n",
    "def net_pred(train, convs):\n",
    "    predspl=[]\n",
    "    predsfps=[]\n",
    "    predsbkg=[]\n",
    "    for el in convs:\n",
    "        pred=el.predict(train)\n",
    "        predspl.append(pred[:,:,0])\n",
    "        predsfps.append(pred[:,:,1])\n",
    "        predsbkg.append(pred[:,:,2])\n",
    "    pred\n",
    "    pred_op=[]\n",
    "    predspl=np.asarray(predspl)\n",
    "    predsfps=np.asarray(predsfps)\n",
    "    predsbkg=np.asarray(predsbkg)\n",
    "    for i in range(0,len(train)):\n",
    "        one1=np.max(predspl[:,i,:], axis=0).reshape(-1)\n",
    "        one2=np.max(predsfps[:,i,:], axis=0).reshape(-1)\n",
    "        one3=np.min(predsbkg[:,i,:], axis=0).reshape(-1)\n",
    "        net=[[one1[i], one2[i],one3[i]] for i in range(0,len(one1))]\n",
    "        pred_op.append(net)\n",
    "    '''\n",
    "    for i in range(0,len(train)):\n",
    "        one=np.zeros((4000,3))\n",
    "        for j in range(0,4000):\n",
    "            one[j][0]=max(predspl[:,i,j])\n",
    "            one[j][1]=max(predsfps[:,i,j])\n",
    "            one[j][2]=min(predsbkg[:,i,j])\n",
    "        pred_op.append(one)'''\n",
    "    return(np.asarray(pred_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=4000\n",
    "def build_model(imsz):\n",
    "    conv_ip = keras.layers.Input(shape=(imsz,),name='Input')\n",
    "    xi=keras.layers.Reshape((imsz, 1), input_shape=(imsz,),name='reshape_1')(conv_ip)\n",
    "    #xi=keras.layers.Cropping1D(cropping=(100, 100))(xi)\n",
    "    xi=keras.layers.BatchNormalization()(xi)\n",
    "\n",
    "    x1=keras.layers.SeparableConv1D(16,kernel_size=3,activation='tanh',name='1st16_5')(xi)  #3998, 32\n",
    "    c1=keras.layers.SeparableConv1D(16,kernel_size=3,strides=1,activation='tanh',name='2nd16_3')(x1)  #3996, 32\n",
    "\n",
    "    x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "    x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1')(x2)  #1998, 64\n",
    "    #x2=keras.layers.Conv1D(16,kernel_size=1,strides=2,name='maxpool_1')(x2)  #1998, 64\n",
    "    x2=keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='tanh',name='1st32_5')(x2) #1996, 64\n",
    "    c2=keras.layers.SeparableConv1D(32,kernel_size=5,strides=1,activation='tanh',name='2nd32_3')(x2) #1992, 64\n",
    "\n",
    "    x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "    x3=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_2')(x3)  #996, 64\n",
    "    #x3=keras.layers.Conv1D(32,kernel_size=1,strides=2,name='maxpool_2')(x3)  #996, 64\n",
    "    x3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',name='1st64_5')(x3) #994, 128\n",
    "    c3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',name='2nd64_3')(x3) #992, 128\n",
    "\n",
    "    x4=keras.layers.BatchNormalization(name='bn3')(c3)\n",
    "    x4=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_3')(x4)  #496, 64\n",
    "    #x4=keras.layers.Conv1D(64,kernel_size=1,strides=2,name='maxpool_3')(x4)  #496, 128\n",
    "    x4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='1st128_5')(x4)  #494, 256\n",
    "    c4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='2nd128_5')(x4) #492, 256\n",
    "\n",
    "\n",
    "    x5=keras.layers.BatchNormalization(name='bn4')(c4) \n",
    "    x5=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_4')(x5) #246, 256 \n",
    "    #x5=keras.layers.Conv1D(128,kernel_size=1,strides=2,name='maxpool_4')(x5)  #246, 256  \n",
    "    x5=keras.layers.SeparableConv1D(512,kernel_size=3,strides=1,activation='tanh',name='1st512_5')(x5)  #244, 256\n",
    "    x5=keras.layers.SeparableConv1D(512,kernel_size=3,strides=1,activation='tanh',name='2nd512_5')(x5)  #242, 256\n",
    "\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=4, activation='relu', strides=2, name=\"T1st128_5\")(x5)  #486, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=5, activation='relu', strides=1, name=\"T2nd128_5\")(x5)  #490, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T3rd128_5\")(x5)  #492, 256\n",
    "    x5 =keras.layers.BatchNormalization(name='bn5')(x5) \n",
    "\n",
    "    #c4 = keras.layers.Cropping1D((2,2))(c4) #488, 256\n",
    "    x6 = keras.layers.Concatenate(axis=2, name='cn1')([c4,x5])  #492, 512\n",
    "    x6 = keras.layers.Conv1DTranspose(128,kernel_size=3,strides=1,activation='relu',name='3rd128_3')(x6) #494, 256\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=4, activation='relu', strides=2, name=\"T1st64_3\")(x6) #990, 128\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=1, name=\"T2nd64_3\")(x6) #992, 128\n",
    "    x6 = keras.layers.BatchNormalization(name='bn6')(x6)  \n",
    "\n",
    "    #c3 = keras.layers.Cropping1D((4,4))(c3) #984, 128\n",
    "    x7 = keras.layers.Concatenate(axis=2, name='cn2')([c3,x6]) #992, 256\n",
    "    x7 = keras.layers.Conv1DTranspose(64,kernel_size=3,strides=1,activation='relu',name='3rd64_3')(x7) #994, 128\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=4, activation='relu', strides=2, name=\"T1st32_3\")(x7) #1990, 64\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', strides=1, name=\"T2nd32_3\")(x7) #1992, 64\n",
    "    x7 = keras.layers.BatchNormalization(name='bn7')(x7)  \n",
    "\n",
    "    x8 = keras.layers.Concatenate(axis=2, name='cn3')([c2,x7])  #1992, 128\n",
    "    x8 = keras.layers.Conv1DTranspose(32,kernel_size=3,strides=1,activation='relu',name='3rd32_3')(x8)  #1994, 64\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=4,strides=2,activation='relu',name='T1st16_3')(x8) #3990, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T2nd16_3')(x8) #3994, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='T4rth16_3')(x8) #3996, 32\n",
    "    x8 = keras.layers.BatchNormalization(name='bn8')(x8)  \n",
    "\n",
    "    #c1 = keras.layers.Cropping1D((1,1))(c1) #3994, 32\n",
    "    x9 = keras.layers.Concatenate(axis=2, name='cn4')([c1,x8])  #3996, 64\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='3rd16_3')(x9) #3998, 32\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T3rd16_3')(x9) #4002, 32\n",
    "    x9 = keras.layers.BatchNormalization(name='bn9')(x9)\n",
    "\n",
    "    conv_op = keras.layers.Conv1D(3,kernel_size=3,strides=1,name='semiop',activation='softmax')(x9) # (4000, 3)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=focal_dice_loss ,metrics=[dice_coeff] )\n",
    "    return(convNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=4000\n",
    "def build_model2(imsz):\n",
    "    conv_ip = keras.layers.Input(shape=(imsz,),name='Input')\n",
    "    xi=keras.layers.Reshape((imsz, 1), input_shape=(imsz,),name='reshape_1')(conv_ip)\n",
    "\n",
    "    x1=keras.layers.Conv1D(16,kernel_size=10,dilation_rate=3,activation='tanh',name='1st16_5', padding='same')(xi)  #3998, 32\n",
    "    c1=keras.layers.Conv1D(16,kernel_size=5,activation='tanh',name='2nd16_3', padding='same')(x1)  #3996, 32\n",
    "\n",
    "    x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "    x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1', padding='same')(x2)  #1998, 64\n",
    "    x2=keras.layers.Conv1D(32,kernel_size=5,dilation_rate=2,strides=1,activation='tanh',name='1st32_5', padding='same')(x2) #1996, 64\n",
    "    c2=keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='tanh',name='2nd32_3', padding='same')(x2) #1992, 64\n",
    "\n",
    "    x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "    x3=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_2', padding='same')(x3)  #996, 64\n",
    "    x3=keras.layers.Conv1D(64,kernel_size=5,dilation_rate=2,strides=1,activation='tanh',name='1st64_5', padding='same')(x3) #994, 128\n",
    "    c3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',name='2nd64_3', padding='same')(x3) #992, 128\n",
    "\n",
    "    x4=keras.layers.BatchNormalization(name='bn3')(c3)\n",
    "    x4=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_3', padding='same')(x4)  #496, 64\n",
    "    x4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='1st128_5', padding='same')(x4)  #494, 256\n",
    "    c4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='2nd128_5', padding='same')(x4) #492, 256\n",
    "\n",
    "    x_5=keras.layers.BatchNormalization(name='bn4')(c4) \n",
    "    x_5=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_4', padding='same')(x_5) #246, 256 \n",
    "    x_5=keras.layers.SeparableConv1D(256,kernel_size=3,strides=1,activation='tanh',name='1st512_5', padding='same')(x_5)  #244, 256\n",
    "    x_5=keras.layers.SeparableConv1D(256,kernel_size=3,strides=1,activation='tanh',name='2nd512_5', padding='same')(x_5)  #242, 256\n",
    "    #x_5=keras.layers.SeparableConv1D(256,kernel_size=3,strides=1,activation='tanh',name='3rd512_5', padding='same')(x_5)  #242, 256\n",
    "\n",
    "    #tdepth, snr\n",
    "    extra_ip = keras.layers.Input(shape=(2,),name='Input2')\n",
    "    xi2=keras.layers.RepeatVector(250)(extra_ip)\n",
    "    xi2 = keras.layers.Conv1D(256, 3, padding='same')(xi2)\n",
    "    x5 = keras.layers.Multiply()([xi2, x_5])\n",
    "\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=2, name=\"T1st128_5\", padding='same')(x5)  #486, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T2nd128_5\", padding='same')(x5)  #490, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T3rd128_5\", padding='same')(x5)  #492, 256\n",
    "    x5 =keras.layers.BatchNormalization(name='bn5')(x5) \n",
    "\n",
    "    x6 = keras.layers.Concatenate(axis=2, name='cn1')([c4,x5])  #492, 512\n",
    "    x6 = keras.layers.Conv1DTranspose(128,kernel_size=3,strides=1,activation='relu',name='3rd128_3', padding='same')(x6) #494, 256\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=2, name=\"T1st64_3\", padding='same')(x6) #990, 128\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=1, name=\"T2nd64_3\", padding='same')(x6) #992, 128\n",
    "    x6 = keras.layers.BatchNormalization(name='bn6')(x6)  \n",
    "\n",
    "    x7 = keras.layers.Concatenate(axis=2, name='cn2')([c3,x6]) #992, 256\n",
    "    x7 = keras.layers.Conv1DTranspose(64,kernel_size=3,strides=1,activation='relu',name='3rd64_3', padding='same')(x7) #994, 128\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', strides=2, name=\"T1st32_3\", padding='same')(x7) #1990, 64\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=5, activation='relu', strides=1, name=\"T2nd32_3\", padding='same')(x7) #1992, 64\n",
    "    x7 = keras.layers.BatchNormalization(name='bn7')(x7)  \n",
    "\n",
    "    x8 = keras.layers.Concatenate(axis=2, name='cn3')([c2,x7])  #1992, 128\n",
    "    x8 = keras.layers.Conv1DTranspose(32,kernel_size=3,strides=1,activation='relu',name='3rd32_3', padding='same')(x8)  #1994, 64\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=2,activation='relu',name='T1st16_3', padding='same')(x8) #3990, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T2nd16_3', padding='same')(x8) #3994, 32\n",
    "    x8 = keras.layers.BatchNormalization(name='bn8')(x8)  \n",
    "\n",
    "    #c1 = keras.layers.Cropping1D((1,1))(c1) #3994, 32\n",
    "    x9 = keras.layers.Concatenate(axis=2, name='cn4')([c1,x8])  #3996, 64\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='3rd16_3', padding='same')(x9) #3998, 32\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T3rd16_3', padding='same')(x9) #4002, 32\n",
    "    x9 = keras.layers.BatchNormalization(name='bn9')(x9)\n",
    "\n",
    "    conv_op = keras.layers.Conv1D(2,kernel_size=1,strides=1,name='semiop',activation='sigmoid', padding='same')(x9) # (4000, 3)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    convNN = keras.Model(inputs=[conv_ip,extra_ip], outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=focal_dice_loss ,metrics=[generalized_dice_coeff] )\n",
    "    return(convNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "CV_PATH='../../training_data/cross_validation/'\n",
    "MN_PATH='../../training_data/'\n",
    "'''Xtrain1, Ytrain1, Ctrain1, TrainID1 = gc.read_tfr_record(CV_PATH+'zero_aug__s0',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "Xtrain2, Ytrain2, Ctrain2, TrainID2 = gc.read_tfr_record(CV_PATH+'zero_aug__s1',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "Xtrain3, Ytrain3, Ctrain3, TrainID3 = gc.read_tfr_record(CV_PATH+'zero_aug__s2',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "Xtrain4, Ytrain4, Ctrain4, TrainID4 = gc.read_tfr_record(CV_PATH+'zero_aug__s3',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "Xtest, Ytest, Ctest, TestID = gc.read_tfr_record(CV_PATH+'zero_aug__test',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "sm_wt1=np.loadtxt(CV_PATH+'samps0.csv', delimiter=\" \" )\n",
    "sm_wt2=np.loadtxt(CV_PATH+'samps1.csv', delimiter=\" \" )\n",
    "sm_wt3=np.loadtxt(CV_PATH+'samps2.csv', delimiter=\" \" )\n",
    "sm_wt4=np.loadtxt(CV_PATH+'samps3.csv', delimiter=\" \" )'''\n",
    "Xtrain, Ytrain, Ctrain, TrainID = gc.read_tfr_record(MN_PATH+'sem_seg_av_zer_aug_train',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "\n",
    "Xtest, Ytest, Ctest, TestID = gc.read_tfr_record(MN_PATH+'sem_seg_av_zer_aug_test',\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter: make residue network... see if this is any better... get all networks together?\n",
    "'''Xtrainf1 = np.asarray([Xtrain1[i] for i in range(0,len(Xtrain1)) if (sm_wt1[i])])\n",
    "Ytrainf1 = np.asarray([Ytrain1[i] for i in range(0,len(Xtrain1)) if (sm_wt1[i])])\n",
    "Xtrainf2 = np.asarray([Xtrain2[i] for i in range(0,len(Xtrain2)) if (sm_wt2[i])])\n",
    "Ytrainf2 = np.asarray([Ytrain2[i] for i in range(0,len(Xtrain2)) if (sm_wt2[i])])\n",
    "Xtrainf3 = np.asarray([Xtrain3[i] for i in range(0,len(Xtrain3)) if (sm_wt3[i])])\n",
    "Ytrainf3 = np.asarray([Ytrain3[i] for i in range(0,len(Xtrain3)) if (sm_wt3[i])])\n",
    "Xtrainf4 = np.asarray([Xtrain4[i] for i in range(0,len(Xtrain4)) if (sm_wt4[i])])\n",
    "Ytrainf4 = np.asarray([Ytrain4[i] for i in range(0,len(Xtrain4)) if (sm_wt4[i])])\n",
    "\n",
    "Xtrain = np.concatenate([Xtrainf1, Xtrainf2, Xtrainf3, Xtrainf4])\n",
    "Ytrain = np.concatenate([Ytrainf1, Ytrainf2, Ytrainf3, Ytrainf4])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tr=np.asarray([[min(row),np.abs(np.mean(row)/np.std(row))] for row in Xtrain])\n",
    "min_test=np.asarray([[min(row),np.abs(np.mean(row)/np.std(row))] for row in Xtest])\n",
    "min_tr=1/(1+np.exp(-5*min_tr))\n",
    "min_test=1/(1+np.exp(-5*min_test))\n",
    "print(min_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(min_test[:,0]),max(min_test[:,0]))\n",
    "print((min_test[:,0]>0.499).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain=np.asarray(Ytrain,dtype = 'float32').reshape(len(Xtrain),4000,3)\n",
    "Ytest=np.asarray(Ytest,dtype = 'float32').reshape(len(Xtest),4000,3)\n",
    "Xtrain=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
    "Xtest=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])\n",
    "#Xtrainf=np.asarray([0.5*(np.tanh(0.1*(row - np.median(row))/np.std(row))) for row in Xtrain])\n",
    "#Xtestf=np.asarray([0.5*(np.tanh(0.1*(row - np.median(row))/np.std(row))) for row in Xtest])\n",
    "print(Ytrain.shape, Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = []\n",
    "sr =[]\n",
    "for el in Xtrain:\n",
    "    mn.append(np.mean(el))\n",
    "    sr.append(np.std(el))\n",
    "\n",
    "    #print(np.mean(el),np.std(el))\n",
    "sr=np.asarray(sr)\n",
    "mn=np.asarray(mn)\n",
    "print(max(mn),min(mn))\n",
    "print(max(np.abs(mn/sr)),min(np.abs(mn/sr)))\n",
    "print(np.median(np.asarray(sr)))\n",
    "print((mn>0.25).sum()/len(sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train test splits\n",
    "X_trN, X_trC, Y_trN,  Y_trC = split_by_snr(Xtrain, Ytrain, 0.015)\n",
    "X_testN, X_testC, Y_testN,  Y_testC = split_by_snr(Xtest, Ytest, 0.015)\n",
    "print(X_trN.shape, Y_trN.shape, X_trC.shape, Y_trC.shape)\n",
    "print(X_testN.shape, Y_testN.shape, X_testC.shape, Y_testC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,1, figsize=(10,10))\n",
    "for i in range(6):\n",
    "    n= Ytrainf1[i+90].reshape(4000,3)\n",
    "    m=min(Xtrainf1[i+90])\n",
    "    ax[i].plot(Xtrainf1[i+90])\n",
    "    ax[i].plot(n[:,0]*m)\n",
    "    ax[i].plot(n[:,1]*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=[1 for el in Y_testN if(np.any(el[:,0]>0))]\n",
    "print(np.asarray(cnt).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valf, X_testf, Y_valf, Y_testf = train_test_split(Xtest, Ytest, test_size=0.5, shuffle=True)\n",
    "print(X_testf.shape, X_valf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "#vararr=[5,247,3,375]\n",
    "#vararr=[375,376,396,442,456,613]\n",
    "vararr=[0,1,2,3,4]\n",
    "fig,ax=plt.subplots(4,3,figsize=(12,12))\n",
    "j=0\n",
    "\n",
    "xaxis = np.arange(0,4000,1)*29.5/1440\n",
    "plt.suptitle('Segmentation Signature',size=20)\n",
    "ax[0][0].set_title('Input',size=15)\n",
    "ax[0][1].set_title('Output',size=15)\n",
    "ax[0][2].set_title('Result',size=15)\n",
    "\n",
    "ax[3][0].set_xlabel('Time(Days)',size=12)\n",
    "ax[3][1].set_xlabel('Time(Days)',size=12)\n",
    "ax[3][2].set_xlabel('Time(Days)',size=12)\n",
    "for i in range(0,4):\n",
    "    print(np.std(X_trC[vararr[i]]))\n",
    "    ax[i][0].plot(np.arange(0,4000,1)*29.4/1440,X_trC[vararr[i]],color='#077b8a',marker='.',ls='None')\n",
    "    #ax[i][1].plot(Ytrain[vararr[i],:,2])\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in Ytrain[vararr[i]]])\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    #predpl = np.where(Ytest[ar[i],:,0]==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "    ax[i][0].set_ylabel('Flux',size=12)\n",
    "    ax[i][2].plot(bkg*29.4/1440,Xtrain[vararr[i]][bkg],marker='.',ls='None',color='#a2d5c6',label='bkg')\n",
    "    ax[i][2].plot(pl*29.4/1440,Xtrain[vararr[i]][pl],marker='.',ls='None',color='#5c3c92',label='pl')\n",
    "    ax[i][2].plot(fps*29.4/1440,Xtrain[vararr[i]][fps],marker='.',ls='None',color='#d72631',label='fps')\n",
    "    ax[i][1].plot(np.arange(0,4000,1)*29.4/1440,Ytrain[vararr[i],:,0],color='#5c3c92',label='pl')\n",
    "    ax[i][1].plot(np.arange(0,4000,1)*29.4/1440,Ytrain[vararr[i],:,1],color='#d72631',label='fps')\n",
    "    ax[0][1].legend()\n",
    "    ax[0][2].legend()\n",
    "    ax[i][2].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "    ax[i][1].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "    ax[i][0].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "#plt.savefig('fprez_segmentation2')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN2=build_model2(4000)\n",
    "#convNN2.load_weights(MN_PATH+'diltest.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "#history=convNN2.fit(np.asarray(Xtrain),np.asarray(Ytrain), batch_size=16, epochs=10 , verbose=1, sample_weight=10*sm_wt,\n",
    "#     validation_data=(X_valf, Y_valf), callbacks=[es_callback])  \n",
    "history=convNN2.fit([np.asarray(Xtrain),min_tr],np.asarray(Ytrain[:,:,0:2]), batch_size=16, epochs=2, verbose=1,\n",
    "     validation_data=([Xtest,min_test], Ytest[:,:,0:2]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('fprez_segment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=convNN2.predict([np.array(Xtest),min_test])\n",
    "#pred_optr=convNN2.predict([np.array(Xtrain),min_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_testf = np.asarray(X_testf)\n",
    "\n",
    "fig,ax=plt.subplots(6,2,figsize=(15,15))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output')\n",
    "ar=np.random.randint(len(Xtest),size=10)\n",
    "#ar=[0,1,2,3,8,7]\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,6):\n",
    "    #m = min(Xtestf)\n",
    "    #ax[i][0].plot(Xtest[ar[i]],color='gray',ls='None',marker='.',label='data')\n",
    "    #ax[i][0].plot(-pred_op[ar[i],:,2],color='yellow',ls='None',marker='.',label='bkg')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    #ax[i][0].plot(-pred_op[ar[i],:,2],color='gray',ls='None',marker='.',label='pl')\n",
    "\n",
    "    #ax[i][1].plot(Xtest[ar[i],:],color='gray',ls='None',marker='.',label='data')\n",
    "    #ax[i][1].plot(-Ytest[ar[i],:,1],color='yellow',ls='None',marker='.',label='bkg')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    \n",
    "    #ax[i][1].plot(pred_op_mod[ar[i]],color='black',ls='None',marker='.')\n",
    "    ax[i][0].legend('flux')\n",
    "    #ax[i][0].set_ylim(-1.05,0.1)\n",
    "    #ax[i][1].set_ylim(-1.05,0.1)\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "ax[5][0].set_xlabel('Phase')\n",
    "ax[5][1].set_xlabel('Phase')\n",
    "\n",
    "#plt.savefig('present_itsamust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN2.save(MN_PATH+'diltest_ub.h5')\n",
    "#convNN2.save_weights(MN_PATH+'diltest_w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(Ytest, pred_op)\n",
    "#corr(Ytrain, pred_optr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_tr=corrarr(Ytrain, pred_optr)\n",
    "samp_test=corrarr(Ytest, pred_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net ensemble\n",
    "#convNN5= keras.models.load_model(CV_PATH+'s0.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "#convNN6= keras.models.load_model(CV_PATH+'s1.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "#convNN7= keras.models.load_model(CV_PATH+'s2.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "#convNN8= keras.models.load_model(CV_PATH+'s3.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "convNN1= keras.models.load_model(CV_PATH+'s0_2.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "convNN2= keras.models.load_model(CV_PATH+'s1_2.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "convNN3= keras.models.load_model(CV_PATH+'s2_2.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "convNN4= keras.models.load_model(CV_PATH+'s3_2.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})\n",
    "convNN5= keras.models.load_model(CV_PATH+'sm1_3.h5',custom_objects={'focal_dice_loss': focal_dice_loss,'dice_coeff':dice_coeff})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=convNN1.predict(Xtest)\n",
    "pred2=convNN2.predict(Xtest)\n",
    "pred3=convNN3.predict(Xtest)\n",
    "pred4=convNN4.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=np.asarray(pred_op)\n",
    "fig,ax=plt.subplots(6,2,figsize=(15,15))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output')\n",
    "#ar=np.random.randint(len(Xtest),size=10)\n",
    "ar=[0,1,2,3,8,7]\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,6):\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][0].plot(-pred_op[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,1],color='green',ls='None',marker='.',label='fps')\n",
    "    ax[i][1].plot(-Ytest[ar[i],:,0],color='black',ls='None',marker='.',label='pl')\n",
    "    ax[i][0].legend('flux')\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "ax[5][0].set_xlabel('Phase')\n",
    "ax[5][1].set_xlabel('Phase')\n",
    "plt.show()\n",
    "#plt.savefig('present_itsamust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=net_pred(Xtest,[convNN1, convNN2, convNN3, convNN4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= corrarr(Ytest, pred_op)\n",
    "print(np.asarray(out).shape)\n",
    "print(np.asarray(out).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtestf = np.asarray([Xtest[i] for i in range(0,len(Xtest)) if (out[i])])\n",
    "Ytestf = np.asarray([Ytest[i] for i in range(0,len(Xtest)) if (out[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op2=convNN5.predict(Xtestf)\n",
    "out2= corrarr(Ytestf, pred_op2)\n",
    "print(np.asarray(out2).shape)\n",
    "print(np.asarray(out2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(Ytest,pred_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=dice_coeff(Ytest,np.around(np.asarray(pred_op,dtype='float32')))\n",
    "print(val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.MeanIoU(num_classes=3)\n",
    "m.update_state(np.around(Ytest), np.around(pred_op))\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netsamps=[]\n",
    "for x in range(4):\n",
    "    Xtrain, Ytrain, Ctrain, TrainID = gc.read_tfr_record(CV_PATH+'zero_aug__s'+str(x),\n",
    "    ['input','map','counts','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.int8, tf.string])\n",
    "    Ytrain=np.asarray(Ytrain,dtype = 'float32').reshape(len(Xtrain),4000,3)\n",
    "    Xtrain=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
    "    print('import')\n",
    "    op=net_pred(Xtrain,[convNN1, convNN2, convNN3, convNN4])\n",
    "    #samps=[] \n",
    "    samps=corrarr(Ytrain, op)\n",
    "    '''\n",
    "    for i in range(0,len(Xtrain)):\n",
    "        val=(dice_coeff(Ytrain[i,:,0:2],np.around(np.asarray(op[i,:,0:2],dtype='float32')))).numpy()\n",
    "        samps.append(1-val)\n",
    "    #print(samps)'''\n",
    "    netsamps.append(samps)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(netsamps[1]).shape)\n",
    "for i in range(0,4):\n",
    "    np.savetxt(CV_PATH+'samps'+str(i)+'.csv',netsamps[i],delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    conv_ip = keras.layers.Input(shape=(4000,),name='Input')\n",
    "    xi=keras.layers.Reshape((4000, 1), input_shape=(4000,2),name='reshape_1')(conv_ip)\n",
    "    \n",
    "    x1=keras.layers.Conv1D(16,kernel_size=3,strides=1,activation='tanh',name='1st16_5')(xi)  #3998, 32\n",
    "    c1=keras.layers.Conv1D(16,kernel_size=3,strides=1,activation='tanh',name='2nd16_3')(x1)  #3996, 32\n",
    "\n",
    "    x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "    x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1')(x2)  #1998, 64\n",
    "    x2=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',name='1st32_5')(x2) #1996, 64\n",
    "    c2=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',name='2nd32_3')(x2) #1992, 64\n",
    "\n",
    "    x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "    x3=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_2')(x3)  #996, 64\n",
    "    x3=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',name='1st64_5')(x3) #994, 128\n",
    "    c3=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',name='2nd64_3')(x3) #992, 128\n",
    "\n",
    "\n",
    "    x5=keras.layers.Flatten(name='flat_1')(c3)\n",
    "    x5=keras.layers.Dense(64,activation='relu')(x5)\n",
    "    x5=keras.layers.Dense(64,activation='relu')(x5)\n",
    "    x5=keras.layers.Dense(64,activation='relu')(x5)\n",
    "    conv_op=keras.layers.Dense(1, activation='sigmoid')(x5)\n",
    "\n",
    "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001), loss='bce' ,metrics=['accuracy'] )\n",
    "    return(convNN)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ac=[]\n",
    "for el in Ytrain:\n",
    "    prop1 = np.asarray(el[:,0]).sum()\n",
    "    prop2 = np.asarray(el[:,1]).sum()\n",
    "    if(prop1>prop2): tr_ac.append(0)\n",
    "    else: tr_ac.append(1)\n",
    "\n",
    "tst_ac=[]\n",
    "for el in Ytest:\n",
    "    prop1 = np.asarray(el[:,0]).sum()\n",
    "    prop2 = np.asarray(el[:,1]).sum()\n",
    "    if(prop1>prop2): tst_ac.append(0)\n",
    "    else: tst_ac.append(1)\n",
    "\n",
    "print(np.array(tr_ac).shape, np.array(tst_ac).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsf = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = clsf.fit(np.asarray(Xtrain),np.asarray(samp_tr), batch_size=64, epochs=4, verbose=1,\n",
    "     validation_data=(np.asarray(Xtest), np.asarray(samp_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=clsf.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix( samp_test, np.asarray(val>0.4))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(min_test[:,0]),max(min_test[:,0]))\n",
    "print(len([1 for i in range(0,len(samp_test)) if min_test[i,1]<0.51]))\n",
    "len([print(min_test[i]) for i in range(0,len(samp_test)) if samp_test[i] and min_test[i,1]<0.51])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

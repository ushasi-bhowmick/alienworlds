{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import os\n",
    "from astropy.io import fits,ascii\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras import backend as K\n",
    "import GetLightcurves as gc\n",
    "from scipy.signal import find_peaks,lombscargle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, true_label1, pred_label1 = gc.read_tfr_record('../../training_data/jointanalysis_test',['id','true_label','pred_label'],\n",
    "    ['b','ar','ar'],[tf.string,tf.bool, tf.float32])\n",
    "id2, pred_label2 = gc.read_tfr_record('../../training_data/jointanalysis_test_2',\n",
    "    ['id','pred_label'], ['b','ar'],[tf.string,tf.bool])\n",
    "#ip, idr, loc, glob,lb_temp = gc.read_tfr_record('../../training_data/total_tstest', ['input','id','local','global','label'],\n",
    "#    ['ar','b','ar','ar','ar'], [tf.float32, tf.string, tf.float32, tf.float32,tf.bool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax=plt.subplots(3,3,figsize=(10,7),gridspec_kw={'width_ratios': [2,1,2]},sharex='col', sharey='row')\n",
    "ch=[6,4,16]\n",
    "#ch=[16,17,18]\n",
    "i=0\n",
    "plt.suptitle('Processed Data', size=20)\n",
    "tarr=np.arange(0,4000,1)*29.4/1440\n",
    "pharr=np.linspace(-0.25,0.75,2000)\n",
    "pharrl=np.linspace(-0.5,0.5,200)\n",
    "ax[0][0].set_title('Raw', size=15)\n",
    "ax[0][1].set_title('Local View', size=15)\n",
    "ax[0][2].set_title('Global View', size=15)\n",
    "ax[2][0].set_xlabel('time(days)', size=13)\n",
    "ax[2][1].set_xlabel('phase', size=13)\n",
    "ax[2][2].set_xlabel('phase', size=13)\n",
    "for k in range(0,3):\n",
    "    if(lb_temp[ch[i]][0]): lb='pl'\n",
    "    else: lb='fps'\n",
    "    ax[k][0].plot(tarr,ip[ch[i]], color='#3d0000')\n",
    "    ax[k][1].plot(pharrl,loc[ch[i]],label=lb, marker='.', ls='none',color='#950101')\n",
    "    ax[k][2].plot(pharr,glob[ch[i]], marker='.', ls='none', color='#ff0000')\n",
    "    ax[k][0].set_xlim(1000*29.4/1440,3000*29.4/1440)\n",
    "    ax[k][0].set_ylabel('flux', size=13)\n",
    "    ax[k][1].legend()\n",
    "    print(idr[ch[i]], lb_temp[ch[i]])\n",
    "    i+=1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.savefig('fprez_comb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf1  = np.asarray([str(id1[i])[2:11] for i in range(0,len(id1))])\n",
    "idf2  = np.asarray([str(id2[i])[2:11] for i in range(0,len(id2))])\n",
    "pred_labelf1=np.array(np.argmax(pred_label1, axis=1), dtype='bool')\n",
    "true_labelf1 = np.asarray(np.argmax(true_label1, axis=1), dtype='bool')\n",
    "pred_labelf2 = np.asarray(pred_label2, dtype='bool')\n",
    "#idfr  = np.asarray([str(idr[i])[2:13] for i in range(0,len(idr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.array([[0,0],[0,0]])\n",
    "for i in range(len(idf1)):\n",
    "    if(idf1[i]!=idf2[i]): print('odd')\n",
    "    if(true_labelf1[i]==pred_labelf1[i] and true_labelf1[i]==pred_labelf2[i]): cm[0,0]+=1\n",
    "    elif(true_labelf1[i]!=pred_labelf1[i] and true_labelf1[i]==pred_labelf2[i]): cm[0,1]+=1\n",
    "    elif(true_labelf1[i]==pred_labelf1[i] and true_labelf1[i]!=pred_labelf2[i]): cm[1,0]+=1\n",
    "    else: cm[1,1]+=1\n",
    "\n",
    "print(cm/cm.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparision array\n",
    "arr=[]\n",
    "#print(pred_label2[:10])\n",
    "for i in range(0,len(idf1)):\n",
    "    idstr=idf1[i]\n",
    "    #if(idstr[-1]!='1'): continue\n",
    "    t1 = np.argmax(np.asarray(pred_label1[i]))\n",
    "    t3 = np.argmax(np.asarray(true_label1[i]))\n",
    "    if(np.all(np.asarray(pred_label2[i])==[1,1])): t2 = np.argmax(np.asarray(true_label1[i]))\n",
    "    else: t2 = np.argmax(np.asarray(pred_label2[i]))\n",
    "    arr.append([t1,t2,t3])\n",
    "\n",
    "#print(arr)\n",
    "arr=np.asarray(arr)\n",
    "#print(arr[:10])\n",
    "sumarr=np.asarray([np.sum(el) for el in arr])\n",
    "#print(sumarr)\n",
    "tot1=np.asarray([np.all(el==np.asarray([0,1,1])) for el in arr]).sum() + np.asarray([np.all(el==np.asarray([1,0,0])) for el in arr]).sum()\n",
    "tot2=np.asarray([np.all(el==np.asarray([1,0,1])) for el in arr]).sum() + np.asarray([np.all(el==np.asarray([0,1,0])) for el in arr]).sum()\n",
    "tot3=np.asarray([np.all(el==np.asarray([1,1,0])) for el in arr]).sum() + np.asarray([np.all(el==np.asarray([0,0,1])) for el in arr]).sum()\n",
    "print(tot1/len(sumarr), tot2/len(sumarr), tot3/len(sumarr))\n",
    "#print(arr,tot1)\n",
    "#print(np.asarray(sumarr==0).sum()/len(sumarr) + np.asarray(sumarr==2).sum()/len(sumarr))\n",
    "print(np.asarray(sumarr==0).sum()/len(sumarr) + np.asarray(sumarr==3).sum()/len(sumarr))\n",
    "\n",
    "cm=confusion_matrix(arr[:,0],arr[:,1])\n",
    "print(cm/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both wronged\n",
    "wr_ind = [i for i in range(len(arr)) if np.all(arr[i]==np.asarray([1,0,1])) or np.all(arr[i]==np.asarray([0,1,0]))]\n",
    "print(len(wr_ind))\n",
    "np.random.shuffle(wr_ind)\n",
    "fig, ax= plt.subplots(5,3, figsize=(15,15))\n",
    "for i in range(0,5):\n",
    "    ax[i][2].plot(ip[wr_ind[i]])\n",
    "    mp = pred_map2[wr_ind[i]].reshape(4000,3)\n",
    "    tmp = true_map2[wr_ind[i]].reshape(4000,3)\n",
    "    minim=min(ip[wr_ind[i]])\n",
    "    ax[i][0].plot(loc[wr_ind[i]],label=arr[wr_ind[i],0])\n",
    "    ax[i][1].plot(glob[wr_ind[i]],label=arr[wr_ind[i],1])\n",
    "\n",
    "    ax[i][2].plot(ip[wr_ind[i]],  color='gray')\n",
    "    ax[i][2].plot(mp[:,0]*minim,color='green')\n",
    "    ax[i][2].plot(mp[:,1]*minim,color='red')\n",
    "    ax[i][2].plot(tmp[:,0]*minim,color='green',ls='none', marker='_',label=arr[wr_ind[i],2])\n",
    "    ax[i][2].plot(tmp[:,1]*minim,color='red',ls='none', marker='_')\n",
    "    \n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].legend()\n",
    "    ax[i][2].legend()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='../../processed_directories/new_pl/'\n",
    "PATH2='../../processed_directories/expand_test_result_av/'\n",
    "FILEPATH_FPS=\"E:\\Masters_Project_Data\\\\alienworlds_fps\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 3\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def log_cosh_dice_loss(y_true, y_pred):\n",
    "        x = generalized_dice_coeff(y_true, y_pred)\n",
    "        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)*0.5 + log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    #w = 1 - w\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred):\n",
    "    loss = FocalLoss(y_true, y_pred) + 1*log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def remove_nan(red_flux,bins):\n",
    "    #for i in range(0,len(red_flux)):\n",
    "    #    if(np.isnan(red_flux[i])):\n",
    "    #        t=1\n",
    "    #        try:\n",
    "    #            while(np.isnan(red_flux[i-t]) or np.isnan(red_flux[i+t])):    \n",
    "    #                if(i-t == 0):\n",
    "    #                    red_flux[i]=red_flux[i+t]\n",
    "    #                    break\n",
    "    #                elif(i+t == bins-1): \n",
    "    #                    red_flux[i]=red_flux[i-t]\n",
    "    #                    break\n",
    "    #                t+=1\n",
    "    #            red_flux[i]=(red_flux[i-t]+red_flux[i+t])/2\n",
    "    #        except:\n",
    "    #            red_flux[i]=0\n",
    "    for i in range(0,len(red_flux)):\n",
    "        if np.isnan(red_flux[i]):\n",
    "            red_flux[i]=0\n",
    "#print(tfa.losses.SigmoidFocalCrossEntropy(Ytest, Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this chunk will contain modules to find lomb scargle peak periods in the NN...\n",
    "def open_the_file_and_chunk(filepath, hdu_no, dil_rate):\n",
    "    hdu = fits.open(filepath)\n",
    "    flux=hdu[hdu_no].data['LC_DETREND']\n",
    "    #flux=hdu[len(hdu)-1].data['RESIDUAL_LC']\n",
    "    tdurs = [hdu[n].header['TPERIOD'] for n in range(1,len(hdu)-1)]\n",
    "    \n",
    "    remove_nan(flux,4000)\n",
    "    inc=4000*dil_rate\n",
    "    chunks = np.asarray([flux[np.arange(i,i+inc,dil_rate)] for i in range(0,len(flux)-inc, inc)])\n",
    "    return(chunks, tdurs)\n",
    "\n",
    "def chunk_file_from_pd(filepath, dil_rate):\n",
    "    input, map=gc.read_tfr_record(filepath, ['input','mask'],['ar','ar'], [tf.float32, tf.bool])\n",
    "    \n",
    "    map=np.asarray(map).reshape(len(input),4000,3)\n",
    "    input=np.concatenate(np.asarray(input),axis=0)\n",
    "    map=np.concatenate(np.asarray(map),axis=0)\n",
    "    \n",
    "    inc=4000*dil_rate\n",
    "    chunks = np.asarray([input[np.arange(i,i+inc,dil_rate)] for i in range(0,len(input)-inc, inc)])\n",
    "    mpch = np.asarray([map[np.arange(i,i+inc,dil_rate)] for i in range(0,len(map)-inc, inc)])\n",
    "    return(chunks, mpch)\n",
    "\n",
    "def periodogram_and_freq(chunks, minp, maxp,ch=0, prec=10000):\n",
    "    p = np.linspace(minp,maxp,prec)\n",
    "    f = 2*np.pi /p\n",
    "    tot = np.concatenate(chunks, axis=0)\n",
    "    #thres = np.mean(tot)\n",
    "    #tot = np.asarray([1 if(el>thres) else 0 for el in tot])\n",
    "    #print(tot.shape, thres)\n",
    "    print(tot.shape)\n",
    "    tot = [np.argmax(el) for el in tot]\n",
    "    print(len(tot))\n",
    "    tot = np.asarray(np.asarray(tot)==ch)\n",
    "    print(tot)\n",
    "    \n",
    "    predp = lombscargle(np.arange(0,len(tot))*29.4/1440, tot,f, normalize=True)\n",
    "    return(p, predp)\n",
    "\n",
    "def what_the_peak(per, predp, ip, ch=0,thres=1):\n",
    "    newp=[]\n",
    "    h = np.median(predp)+thres*np.std(predp)\n",
    "    totip = np.concatenate(ip, axis=0)\n",
    "\n",
    "    plp, _ = find_peaks(np.asarray(predp), height=h, distance=40)\n",
    "    peakf = per[plp]\n",
    "    print(peakf)\n",
    "    peakinfo = np.asarray([predp[plp],per[plp]])\n",
    "    sortedArr = peakinfo [ :, peakinfo[0].argsort()]\n",
    "    mn = np.mean(totip)\n",
    "    #print(sortedArr)\n",
    "    other = np.arange(0,len(totip),1)\n",
    "    for p in np.flip(np.asarray(sortedArr[1])):\n",
    "        #print(p)\n",
    "        mask = color_a_peak_v2(totip, p)\n",
    "        nother = np.setdiff1d(other,mask)\n",
    "        #tmn = np.mean(totip[other])\n",
    "        print(p,len(other),len(nother),(len(other)-len(nother))/len(mask))\n",
    "        if((len(other)-len(nother))/len(mask) > 0.25):\n",
    "            newp.append(p)\n",
    "            other = nother\n",
    "        #print(mn, tmn)\n",
    "        #if(tmn>mn): \n",
    "        #    mn = tmn\n",
    "        #    newp.append(p)\n",
    "        #else: continue\n",
    "    return(newp)\n",
    "    #choose good ones.\n",
    "\n",
    "\n",
    "def plot_a_map(input, map):\n",
    "    totip = np.concatenate(input)\n",
    "    totop = np.concatenate(map, axis=0)\n",
    "\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in totop])\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "\n",
    "    m=min(totip)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Total Segmentation Map')\n",
    "    plt.xlabel('time (days)')\n",
    "    plt.ylabel('flux')\n",
    "    #plt.plot(np.arange(0,len(totip))*29.4/1440,totop[:,0])\n",
    "    #plt.plot(np.arange(0,len(totip))*29.4/1440,totop[:,1])\n",
    "    plt.plot(np.asarray(bkg)*29.4/1440,totip[bkg], color='#a4b3b6', marker='.', ls='none', label='bkg')\n",
    "    plt.plot(np.asarray(fps)*29.4/1440,totip[fps], color='#44318d', marker='.', ls='none', label='fps')\n",
    "    plt.plot(np.asarray(pl)*29.4/1440,totip[pl], color='#d83f87', marker='.', ls='none', label='pl')\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_a_peak(perarr, input, map, shift,num1=1,num2=1, scale=[10], thres=0.5):\n",
    "    flag = 0\n",
    "    if(len(perarr)==1): flag=1\n",
    "    fig, ax = plt.subplots(num1,num2,figsize=(10,10))\n",
    "    plt.suptitle('Sample Phasefolds', size=20)\n",
    "    i=0\n",
    "    j=0\n",
    "    for el in range(len(perarr)):\n",
    "        #print(perarr)\n",
    "        per = int(perarr[el] * 24 *60/29.4)\n",
    "        bias = int(shift[el]*per)\n",
    "        totip = np.concatenate(input, axis=0)\n",
    "        totmp = np.concatenate(map, axis=0)\n",
    "        phfip = [totip[i:i+per] for i in range(bias,len(totip)-per,per)]\n",
    "        phfmap = np.asarray([totmp[i:i+per] for i in range(bias,len(totmp)-per,per)])\n",
    "\n",
    "        phfmap = np.mean(phfmap, axis=0)\n",
    "        print(len(phfmap))\n",
    "        phfip = np.mean(phfip, axis=0)\n",
    "        box = np.ones(3)/3\n",
    "        phfip = np.convolve(phfip, box, mode='same')\n",
    "        md = np.median(phfip)\n",
    "        st = min(phfip)\n",
    "\n",
    "        phaseax=np.linspace(0,1,len(phfmap))\n",
    "        print(md, st)\n",
    "        ker=np.ones(10)/10\n",
    "        if(flag==1):\n",
    "            #ax.plot(phaseax,phfmap, label='map')\n",
    "            ax.plot(phaseax,np.convolve(phfip*scale[el], ker, mode='same'), label='input', marker='.',ls='none', color='#2a1b3d')\n",
    "            ax.set_title('period:'+\"{:.2f}\".format(perarr[el]), size='14')\n",
    "            #ax.plot(phaseax,(md+thres*st)*np.ones(len(phfip))*scale[el])\n",
    "            #print('md:',md)\n",
    "            ax.set_xlabel('Time(Days)', size='13')\n",
    "            ax.set_ylabel('Flux', size='13')\n",
    "            ax.legend()\n",
    "        elif(num2==1):\n",
    "            #ax[i].plot(phaseax,phfmap, label='map')\n",
    "            ax[i].plot(phaseax,np.convolve(phfip*scale[el], ker, mode='same'), label='input', marker='.',ls='none', color='#2a1b3d')\n",
    "            ax[i].set_title('period:'+ \"{:.2f}\".format(perarr[el]), size='14')\n",
    "            #ax[i].plot(phaseax,(md+thres*st)*np.ones(len(phfip))*scale[el])\n",
    "            ax[i].set_xlabel('Time(Days)', size='13')\n",
    "            ax[i].set_ylabel('Flux',size='13')\n",
    "            ax[i].legend()\n",
    "        else:\n",
    "            #ax[i].plot(phaseax,phfmap, label='map')\n",
    "            ax[i][j].plot(phaseax,np.convolve(phfip*scale[el], ker, mode='same'), label='input', marker='.',ls='none', color='#2a1b3d')\n",
    "            ax[i][j].set_title('period:'+ \"{:.2f}\".format(perarr[el]), size='14')\n",
    "            ax[i][j].plot(phaseax,(md+thres*st)*np.ones(len(phfip))*scale[el])\n",
    "            ax[i][j].set_xlabel('Time(Days)', size='13')\n",
    "            ax[i][j].set_ylabel('Flux',size='13')\n",
    "            ax[i][j].legend()\n",
    "        j+=1\n",
    "        if(j==num2):\n",
    "            j=0\n",
    "            i+=1\n",
    "        if(i==num1): break\n",
    "\n",
    "def color_a_peak(chunks, peakarr, thres):\n",
    "    mainmask=[]\n",
    "    mno=1\n",
    "    input = np.concatenate(chunks)\n",
    "    k=0\n",
    "    for el in peakarr:\n",
    "        req_p = int(el * 24 *60/29.4)\n",
    "        phf = np.asarray([input[i:i+req_p] for i in range(0,len(input)-req_p,req_p)])\n",
    "        phf = np.mean(phf, axis = 0)\n",
    "        md = np.median(phf)\n",
    "        std = min(phf)\n",
    "        \n",
    "        pts = np.asarray([i for i in range(len(phf)) if(phf[i]<md+thres[k]*std)])\n",
    "        newpts = pts\n",
    "        x=len(phf)\n",
    "        k+=1\n",
    "        while(x<68000):\n",
    "            #print(x, len(pts))\n",
    "            #print(len(pts+x*np.ones(len(pts))))\n",
    "            newpts = np.concatenate([newpts,np.asarray(pts+x*np.ones(len(pts)),dtype='int')])\n",
    "            x+=len(phf)\n",
    "        newpts = [el for el in newpts if el<68000] \n",
    "        mainmask.append(newpts)\n",
    "    return(mainmask)\n",
    "\n",
    "def color_a_peak_v2(input, el):\n",
    "    req_p = int(el * 24 *60/29.4)\n",
    "    phf = np.asarray([input[i:i+req_p] for i in range(0,len(input)-req_p,req_p)])\n",
    "    phf = np.mean(phf, axis = 0)\n",
    "    md = np.median(phf)\n",
    "    std = np.std(phf)\n",
    "    pts = np.asarray([i for i in range(len(phf)) if(phf[i]<md-0.7*std)])\n",
    "    newpts = pts\n",
    "    x=len(phf)\n",
    "    while(x<68000):\n",
    "        #print(x, len(pts))\n",
    "        #print(len(pts+x*np.ones(len(pts))))\n",
    "        newpts = np.concatenate([newpts,np.asarray(pts+x*np.ones(len(pts)),dtype='int')])\n",
    "        x+=len(phf)\n",
    "    #print(newpts)\n",
    "    newpts = [el for el in newpts if el<68000] \n",
    "    return(newpts)\n",
    "\n",
    "def getids(test_samp,dir): \n",
    "    cnt, label=gc.read_tfr_record(test_samp, ['counts','id'],['ar','b'], [tf.int8, tf.string])\n",
    "    entries=os.listdir(dir)\n",
    "   \n",
    "    ID = [str(el)[2:11] for el in label]\n",
    "    \n",
    "    neID=[]\n",
    "    for x in ID:\n",
    "        temp =[el for el in entries if el.find(x)>0]\n",
    "        neID.append(temp[0])\n",
    "    return(neID,cnt)\n",
    "\n",
    "\n",
    "def dil_conc_avg(arr1,arr2,arr3):\n",
    "    totarr1 = np.concatenate(arr1, axis=0) #largest\n",
    "    totarr2 = np.concatenate(arr2, axis=0)\n",
    "    totarr3 = np.concatenate(arr3, axis=0) #smallest\n",
    "    dilr=int(len(totarr1)/len(totarr3))\n",
    "    for i in range(0,len(totarr3)):\n",
    "        totarr1[dilr*i]=np.mean(np.asarray([totarr1[dilr*i],totarr3[i]]))\n",
    "    dilr=int(len(totarr1)/len(totarr2))\n",
    "    for i in range(0,len(totarr2)):\n",
    "        totarr1[dilr*i]=np.mean(np.asarray([totarr1[dilr*i],totarr2[i]]))\n",
    "    return(totarr1)\n",
    "\n",
    "def corr(tpl, tfps, ppl, pfps):\n",
    "    cm=[[0,0],[0,0]]\n",
    "    print(len(ppl))\n",
    "    for i in range(0, len(ppl)):\n",
    "        if(np.any(tpl[i]>0.5)):\n",
    "            val1 = np.corrcoef(pfps[i], tpl[i])\n",
    "            val2 = np.corrcoef(ppl[i], tpl[i])\n",
    "            if(val2[0,1]>val1[0,1]): \n",
    "                cm[0][0]+=1\n",
    "            else: \n",
    "                cm[1][0]+=1\n",
    "            print(val1[0,1],val2[0,1])\n",
    "        #fps detect\n",
    "        if(np.any(tfps[i]>0.5)):\n",
    "            val1 = np.corrcoef(pfps[i], tfps[i])\n",
    "            val2 = np.corrcoef(ppl[i], tfps[i])\n",
    "            if(np.isnan(val1[0,1]) or np.isnan(val2[0,1])): continue\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                cm[1][1]+=1\n",
    "            else: \n",
    "                cm[0][1]+=1\n",
    "            print(val1[0,1],val2[0,1])\n",
    "    \n",
    "    print(np.asarray(cm))\n",
    "    print((cm[0][0]+cm[1][1])/np.asarray(cm).sum())\n",
    "\n",
    "#print(getids('../../training_data/sem_seg_av_zer_aug_test', FILEPATH_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = os.listdir(PATH)\n",
    "#ip,tp,pp,sm,ss,plp,fpsp = gc.read_tfr_record(PATH+entries[0],\n",
    "#    ['input','true_map','pred_map','scale_median','scale_std','pl_peaks','fps_peaks'],\n",
    "#    ['ar','ar','ar','fl','fl','ar','ar'], \n",
    "#    [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.int16, tf.int16])\n",
    "ip,mp,ct = gc.read_tfr_record(PATH+entries[0],\n",
    "    ['input','mask','counts'],\n",
    "    ['ar','ar','ar'], \n",
    "    [tf.float32, tf.bool, tf.int8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = 'E:\\Masters_Project_Data\\\\alienworlds_fps\\\\'\n",
    "#kplr004852528-20160128150956_dvt.fits\n",
    "#kplr011442793-20160128150956_dvt.fits\n",
    "#kplr008480285-20160128150956_dvt.fits\n",
    "#kplr011619964-20160128150956_dvt.fits\n",
    "NAME = 'kplr011442793-20160128150956_dvt.fits'\n",
    "\n",
    "chunks, tdur = open_the_file_and_chunk(FILEPATH+NAME,1,4)\n",
    "print(chunks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirids,counted = getids('../../training_data/sem_seg_av_zer_aug_test',FILEPATH_FPS)\n",
    "dirids=[el[4:13] for el in dirids]\n",
    "print(len(dirids))\n",
    "chunk4000=[]\n",
    "chunk8000=[]\n",
    "chunk12000=[]\n",
    "mapspl4000=[]\n",
    "mapspl8000=[]\n",
    "mapspl12000=[]\n",
    "mapsfps4000=[]\n",
    "mapsfps8000=[]\n",
    "mapsfps12000=[]\n",
    "for el in dirids:\n",
    "    chunks, maps = chunk_file_from_pd('../../processed_directories/sem_seg_av_test/'+el,1)\n",
    "    #maps=maps.reshape(len(chunks),4000,3)\n",
    "    md=np.median(chunks)\n",
    "    sd=np.std(chunks)\n",
    "    chunks2=np.asarray([0.5*(np.tanh(0.1*(row - md)/sd)) for row in chunks[1:7]])\n",
    "    chunk4000.append(chunks2)\n",
    "    mapspl4000.append(maps[1:7,:,0])\n",
    "    mapsfps4000.append(maps[1:7,:,1])\n",
    "    chunks, maps = chunk_file_from_pd('../../processed_directories/sem_seg_av_test/'+el,2)\n",
    "    #maps=maps.reshape(len(chunks),4000,3)\n",
    "    md=np.median(chunks)\n",
    "    sd=np.std(chunks)\n",
    "    chunks2=np.asarray([0.5*(np.tanh(0.1*(row - md)/sd)) for row in chunks[1:4]])\n",
    "    chunk8000.append(chunks2)\n",
    "    mapspl8000.append(maps[1:4,:,0])\n",
    "    mapsfps8000.append(maps[1:4,:,1])\n",
    "    chunks, maps = chunk_file_from_pd('../../processed_directories/sem_seg_av_test/'+el,3)\n",
    "    #maps=maps.reshape(len(chunks),4000,3)\n",
    "    md=np.median(chunks)\n",
    "    sd=np.std(chunks)\n",
    "    chunks2=np.asarray([0.5*(np.tanh(0.1*(row - md)/sd)) for row in chunks[1:3]])\n",
    "    chunk12000.append(chunks2)\n",
    "    mapspl12000.append(maps[1:3,:,0])\n",
    "    mapsfps12000.append(maps[1:3,:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN = keras.models.load_model('newtests.h5',custom_objects={'focal_dice_loss': focal_dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#about 1393 days of obs... 460\n",
    "#chunks2 = np.asarray([np.tanh(100*row) for row in chunks])\n",
    "chunks2=np.asarray([0.5*(np.tanh(0.1*(row - np.median(row))/np.std(row))) for row in chunks])\n",
    "#m = chunks.reshape(-1)[np.argmin(chunks)]\n",
    "#chunks2= np.asarray([(row-np.median(row))/(-m+np.median(row)) for row in chunks])\n",
    "\n",
    "opchunk = convNN.predict(chunks2)\n",
    "\n",
    "f1, predp1= periodogram_and_freq(opchunk[:,:,0:3], 1.5/24, 600,0, 10000)\n",
    "f2, predp2= periodogram_and_freq(opchunk[:,:,0:3], 1.5/24, 600,1, 10000)\n",
    "#f, predp = periodogram_and_freq(opchunk[:,:,0], 10, 20, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totanspl=[]\n",
    "totansfps=[]\n",
    "totch=[]\n",
    "totmappl=[]\n",
    "totmapfps=[]\n",
    "for i in range(0,len(chunk4000)):\n",
    "    totch.append(np.concatenate(chunk4000[i],axis=0))\n",
    "    totmappl.append(np.concatenate(mapspl4000[i],axis=0))\n",
    "    totmapfps.append(np.concatenate(mapsfps4000[i],axis=0))\n",
    "    op4000=convNN.predict(chunk4000[i])\n",
    "    op8000=convNN.predict(chunk8000[i])\n",
    "    op12000=convNN.predict(chunk12000[i])\n",
    "    anspl=dil_conc_avg(op4000[:,:,0],op8000[:,:,0],op12000[:,:,0])\n",
    "    ansfps=dil_conc_avg(op4000[:,:,1],op8000[:,:,1],op12000[:,:,1])\n",
    "    totanspl.append(anspl)\n",
    "    totansfps.append(ansfps)\n",
    "    \n",
    "print(np.asarray(totansfps).shape)\n",
    "print(np.asarray(totanspl).shape)\n",
    "print(np.asarray(totmappl).shape)\n",
    "print(np.asarray(totmapfps).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no=2\n",
    "plt.plot(-totansfps[no])\n",
    "plt.plot(-totanspl[no])\n",
    "plt.plot(totch[no])\n",
    "plt.plot(totmappl[no])\n",
    "plt.plot(totmapfps[no])\n",
    "print(counted[no])\n",
    "print(np.corrcoef(totmapfps[no],totanspl[no]))\n",
    "print(np.corrcoef(totmapfps[no],totansfps[no]))\n",
    "#plt.xlim(4000,5000)\n",
    "corr(np.asarray(totmappl), np.asarray(totmapfps), np.asarray(totanspl), np.asarray(totansfps))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pks= what_the_peak(f2, predp2, chunks, 1)\n",
    "print(pks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f1,predp1)\n",
    "plt.plot(f2,predp2)\n",
    "#plt.scatter(pks,np.zeros(len(pks)), color='green')\n",
    "#print(pks)\n",
    "print(np.asarray(tdur))\n",
    "#plt.xlim(0,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks=np.append(chunks, [np.zeros(4000)], axis=0)\n",
    "plot_a_map(chunks2,opchunk)\n",
    "#plt.ylim(-0.005,0.01)\n",
    "#plt.savefig('fprez_example3_raw.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_peak(pks,chunks,\n",
    "    opchunk[:,:,0],[0.3,0.3,0.1,0.0,0.0,0.0,0.2,0.2,0.0], num1=3, num2=3,scale=1*np.ones(9), thres=0.2)\n",
    "\n",
    "#plt.savefig('fprez_example3_peak.png') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch=[29.62,14.81,9.55,10]\n",
    "ch=pks\n",
    "mask = color_a_peak(chunks,ch, [0.5,0.5,0.05,0.21])\n",
    "#print(np.asarray(mask).shape)\n",
    "\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(2,1, figsize=(10,10))\n",
    "\n",
    "ax[0].set_title('Derived Periodic Signature from Segmentation')\n",
    "ax[0].set_xlabel('time (Days)')\n",
    "ax[0].set_ylabel('flux')\n",
    "ax[1].set_xlabel('time (Days)')\n",
    "ax[1].set_ylabel('flux')\n",
    "ax[0].set_ylim(-0.005,0.005)\n",
    "ax[1].set_ylim(-0.005,0.005)\n",
    "print(pks)\n",
    "\n",
    "val = np.concatenate(chunks)\n",
    "xaxis = np.arange(0,len(val),1)*29.4/1440\n",
    "#print(pks)\n",
    "ax[0].plot(xaxis,val, ls='none', marker='.', color='#a4b3b6', label='bkg')\n",
    "ax[1].plot(xaxis,val, ls='none', marker='.', color='#a4b3b6', label='bkg')\n",
    "\n",
    "ax[0].plot(np.asarray(mask[0])*29.4/1440,val[mask[0]], ls='none', marker='.',color='black',label= \"{:.2f}\".format(ch[0])+' days')\n",
    "other = np.setdiff1d(mask[1],mask[0])\n",
    "ax[0].plot(other*29.4/1440,val[other], ls='none',marker='.',color='#44318d',label= \"{:.2f}\".format(ch[1])+' days')\n",
    "other = np.setdiff1d(mask[2],mask[0])\n",
    "other = np.setdiff1d(other,mask[1])\n",
    "ax[1].plot(other*29.4/1440,val[other], ls='none', marker='.',label= \"{:.2f}\".format(ch[2])+' days')\n",
    "other = np.setdiff1d(mask[3],mask[2])\n",
    "other = np.setdiff1d(other,mask[1])\n",
    "other = np.setdiff1d(other,mask[0])\n",
    "ax[1].plot(other*29.4/1440,val[other], ls='none', marker='.',label= \"{:.2f}\".format(ch[3])+' days')\n",
    "'''other = np.setdiff1d(mask[2],mask[3])\n",
    "other = np.setdiff1d(other,mask[2])\n",
    "other = np.setdiff1d(other,mask[1])\n",
    "other = np.setdiff1d(other,mask[0])'''\n",
    "#plt.plot(other,val[other], ls='none', marker='.')\n",
    "#other = np.setdiff1d(mask[5],mask[4])\n",
    "#other = np.setdiff1d(other,mask[3])\n",
    "#other = np.setdiff1d(other,mask[2])\n",
    "#other = np.setdiff1d(other,mask[1])\n",
    "#other = np.setdiff1d(other,mask[0])\n",
    "#plt.plot(other,val[other], ls='none', marker='.')\n",
    "#other = np.setdiff1d(mask[4],mask[3])\n",
    "#plt.plot(other,val[other], ls='none', marker='.')\n",
    "#other = np.setdiff1d(mask[5],mask[4])\n",
    "#plt.plot(other,val[other], ls='none', marker='.')\n",
    "#plt.plot(other,val[other], ls='none', color='gray',marker='.')\n",
    "#ax[0].set_xlim(100,800)\n",
    "ax[1].set_xlim(300,400)\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "#plt.savefig('fprez_example5.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try running it on some of the test samples, and hopefully try to put a number on the accuracy ...\n",
    "#give a trial run out of the maximum peak of LS periodogram... dont think its gonna work but still\n",
    "chunkarrs, perarrs,label, idarr = gc.read_tfr_record('../../training_data/tstest',\n",
    "    ['input','period','label','id'],['ar','ar','ar','b'],[tf.float32, tf.float32, tf.bool, tf.string])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for el in chunkarrs[:10]:\n",
    "    chunks = el.reshape(17,4000)\n",
    "    chunks = np.asarray([np.tanh(100*row) for row in chunks])\n",
    "    opchunk = convNN.predict(chunks)\n",
    "    p, predf = periodogram_and_freq(opchunk[:,:,0],1, 68000*29.4/1440)\n",
    "    #pks,maxm = what_the_peak(p, predf, opchunk[:,:,0], 1)\n",
    "    p2, predf2 = periodogram_and_freq(opchunk[:,:,1],1, 68000*29.4/1440)\n",
    "    #pks,maxm2 = what_the_peak(p, predf, opchunk[:,:,1], 1)\n",
    "    print(max(predf),max(predf2),label[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chunkarrs[1])\n",
    "print(idarr[1],label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholesome thing.... lets see if this is any better\n",
    "entries = os.listdir(PATH2)\n",
    "corrects = 0 \n",
    "incorrects = 0\n",
    "cm=[[0,0],[0,0]]\n",
    "\n",
    "for el in entries:\n",
    "    ip,tp,pp,sm,ss = gc.read_tfr_record(PATH2+el,\n",
    "        ['input','true_map','pred_map','scale_median','scale_std'],\n",
    "        ['ar','ar','ar','fl','fl'], \n",
    "        [tf.float32, tf.bool, tf.float32, tf.float32, tf.float32])\n",
    "\n",
    "    pp = np.reshape(pp,(len(tp),4000,3))\n",
    "    tp = np.reshape(tp,(len(tp),4000,3))\n",
    "\n",
    "    totpp = np.concatenate(pp, axis=0)\n",
    "    tottp = np.concatenate(tp, axis=0)\n",
    "\n",
    "    #print(np.mean(totpp, axis=0))\n",
    "    #print(totpp.shape)\n",
    "    #planet detection:\n",
    "    if(np.any(tottp[:,0]>0)):\n",
    "        val1 = np.corrcoef(totpp[:,1], tottp[:,0])\n",
    "        val2 = np.corrcoef(totpp[:,0], tottp[:,0])\n",
    "        if(val2[0,1]>val1[0,1]): \n",
    "            cm[0][0]+=1\n",
    "            corrects+=1\n",
    "        else: \n",
    "            cm[1][0]+=1\n",
    "            incorrects+=1\n",
    "        #print(val1[0,1], val2[0,1])\n",
    "    \n",
    "    #fps detect\n",
    "    if(np.any(tottp[:,1]>0)):\n",
    "        val1 = np.corrcoef(totpp[:,1], tottp[:,1])\n",
    "        val2 = np.corrcoef(totpp[:,0], tottp[:,1])\n",
    "        if(val1[0,1]>val2[0,1]): \n",
    "            cm[1][1]+=1\n",
    "            corrects+=1\n",
    "        else: \n",
    "            cm[0][1]+=1\n",
    "            incorrects+=1\n",
    "    \n",
    "print(corrects, incorrects)\n",
    "print(corrects/(corrects+incorrects))\n",
    "print(cm/np.asarray(cm).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholesome thing.... lets see if this is any better\n",
    "entries = os.listdir (PATH2)\n",
    "trues=[]\n",
    "predicts=[]\n",
    "\n",
    "for el in entries:\n",
    "    ip,tp,pp,sm,ss = gc.read_tfr_record(PATH2+el,\n",
    "        ['input','true_map','pred_map','scale_median','scale_std'],\n",
    "        ['ar','ar','ar','fl','fl'], \n",
    "        [tf.float32, tf.bool, tf.float32, tf.float32, tf.float32])\n",
    "\n",
    "    pp = np.reshape(pp,(len(tp),4000,3))\n",
    "    tp = np.reshape(tp,(len(tp),4000,3))\n",
    "\n",
    "    totpp = np.concatenate(pp, axis=0)\n",
    "    tottp = np.concatenate(tp, axis=0)\n",
    "    \n",
    "    #planet detection:\n",
    "    ftp = np.fft.fft(np.around(totpp[:,0]))\n",
    "    ftfp = np.fft.fft(np.around(totpp[:,1]))\n",
    "\n",
    "    if(max(np.abs(ftp[1:]))<max(np.abs(ftfp[1:]))): predicts.append(1)\n",
    "    else: predicts.append(0)\n",
    "    \n",
    "    if(tottp[:,0].sum()<tottp[:,1].sum()): trues.append(1)\n",
    "    else: trues.append(0)\n",
    "    \n",
    "\n",
    "cm=confusion_matrix(np.asarray(trues),np.asarray(predicts))\n",
    "print(cm/cm.sum())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

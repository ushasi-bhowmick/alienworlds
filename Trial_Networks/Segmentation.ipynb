{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to improve the NN\n",
    "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=np.loadtxt('training_data/Xtrain_inst_mask.csv',delimiter=',')\n",
    "Xtest=np.loadtxt('training_data/Xtest_inst_mask.csv',delimiter=',')\n",
    "Mtrain=np.loadtxt('training_data/Mtrain_inst_mask.csv',delimiter=',')\n",
    "Mtest=np.loadtxt('training_data/Mtest_inst_mask.csv',delimiter=',')\n",
    "\n",
    "#Xtest = np.array([[0 if(np.isnan(el)) else el for el in x] for x in Xtest])\n",
    "\n",
    "Xtrain=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
    "Xtest=np.array([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])\n",
    "print(Xtrain.shape,Mtrain.shape,Xtest.shape,Mtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "fig,ax=plt.subplots(10,1,figsize=(10,20))\n",
    "for i in range(0,10):\n",
    "    ax[i].plot(Mtrain[vararr[i]]*min(Xtrain[vararr[i]]))\n",
    "    ax[i].plot(Xtrain[vararr[i]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    #sh = tf.shape(y_true)\n",
    "    #y_true_int = tf.reshape(y_true, [sh[0], sh[1]*sh[2]])\n",
    "    #y_pred_int = tf.reshape(y_pred, [sh[0], sh[1]*sh[2]])\n",
    "    #w = 1 - tf.reduce_sum(y_true,(1,2)) / 9600.\n",
    "    #w = tf.reshape(w, [len(w),1])\n",
    "    # Flatten\n",
    "    #y_true_f = tf.reshape(tf.multiply(y_true_int,w), [-1])\n",
    "    #y_pred_f = tf.reshape(tf.multiply(y_pred_int,w), [-1])\n",
    "    #y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    #y_pred_int_f = tf.reshape(tf.matmul(w, y_pred_int), [-1])\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) +  tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(Ytest,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    w=tf.cast(w,tf.float32)\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=4800\n",
    "#add hidden layers\n",
    "conv_ip = keras.layers.Input(shape=(IMG_SIZE,),name='Input')\n",
    "xi=keras.layers.Reshape((IMG_SIZE, 1), input_shape=(IMG_SIZE,),name='reshape_1')(conv_ip)\n",
    "xi=keras.layers.BatchNormalization()(xi)\n",
    "x1=keras.layers.Conv1D(32,kernel_size=3,strides=2,activation='tanh',padding='same',name='second_conv16_5')(xi)\n",
    "c1=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv16_5')(x1)    #1600\n",
    "\n",
    "x2=keras.layers.BatchNormalization()(c1)\n",
    "x2=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_1')(x2)   #size 800,16\n",
    "x2=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='first_conv32_5')(x2)\n",
    "c2=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv32_5')(x2)\n",
    "\n",
    "x3=keras.layers.BatchNormalization()(c2)\n",
    "x3=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_2')(x3)    #size 400,32\n",
    "\n",
    "m1=keras.layers.Conv1D(256,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv128_5')(x3)\n",
    "m2=keras.layers.Conv1D(256,kernel_size=3,strides=1,dilation_rate=2,activation='tanh',padding='same',name='first_conv128_5')(m1)\n",
    "#m3=keras.layers.Conv1D(128,kernel_size=3,strides=1,dilation_rate=2,activation='tanh',padding='same',name='second_conv128_5')(m2)\n",
    "x4 = keras.layers.Concatenate(axis=1,name='midjoin')([m1,m2])\n",
    "x4=keras.layers.MaxPool1D(3,strides=2,data_format='channels_last',padding='same',name='maxpool_m')(x4)    \n",
    "\n",
    "x4 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='tanh', padding=\"same\", strides=2, name=\"decoder_conv_tran_1\")(x4)\n",
    "x4 =keras.layers.BatchNormalization()(x4)\n",
    "\n",
    "x5 = keras.layers.Concatenate(axis=2)([c2,x4])\n",
    "x5 = keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='relu',padding='same',name='dec_conv32_5')(x5)\n",
    "x5 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', padding=\"same\", strides=2, name=\"decoder_conv_tran_3\")(x5)\n",
    "x5 = keras.layers.BatchNormalization()(x5)\n",
    "\n",
    "x6 = keras.layers.Concatenate()([c1,x5]) #400\n",
    "x6 = keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='relu',padding='same',name='dec_conv8_5')(x6)\n",
    "x6 = keras.layers.Conv1DTranspose(1,kernel_size=3,strides=2,padding='same',name='semiop',activation='sigmoid')(x6)\n",
    "conv_op=keras.layers.Reshape((IMG_SIZE, ), input_shape=(IMG_SIZE,1),name='reshape_2')(x6)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "convNN.summary()\n",
    "convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005), loss=bce_dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history=convNN.fit(np.asarray(Xtrain),np.asarray(Mtrain), batch_size=64, epochs=20 , verbose=1, \n",
    "     validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('present_segment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_op=convNN.predict(np.array(Xtest))\n",
    "\n",
    "fig,ax=plt.subplots(10,2,figsize=(10,20))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('sample reconstructions')\n",
    "ar=np.random.randint(len(Xtest),size=10)\n",
    "ax[0][0].set_title('Generated')\n",
    "ax[0][1].set_title('Original')\n",
    "for i in range(0,10):\n",
    "    ax[i][0].plot(pred_op[ar[i]],color='black',ls='None',marker='.')\n",
    "    ax[i][1].plot(Mtest[ar[i]],color='red',ls='None',marker='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2train=np.loadtxt('training_data/X2train_inst_mask.csv',delimiter=',')\n",
    "X2test=np.loadtxt('training_data/X2test_inst_mask.csv',delimiter=',')\n",
    "Ytrain=np.loadtxt('training_data/Ytrain_inst_mask.csv',delimiter=',')\n",
    "Ytest=np.loadtxt('training_data/Xtest_inst_mask.csv',delimiter=',')\n",
    "\n",
    "print(X2train.shape,Ytrain.shape,X2test.shape,Ytest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2train=np.transpose(X2train).reshape(len(Ytrain),4,4800)\n",
    "X2test=np.transpose(X2test).reshape(len(Ytest),4,4800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2train=np.asarray([[(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in temp] for temp in X2train])\n",
    "#X2test=np.asarray([[(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in temp] for temp in Xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train=np.asarray([convNN.predict(el) for el in X2train])\n",
    "#mask_test=np.asarray([convNN.predict(el) for el in X2test])\n",
    "\n",
    "print(np.asarray(mask_train).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_train=np.asarray([[[1 if el>0.3 else 0 for el in y]for y in x ] for x in mask_train])\n",
    "#mask_test=np.asarray([[[1 if el>0.3 else 0 for el in y] for y in x ] for x in mask_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Itrain=np.multiply(X2train,mask_train)\n",
    "#Itest=np.multiply(X2test,mask_test)\n",
    "\n",
    "\n",
    "print(Itrain.shape,Ytrain.shape,Ytest.shape)\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "fig,ax=plt.subplots(10,1,figsize=(10,20))\n",
    "for i in range(0,10,2):\n",
    "    \n",
    "    ax[i].plot(X2train[vararr[i],1],label=Ytrain[vararr[i]])\n",
    "    ax[i].plot(Itrain[vararr[i],1],label=Ytrain[vararr[i]])\n",
    "    ax[i+1].plot(X2train[vararr[i],2],label=Ytrain[vararr[i]])\n",
    "    ax[i+1].plot(Itrain[vararr[i],2],label=Ytrain[vararr[i]])\n",
    "\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE=4800\n",
    "#add hidden layers\n",
    "class_ip = keras.layers.Input(shape=(IM_SIZE,4),name='Input')\n",
    "m = keras.layers.Flatten()(class_ip)\n",
    "m=keras.layers.Reshape((IM_SIZE*4, 1), input_shape=(IM_SIZE*4,),name='reshape_1')(m)\n",
    "m=keras.layers.BatchNormalization()(m)\n",
    "m=keras.layers.Conv1D(16,kernel_size=3,strides=2,activation='tanh',padding='same',name='second_conv16_5')(m)\n",
    "m=keras.layers.Conv1D(16,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv16_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_1')(m)\n",
    "m=keras.layers.Conv1D(32,kernel_size=3,strides=2,activation='tanh',padding='same',name='second_conv32_5')(m)\n",
    "m=keras.layers.Conv1D(32,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv32_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_2')(m)\n",
    "m=keras.layers.Conv1D(64,kernel_size=3,strides=2,activation='tanh',padding='same',name='second_conv64_5')(m)\n",
    "m=keras.layers.Conv1D(64,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv64_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_4')(m)\n",
    "m=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='second_conv128_5')(m)\n",
    "m=keras.layers.Conv1D(128,kernel_size=3,strides=1,activation='tanh',padding='same',name='third_conv128_5')(m)\n",
    "\n",
    "m=keras.layers.MaxPool1D(3,strides=2 ,data_format='channels_last',padding='same',name='maxpool_3')(m)\n",
    "m=keras.layers.Flatten(name='flat_1')(m)\n",
    "m=keras.layers.Dense(256,name='dense_layer_3',activation='relu')(m)\n",
    "m=keras.layers.Dense(256,name='dense_layer_5',activation='relu')(m)\n",
    "m=keras.layers.Dense(256,name='dense_layer_7',activation='relu')(m)\n",
    "m=keras.layers.Dense(1,name='dense_layer_4',activation='relu')(m)\n",
    "class_op=keras.layers.Dense(2,name='dense_layer_6',activation='softmax')(m)\n",
    "\n",
    "\n",
    "#convNN = keras.Model(inputs=[conv_ip,conv_ipl], outputs=conv_op,name='Convolutional_NN')\n",
    "classify = keras.Model(inputs=class_ip, outputs=class_op,name='Classifier_NN')\n",
    "\n",
    "\n",
    "classify.summary()\n",
    "classify.compile(optimizer=keras.optimizers.Adam(learning_rate=0.000005), loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itrain=[np.transpose(el) for el in Itrain]\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history2=classify.fit(np.asarray(Itrain),np.asarray(Ytrain), batch_size=64, epochs=40 , verbose=1 , shuffle=True,validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy\\n Can the NN work with the raw chunk?')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#  plt.savefig('rec_manual_mask.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = classify.evaluate(np.asarray(Itest), np.asarray(Ytest))\n",
    "print('Test accuracy:', test_acc)\n",
    "Ypred_raw=classify.predict(np.asarray(Itest))\n",
    "Ypred=np.argmax(Ypred_raw, axis=1)\n",
    "Ytest_new=np.argmax(Ytest,axis=1)\n",
    "cm = confusion_matrix(Ytest_new, Ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

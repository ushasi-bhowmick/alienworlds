{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import find_peaks,lombscargle\n",
    "from keras import backend as K\n",
    "import GetLightcurves as gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read reshape normalize input data\n",
    "Xtrain, Ytrain, Ltrain, TrainID = gc.read_tfr_record('../../training_data/cumulative_train',\n",
    "    ['input','map','label','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.bool, tf.string])\n",
    "\n",
    "Xval, Yval, Lval, ValID = gc.read_tfr_record('../../training_data/cumulative_val',\n",
    "    ['input','map','label','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.bool, tf.string])\n",
    "\n",
    "Xtest, Ytest, Ltest, TestID = gc.read_tfr_record('../../training_data/cumulative_test',\n",
    "    ['input','map','label','id'],\n",
    "    ['ar','ar','ar','b'], \n",
    "    [tf.float32, tf.bool, tf.bool, tf.string])\n",
    "\n",
    "Ytrain=np.asarray(Ytrain).reshape(len(Xtrain),4000,3)\n",
    "Yval=np.asarray(Yval).reshape(len(Xval),4000,3)\n",
    "Ytest=np.asarray(Ytest).reshape(len(Xtest),4000,3)\n",
    "print(Ytrain.shape, Ytest.shape)\n",
    "\n",
    "Xtrainf=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
    "Xtestf=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])\n",
    "Xvalf=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xval])\n",
    "\n",
    "TrainID = [str(TrainID[i])[2:11] for i in range(0,len(TrainID))]\n",
    "TestID = [str(TestID[i])[2:11] for i in range(0,len(TestID))]\n",
    "ValID = [str(ValID[i])[2:11] for i in range(0,len(ValID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training maps and all\n",
    "\n",
    "Ytrain = np.asarray(Ytrain, dtype = 'float32')\n",
    "Ytest = np.asarray(Ytest, dtype = 'float32')\n",
    "vararr=np.random.randint(0,len(Xtrain),size=10)\n",
    "vararr=[5,3]\n",
    "#vararr=[375,376,396,442,456,613]\n",
    "fig,ax=plt.subplots(2,2,figsize=(9,9))\n",
    "j=0\n",
    "\n",
    "xaxis = np.arange(0,4000,1)*29.5/1440\n",
    "plt.suptitle('Segmentation Signature',size=20)\n",
    "ax[0][0].set_title('Input',size=15)\n",
    "# ax[0][1].set_title('Output',size=15)\n",
    "ax[0][1].set_title('Result',size=15)\n",
    "\n",
    "ax[1][0].set_xlabel('Time(Days)',size=12)\n",
    "# ax[3][1].set_xlabel('Time(Days)',size=12)\n",
    "ax[1][1].set_xlabel('Time(Days)',size=12)\n",
    "for i in range(0,2):\n",
    "    ax[i][0].plot(np.arange(0,4000,1)*29.4/1440,Xtrain[vararr[i]],color='#077b8a',marker='.',ls='None')\n",
    "    #ax[i][1].plot(Ytrain[vararr[i],:,2])\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in Ytrain[vararr[i]]])\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    #predpl = np.where(Ytest[ar[i],:,0]==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "    ax[i][0].set_ylabel('Flux',size=12)\n",
    "    ax[i][1].plot(bkg*29.4/1440,Xtrain[vararr[i]][bkg],marker='.',ls='None',color='#a2d5c6',label='bkg')\n",
    "    ax[i][1].plot(pl*29.4/1440,Xtrain[vararr[i]][pl],marker='.',ls='None',color='#5c3c92',label='pl')\n",
    "    ax[i][1].plot(fps*29.4/1440,Xtrain[vararr[i]][fps],marker='.',ls='None',color='#d72631',label='fps')\n",
    "    # ax[i][1].plot(np.arange(0,4000,1)*29.4/1440,Ytrain[vararr[i],:,0],color='#5c3c92',label='pl')\n",
    "    # ax[i][1].plot(np.arange(0,4000,1)*29.4/1440,Ytrain[vararr[i],:,1],color='#d72631',label='fps')\n",
    "    # ax[0][1].legend()\n",
    "    ax[0][1].legend()\n",
    "    ax[i][1].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "    # ax[i][1].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "    ax[i][0].set_xlim(2000*29.4/1440,4000*29.4/1440)\n",
    "plt.savefig('fprez_segmentationex.jpg')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions defined\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 0.00001\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection +smooth) / (tf.reduce_sum(y_true_f) +  tf.reduce_sum(y_pred_f) +smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def weightFocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA): \n",
    "    w = tf.reduce_sum(targets,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 - w\n",
    "    #w = 1 / (w  + 0.00001)\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)   \n",
    "    \n",
    "    #inputs = K.flatten(inputs)\n",
    "    #targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(w*K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def log_cosh_dice_loss(y_true, y_pred):\n",
    "        x = generalized_dice_coeff(y_true, y_pred)\n",
    "        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)*0.5 + log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def generalized_dice_coeff(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w  + 0.00001)\n",
    "    #w = 1 - w\n",
    "    #w = w**2\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def generalized_dice_coeff_v2(y_true, y_pred):\n",
    "    # Compute weights: \"the contribution of each label is corrected by the inverse of its volume\"\n",
    "    w = tf.reduce_sum(y_true,(0,1))\n",
    "    w= w/tf.linalg.norm(w)\n",
    "    w = 1 / (w**2  + 0.00001)\n",
    "\n",
    "    z=tf.slice(w,[0],[2])\n",
    "    z2 = tf.slice(w,[2],[1])\n",
    "    z=tf.reduce_mean(z)\n",
    "    z2=tf.reduce_mean(z2)\n",
    "    w =  tf.stack([z,z*0.7,z2])\n",
    "\n",
    "    w=tf.cast(w,tf.float32)\n",
    "\n",
    "\n",
    "    numerator = y_true * y_pred\n",
    "    numerator = w * K.sum(numerator, (0, 1))\n",
    "    numerator = K.sum(numerator)\n",
    "\n",
    "    denominator = y_true + y_pred\n",
    "    denominator = w * K.sum(denominator, (0, 1))\n",
    "    denominator = K.sum(denominator)\n",
    "\n",
    "    gen_dice_coef = numerator / denominator\n",
    "\n",
    "    return 1 - 2 * gen_dice_coef\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred):\n",
    "    loss = FocalLoss(y_true, y_pred) + 1*log_cosh_dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_true, y_pred):\n",
    "    #wholesome thing.... lets see if this is any better\n",
    "    corrects = 0 \n",
    "    incorrects = 0\n",
    "    cm=[[0,0],[0,0]]\n",
    "    pred_arr=[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        tmp=[0,0]\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        #newpl=np.asarray([1 if(el>np.mean(newpl)+np.std(newpl)) else 0 for el in newpl])\n",
    "        #newfps=np.asarray([1 if(el>np.mean(newfps)+np.std(newfps)) else 0 for el in newfps])\n",
    "        if(np.any(y_true[i,:,0]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,0])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,0])\n",
    "            if(val2[0,1]>val1[0,1]): \n",
    "                corrects+=1\n",
    "                cm[0][0]+=1\n",
    "                tmp[0]=1\n",
    "            else: \n",
    "                incorrects+=1\n",
    "                cm[1][0]+=1\n",
    "                tmp[1]=1\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        #fps detect\n",
    "        if(np.any(y_true[i,:,1]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,1])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,1])\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                corrects+=1\n",
    "                cm[1][1]+=1\n",
    "                tmp[1]=1\n",
    "            else: \n",
    "                cm[0][1]+=1\n",
    "                incorrects+=1\n",
    "                tmp[0]=1\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        pred_arr.append(tmp)\n",
    "        #planet detection:\n",
    "    \n",
    "    print(corrects, incorrects)\n",
    "    print(np.asarray(cm)/np.asarray(cm).sum())\n",
    "    print(corrects/(corrects+incorrects))\n",
    "\n",
    "def corr_v2(l_true, y_pred, y_true):\n",
    "    cm=np.array([[0,0],[0,0]])\n",
    "    arr =[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        plorno=[0,0]\n",
    "\n",
    "        if(np.any(y_true[i,:,0]>0)): \n",
    "            plorno[0] = 1\n",
    "            corrpl1 = np.corrcoef(newpl, y_true[i,:,0])[0,1]\n",
    "            corrfps1 = np.corrcoef(newfps, y_true[i,:,0])[0,1]\n",
    "        else: corrpl1= corrfps1 = 0\n",
    "\n",
    "        if(np.any(y_true[i,:,1]>0)): \n",
    "            plorno[1] = 1\n",
    "            corrfps2 = np.corrcoef(newfps, y_true[i,:,1])[0,1]\n",
    "            corrpl2 = np.corrcoef(newpl, y_true[i,:,1])[0,1]\n",
    "        else: corrfps2 = corrpl2 = 0\n",
    "\n",
    "        if(np.all(np.array(plorno)==1)): \n",
    "            print(\"mult\")\n",
    "            #if(corrpl1>corrfps2): cm[0,0]+=1\n",
    "            #else: cm[1,1]+=1\n",
    "            if(l_true[i][0]==1): \n",
    "                arr.append(0)\n",
    "                cm[0,0]+=1\n",
    "            else: \n",
    "                arr.append(1)\n",
    "                cm[1,1]+=1\n",
    "            continue\n",
    "        if(corrfps2>corrpl2 and plorno[1] == 1):\n",
    "            arr.append(1)\n",
    "            cm[1,1]+=1\n",
    "        elif(corrfps1<corrpl1 and plorno[0] == 1): \n",
    "            arr.append(0)\n",
    "            cm[0,0]+=1\n",
    "        elif(corrfps1>corrpl1 and plorno[0]==1): \n",
    "            arr.append(1)\n",
    "            cm[0,1]+=1\n",
    "        elif(corrfps2<corrpl2 and plorno[1]==1): \n",
    "            arr.append(0)\n",
    "            cm[1,0]+=1\n",
    "\n",
    "    print(np.asarray(cm)/np.asarray(cm).sum())\n",
    "    return(np.asarray(arr))\n",
    "\n",
    "\n",
    "\n",
    "def corrarr(y_true, y_pred):\n",
    "    checkarr=[]\n",
    "    labarr=[]\n",
    "    for i in range(0, len(y_true)):\n",
    "        labvalpl = 0\n",
    "        labvalfps = 0\n",
    "        newpl=np.asarray(y_pred[i,:,0])\n",
    "        newfps=np.asarray(y_pred[i,:,1])\n",
    "        if(np.any(y_true[i,:,0]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,0])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,0])\n",
    "            if(val2[0,1]>val1[0,1]):\n",
    "                checkarr.append(0)\n",
    "            else: \n",
    "                checkarr.append(1)\n",
    "            labvalpl = val2[0,1]\n",
    "            #print(val1[0,1], val2[0,1])\n",
    "        #fps detect\n",
    "        if(np.any(y_true[i,:,1]>0)):\n",
    "            val1 = np.corrcoef(newfps, y_true[i,:,1])\n",
    "            val2 = np.corrcoef(newpl, y_true[i,:,1])\n",
    "            if(val1[0,1]>val2[0,1]): \n",
    "                checkarr.append(0)\n",
    "            else: \n",
    "                checkarr.append(1)\n",
    "            labvalfps = val1[0,1]\n",
    "        if(labvalfps>labvalpl): labarr.append(1)\n",
    "        else: labarr.append(0)\n",
    "    return(checkarr,labarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=4000\n",
    "def build_model_unpad(imsz):\n",
    "    conv_ip = keras.layers.Input(shape=(imsz,),name='Input')\n",
    "    xi=keras.layers.Reshape((imsz, 1), input_shape=(imsz,),name='reshape_1')(conv_ip)\n",
    "    #xi=keras.layers.Cropping1D(cropping=(100, 100))(xi)\n",
    "    xi=keras.layers.BatchNormalization()(xi)\n",
    "\n",
    "    x1=keras.layers.SeparableConv1D(16,kernel_size=3,activation='relu',name='1st16_5')(xi)  #3998, 32\n",
    "    c1=keras.layers.SeparableConv1D(16,kernel_size=3,strides=1,activation='relu',name='2nd16_3')(x1)  #3996, 32\n",
    "\n",
    "    x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "    x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1')(x2)  #1998, 64\n",
    "    #x2=keras.layers.Conv1D(16,kernel_size=1,strides=2,name='maxpool_1')(x2)  #1998, 64\n",
    "    x2=keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='relu',name='1st32_5')(x2) #1996, 64\n",
    "    c2=keras.layers.SeparableConv1D(32,kernel_size=5,strides=1,activation='relu',name='2nd32_3')(x2) #1992, 64\n",
    "\n",
    "    x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "    x3=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_2')(x3)  #996, 64\n",
    "    #x3=keras.layers.Conv1D(32,kernel_size=1,strides=2,name='maxpool_2')(x3)  #996, 64\n",
    "    x3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='relu',name='1st64_5')(x3) #994, 128\n",
    "    c3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='relu',name='2nd64_3')(x3) #992, 128\n",
    "\n",
    "    x4=keras.layers.BatchNormalization(name='bn3')(c3)\n",
    "    x4=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_3')(x4)  #496, 64\n",
    "    #x4=keras.layers.Conv1D(64,kernel_size=1,strides=2,name='maxpool_3')(x4)  #496, 128\n",
    "    x4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='relu',name='1st128_5')(x4)  #494, 256\n",
    "    c4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='relu',name='2nd128_5')(x4) #492, 256\n",
    "\n",
    "\n",
    "    x5=keras.layers.BatchNormalization(name='bn4')(c4) \n",
    "    x5=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_4')(x5) #246, 256 \n",
    "    #x5=keras.layers.Conv1D(128,kernel_size=1,strides=2,name='maxpool_4')(x5)  #246, 256  \n",
    "    x5=keras.layers.SeparableConv1D(512,kernel_size=3,strides=1,activation='relu',name='1st512_5')(x5)  #244, 256\n",
    "    x5=keras.layers.SeparableConv1D(512,kernel_size=3,strides=1,activation='relu',name='2nd512_5')(x5)  #242, 256\n",
    "\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=4, activation='relu', strides=2, name=\"T1st128_5\")(x5)  #486, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=5, activation='relu', strides=1, name=\"T2nd128_5\")(x5)  #490, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T3rd128_5\")(x5)  #492, 256\n",
    "    x5 =keras.layers.BatchNormalization(name='bn5')(x5) \n",
    "\n",
    "    #c4 = keras.layers.Cropping1D((2,2))(c4) #488, 256\n",
    "    x6 = keras.layers.Concatenate(axis=2, name='cn1')([c4,x5])  #492, 512\n",
    "    x6 = keras.layers.Conv1DTranspose(128,kernel_size=3,strides=1,activation='relu',name='3rd128_3')(x6) #494, 256\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=4, activation='relu', strides=2, name=\"T1st64_3\")(x6) #990, 128\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=1, name=\"T2nd64_3\")(x6) #992, 128\n",
    "    x6 = keras.layers.BatchNormalization(name='bn6')(x6)  \n",
    "\n",
    "    #c3 = keras.layers.Cropping1D((4,4))(c3) #984, 128\n",
    "    x7 = keras.layers.Concatenate(axis=2, name='cn2')([c3,x6]) #992, 256\n",
    "    x7 = keras.layers.Conv1DTranspose(64,kernel_size=3,strides=1,activation='relu',name='3rd64_3')(x7) #994, 128\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=4, activation='relu', strides=2, name=\"T1st32_3\")(x7) #1990, 64\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', strides=1, name=\"T2nd32_3\")(x7) #1992, 64\n",
    "    x7 = keras.layers.BatchNormalization(name='bn7')(x7)  \n",
    "\n",
    "    x8 = keras.layers.Concatenate(axis=2, name='cn3')([c2,x7])  #1992, 128\n",
    "    x8 = keras.layers.Conv1DTranspose(32,kernel_size=3,strides=1,activation='relu',name='3rd32_3')(x8)  #1994, 64\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=4,strides=2,activation='relu',name='T1st16_3')(x8) #3990, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T2nd16_3')(x8) #3994, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='T4rth16_3')(x8) #3996, 32\n",
    "    x8 = keras.layers.BatchNormalization(name='bn8')(x8)  \n",
    "\n",
    "    #c1 = keras.layers.Cropping1D((1,1))(c1) #3994, 32\n",
    "    x9 = keras.layers.Concatenate(axis=2, name='cn4')([c1,x8])  #3996, 64\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='3rd16_3')(x9) #3998, 32\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T3rd16_3')(x9) #4002, 32\n",
    "    x9 = keras.layers.BatchNormalization(name='bn9')(x9)\n",
    "\n",
    "    conv_op = keras.layers.Conv1D(3,kernel_size=3,strides=1,name='semiop',activation='softmax')(x9) # (4000, 3)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    convNN = keras.Model(inputs=conv_ip, outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=focal_dice_loss ,metrics=[dice_coeff] )\n",
    "    return(convNN)\n",
    "\n",
    "def build_model_pad(imsz):\n",
    "    conv_ip = keras.layers.Input(shape=(imsz,),name='Input')\n",
    "    xi=keras.layers.Reshape((imsz, 1), input_shape=(imsz,),name='reshape_1')(conv_ip)\n",
    "\n",
    "    x1=keras.layers.Conv1D(16,kernel_size=10,dilation_rate=3,activation='tanh',name='1st16_5', padding='same')(xi)  #3998, 32\n",
    "    c1=keras.layers.Conv1D(16,kernel_size=5,activation='tanh',name='2nd16_3', padding='same')(x1)  #3996, 32\n",
    "\n",
    "    x2=keras.layers.BatchNormalization(name='bn1')(c1)\n",
    "    x2=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_1', padding='same')(x2)  #1998, 64\n",
    "    x2=keras.layers.Conv1D(32,kernel_size=5,dilation_rate=2,strides=1,activation='tanh',name='1st32_5', padding='same')(x2) #1996, 64\n",
    "    c2=keras.layers.SeparableConv1D(32,kernel_size=3,strides=1,activation='tanh',name='2nd32_3', padding='same')(x2) #1992, 64\n",
    "\n",
    "    x3=keras.layers.BatchNormalization(name='bn2')(c2) \n",
    "    x3=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_2', padding='same')(x3)  #996, 64\n",
    "    x3=keras.layers.Conv1D(64,kernel_size=5,dilation_rate=2,strides=1,activation='tanh',name='1st64_5', padding='same')(x3) #994, 128\n",
    "    c3=keras.layers.SeparableConv1D(64,kernel_size=3,strides=1,activation='tanh',name='2nd64_3', padding='same')(x3) #992, 128\n",
    "\n",
    "    x4=keras.layers.BatchNormalization(name='bn3')(c3)\n",
    "    x4=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_3', padding='same')(x4)  #496, 64\n",
    "    x4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='1st128_5', padding='same')(x4)  #494, 256\n",
    "    c4=keras.layers.SeparableConv1D(128,kernel_size=3,strides=1,activation='tanh',name='2nd128_5', padding='same')(x4) #492, 256\n",
    "\n",
    "    x_5=keras.layers.BatchNormalization(name='bn4')(c4) \n",
    "    x_5=keras.layers.MaxPool1D(2,strides=2 ,data_format='channels_last',name='maxpool_4', padding='same')(x_5) #246, 256 \n",
    "    x_5=keras.layers.SeparableConv1D(256,kernel_size=3,strides=1,activation='tanh',name='1st512_5', padding='same')(x_5)  #244, 256\n",
    "    x_5=keras.layers.SeparableConv1D(256,kernel_size=3,strides=1,activation='tanh',name='2nd512_5', padding='same')(x_5)  #242, 256\n",
    "    #x_5=keras.layers.SeparableConv1D(256,kernel_size=3,strides=1,activation='tanh',name='3rd512_5', padding='same')(x_5)  #242, 256\n",
    "\n",
    "    #tdepth, snr\n",
    "    extra_ip = keras.layers.Input(shape=(2,),name='Input2')\n",
    "    xi2=keras.layers.RepeatVector(250)(extra_ip)\n",
    "    xi2 = keras.layers.Conv1D(256, 3, padding='same')(xi2)\n",
    "    x5 = keras.layers.Multiply()([xi2, x_5])\n",
    "\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=2, name=\"T1st128_5\", padding='same')(x5)  #486, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T2nd128_5\", padding='same')(x5)  #490, 256\n",
    "    x5 = keras.layers.Conv1DTranspose(128, kernel_size=3, activation='relu', strides=1, name=\"T3rd128_5\", padding='same')(x5)  #492, 256\n",
    "    x5 =keras.layers.BatchNormalization(name='bn5')(x5) \n",
    "\n",
    "    x6 = keras.layers.Concatenate(axis=2, name='cn1')([c4,x5])  #492, 512\n",
    "    x6 = keras.layers.Conv1DTranspose(128,kernel_size=3,strides=1,activation='relu',name='3rd128_3', padding='same')(x6) #494, 256\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=2, name=\"T1st64_3\", padding='same')(x6) #990, 128\n",
    "    x6 = keras.layers.Conv1DTranspose(64, kernel_size=3, activation='relu', strides=1, name=\"T2nd64_3\", padding='same')(x6) #992, 128\n",
    "    x6 = keras.layers.BatchNormalization(name='bn6')(x6)  \n",
    "\n",
    "    x7 = keras.layers.Concatenate(axis=2, name='cn2')([c3,x6]) #992, 256\n",
    "    x7 = keras.layers.Conv1DTranspose(64,kernel_size=3,strides=1,activation='relu',name='3rd64_3', padding='same')(x7) #994, 128\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=3, activation='relu', strides=2, name=\"T1st32_3\", padding='same')(x7) #1990, 64\n",
    "    x7 = keras.layers.Conv1DTranspose(32, kernel_size=5, activation='relu', strides=1, name=\"T2nd32_3\", padding='same')(x7) #1992, 64\n",
    "    x7 = keras.layers.BatchNormalization(name='bn7')(x7)  \n",
    "\n",
    "    x8 = keras.layers.Concatenate(axis=2, name='cn3')([c2,x7])  #1992, 128\n",
    "    x8 = keras.layers.Conv1DTranspose(32,kernel_size=3,strides=1,activation='relu',name='3rd32_3', padding='same')(x8)  #1994, 64\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=2,activation='relu',name='T1st16_3', padding='same')(x8) #3990, 32\n",
    "    x8 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T2nd16_3', padding='same')(x8) #3994, 32\n",
    "    x8 = keras.layers.BatchNormalization(name='bn8')(x8)  \n",
    "\n",
    "    #c1 = keras.layers.Cropping1D((1,1))(c1) #3994, 32\n",
    "    x9 = keras.layers.Concatenate(axis=2, name='cn4')([c1,x8])  #3996, 64\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=3,strides=1,activation='relu',name='3rd16_3', padding='same')(x9) #3998, 32\n",
    "    x9 = keras.layers.Conv1DTranspose(16,kernel_size=5,strides=1,activation='relu',name='T3rd16_3', padding='same')(x9) #4002, 32\n",
    "    x9 = keras.layers.BatchNormalization(name='bn9')(x9)\n",
    "\n",
    "    conv_op = keras.layers.Conv1D(2,kernel_size=1,strides=1,name='semiop',activation='sigmoid', padding='same')(x9) # (4000, 3)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    convNN = keras.Model(inputs=[conv_ip,extra_ip], outputs=conv_op,name='Convolutional_NN')\n",
    "\n",
    "    convNN.summary()\n",
    "    convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=focal_dice_loss ,metrics=[generalized_dice_coeff] )\n",
    "    return(convNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainf,X_valf,Y_train,Y_val = train_test_split(Xtrainf, Ytrain, test_size=0.2, shuffle=True)\n",
    "print(X_trainf.shape, X_valf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN=build_model_unpad(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history=convNN.fit(np.asarray(-Xtrainf),np.asarray(Ytrain), batch_size=16, epochs=15, verbose=1,\n",
    "     validation_data=(-Xvalf, Yvalf))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN.save('fin_joint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_op=convNN.predict(np.array(-Xtestf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot probability spaces\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "Xtest = np.asarray(Xtest)\n",
    "con = 29.4/1440\n",
    "xax = np.arange(0,4000)\n",
    "nmp = cm.get_cmap('twilight_shifted', 256)\n",
    "nnmp = ListedColormap(nmp(np.linspace(0.65,1, 256)))#\n",
    "fig,ax=plt.subplots(4,2,figsize=(10,12))\n",
    "plt.style.use('seaborn-bright')\n",
    "plt.suptitle('Network Output', size=14)\n",
    "#ar=np.random.randint(len(Xtest),size=10)\n",
    "ar=[6,19,22,43]\n",
    "ax[0][0].set_title('Planet', size=13)\n",
    "ax[0][1].set_title('False Positive', size=13)\n",
    "for i in range(0,4):\n",
    "    pcounts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in Ytest[ar[i],:,:]])\n",
    "    ppl = np.where(pcounts==0)[0]\n",
    "    pfps = np.where(pcounts==1)[0]\n",
    "    mn = min(Xtestf[ar[i]])\n",
    "    mx = max(Xtestf[ar[i]])\n",
    "    if(len(ppl)>0): ax[i][0].vlines(x=con*xax[ppl],ymin=2*mn, ymax=2*mx, alpha=0.5, color='#F0C0FE', zorder=0)\n",
    "    if(len(pfps)>0): ax[i][1].vlines(x=con*xax[pfps],ymin=2*mn, ymax=2*mx, alpha=0.5, color='#F0C0FE', zorder=0)\n",
    "    #m = min(Xtestf)\n",
    "    \n",
    "\n",
    "    ax[i][0].scatter(con*xax,Xtestf[ar[i]],c=pred_op[ar[i],:,0],cmap=nnmp,ls='None',marker='.',s=10,label='data',\n",
    "        vmin=0, vmax=1, zorder=2)\n",
    "    #ax[i][0].plot(-pred_op[ar[i],:,2],color='yellow',ls='None',marker='.',label='bkg')\n",
    "\n",
    "    im = ax[i][1].scatter(con*xax,Xtestf[ar[i],:],c=pred_op[ar[i],:,1],cmap=nnmp,ls='None',marker='.',\n",
    "        s=10,label='data', vmin=0, vmax=1, zorder=2)\n",
    "\n",
    "    ax[i][0].set_ylim(mn*1.01,mx*1.01)\n",
    "    ax[i][1].set_ylim(mn*1.01,mx*1.01)\n",
    "    ax[i][0].set_xlim(50,80)\n",
    "    ax[i][1].set_xlim(50,80)\n",
    "\n",
    "#ax[3][0].set_xlabel('Time', size=12)\n",
    "fig.text(0.5, 0.3, 'Normalized Flux', ha='center', size=13)\n",
    "fig.text(0.05, 0.6, 'Time(days)', va='center', rotation='vertical', size=13)\n",
    "#ax[3][1].set_xlabel('Time', size=12)\n",
    "plt.colorbar(im, ax=ax.ravel().tolist(),label=\"Probability\", orientation=\"horizontal\")\n",
    "#plt.colorbar()\n",
    "#plt.savefig('pap_network_op.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tf.keras.metrics.MeanIoU(num_classes=3)\n",
    "#tpred=[[[max(x[0], x[1]), x[2]] for x in el] for el in pred_op]\n",
    "#ttest=[[[max(x[0], x[1]), x[2]] for x in el] for el in Ytest]\n",
    "n.update_state(np.around(Ytest[:300,:,:]), np.around(pred_op[:300,:,:]))\n",
    "n.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ytrainpred=convNN.predict(-Xtrainf)\n",
    "Yvalpred=convNN.predict(-Xvalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_test = corr_v2(Ltest, pred_op, Ytest)\n",
    "lab_train = corr_v2(Ltrain, Ytrainpred,Ytrain)\n",
    "lab_val = corr_v2(Lval, Yvalpred, Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lab_test), len(Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=np.asarray([[TestID[i],lab_test[i]] for i in range(len(lab_test))], dtype='object')\n",
    "gc.write_tfr_record('../../training_data/jointanalysis_test_2',net,['id','pred_label'],['b','ar']\n",
    "    ,['string','bool'])\n",
    "\n",
    "net=np.asarray([[TrainID[i],lab_train[i]] for i in range(len(lab_train))], dtype='object')\n",
    "gc.write_tfr_record('../../training_data/jointanalysis_train_2',net,['id','pred_label'],['b','ar']\n",
    "    ,['string','bool'])\n",
    "\n",
    "net=np.asarray([[ValID[i],lab_val[i]] for i in range(len(lab_val))], dtype='object')\n",
    "gc.write_tfr_record('../../training_data/jointanalysis_val_2',net,['id','pred_label'],['b','ar']\n",
    "    ,['string','bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coeff(Ytest, pred_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pixel accuracy ratio\n",
    "ptest=[np.argmax(np.array([el[0],el[1],el[2]])) for el in np.reshape(pred_op,(len(pred_op)*4000,3))]\n",
    "ppred=[np.argmax(el) for el in np.reshape(Ytest,(len(Ytest)*4000,3))]\n",
    "\n",
    "cm=confusion_matrix(ptest, ppred)\n",
    "print(cm/cm.sum())\n",
    "print((cm[0,0]+cm[1,1]+cm[2,2])/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy in transit detection and classification\n",
    "\n",
    "def prec_rec_detect(thres):\n",
    "    cm=[0,0]\n",
    "    cm2=[0,0]\n",
    "    tot =0\n",
    "    tot2=0\n",
    "    for i in range(len(pred_op)):\n",
    "        pl = pred_op[i,:,0] + pred_op[i,:,1]\n",
    "        #hpl = np.mean(pl)+thres*np.std(pl)\n",
    "        #hpl2 = np.mean(pl)+2*thres*np.std(pl)\n",
    "        hpl = thres\n",
    "        xpl= Ytest[i,:,0] + Ytest[i,:,1]\n",
    "        plp, _ = find_peaks(pl, height=hpl,distance=5)\n",
    "        xplp, _ = find_peaks(xpl, height=0.5,distance=5)\n",
    "        tot += len(plp)\n",
    "        tot2 += len(xplp)\n",
    "        for el in plp:\n",
    "            if(xpl[el]>0): cm[0]+=1\n",
    "            else: cm[1]+=1\n",
    "        for el in xplp:\n",
    "            if(pl[el]<hpl): cm2[1]+=1\n",
    "            else: cm2[0]+=1\n",
    "\n",
    "    return(cm[0]/tot, cm2[0]/tot)\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision and recall in transit detection\n",
    "prec = []\n",
    "rec = []\n",
    "for thr in np.linspace(0,5,5):\n",
    "    p, r =prec_rec_detect(thr)\n",
    "    prec.append(p)\n",
    "    rec.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rec, prec)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxarr_pred = [[ np.argmax(np.asarray([x[0],x[1],x[2]])) for x in el] for el in pred_op]\n",
    "maxarr_predpl  =[[x==0 for x in el] for el in maxarr_pred]\n",
    "maxarr_predfps  =[[x==1 for x in el] for el in maxarr_pred]\n",
    "\n",
    "corrects=np.asarray([[0,0],[0,0]])\n",
    "newc = 0\n",
    "newinc = 0\n",
    "for i in range(0,len(pred_op)):\n",
    "    tick = 0\n",
    "    ntick = 0\n",
    "    plp, _ = find_peaks(maxarr_predpl[i], height=0.0,distance=20) \n",
    "    for m in plp:\n",
    "        if(Ytest[i,m,0]>0.5): \n",
    "            corrects[0,0]+=1\n",
    "            tick+=1\n",
    "        if(Ytest[i,m,1]>0.5): \n",
    "            corrects[1,0]+=1\n",
    "            ntick+=1\n",
    "        #if(Ytrain[i,m,2]>0.5): corrects[2,0]+=1\n",
    "        #else: corrects[2,0]+=1\n",
    "    fpsp, _ = find_peaks(maxarr_predfps[i], height=0.0,distance=20)\n",
    "    for m in fpsp:\n",
    "        if(Ytest[i,m,1]>0.5): \n",
    "            corrects[1,1]+=1\n",
    "            tick+=1\n",
    "        if(Ytest[i,m,0]>0.5): \n",
    "            corrects[0,1]+=1\n",
    "            ntick+=1\n",
    "        #if(Ytrain[i,m,2]>0.1): corrects[0,2]+=1\n",
    "        #else: corrects[0,2]+=1\n",
    "    if(tick>ntick): newc+=1\n",
    "    else: newinc+=1\n",
    "\n",
    "print(corrects/corrects.sum())\n",
    "print(newinc,newc, newinc/(newc+newinc))\n",
    "print((corrects[0,0]+corrects[1,1])/corrects.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNN = keras.models.load_model('fin_joint.h5',custom_objects={'focal_dice_loss': focal_dice_loss, 'dice_coeff':dice_coeff})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for planet signatures, and anomalies and associated stuff\n",
    "# read transit sig... make lomb scargle... right now, predict and redirect NN to the ones which promise best results\n",
    "entries = os.listdir('../../processed_directories/full_lcs_all/')\n",
    "ct=0\n",
    "for el in entries:\n",
    "    input,mask, counts, tdur, tperiod, tdepth = gc.read_tfr_record('../../processed_directories/full_lcs_all/'+el,\n",
    "        ['input','mask','counts','tdur','tperiod','tdepth'],['ar','ar','ar','ar','ar','ar'],\n",
    "        [tf.float32,tf.bool, tf.int8,tf.float16,tf.float16,tf.float16])\n",
    "    tdur = np.array(tdur)\n",
    "    tperiod = np.array(tperiod)\n",
    "    tdepth = np.array(tdepth)\n",
    "    input = np.asarray(input)\n",
    "    if(tdur.shape[1] and tperiod.shape[1] and tdepth.shape[1]):\n",
    "        ct+=1\n",
    "        continue\n",
    "    else:\n",
    "        #print(tdur.shape, tperiod.shape, tdepth.shape)\n",
    "        print(tdur)\n",
    "        maskf=np.asarray(mask).reshape(len(input),4000,3)\n",
    "        mid = np.median(input)\n",
    "        mn = min(input.reshape(-1))\n",
    "        try: inputf=np.asarray([(row-mid)/(-mn+mid) for row in input])\n",
    "        except: \n",
    "            print('norm error',el)\n",
    "            continue\n",
    "        pred_mask = convNN.predict(-inputf)\n",
    "        valp = [np.corrcoef(maskf[i,:,1],pred_mask[i,:,0])[0][1] if(np.any(maskf[i,:,1])>0) else 0 for i in range(0,len(inputf))]\n",
    "        valf = [np.corrcoef(maskf[i,:,0],pred_mask[i,:,1])[0][1] if(np.any(maskf[i,:,0])>0) else 0 for i in range(0,len(inputf))]\n",
    "        print(max(valp), max(valf))\n",
    "        if(max(valp)> 0.5 or max(valf)>0.5):\n",
    "            pred_mask = pred_mask.reshape(len(input), 12000)\n",
    "            net = np.asarray([[input[i],mask[i],pred_mask[i],counts[i],tdur[i],tperiod[i],tdepth[i], valp[i], valf[i]] \n",
    "                for i in range(0,len(counts))],dtype='object')\n",
    "            gc.write_tfr_record('../../processed_directories/full_lcs_ext_best_hyp/'+el,net,\n",
    "            ['input','mask','pred_mask','counts','tdur','tperiod','tdepth','corr_pl','corr_fps'],\n",
    "            ['ar','ar','ar','ar','ar','ar','ar','fl','fl'],\n",
    "            ['float32','bool', 'float16','int8','float16','float16','float16','float16','float16'])\n",
    "print(ct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3047 data points have been properly vetted in catalog... meaning their transit shapes and depths and all are defined well\n",
    "# 1166 out of these have been predicted well by the neural network\n",
    "# check how close the obtained values of periodicity and depth are to the actual predicted values.\n",
    "\n",
    "def periodogram_and_freq(chunks, minp, maxp,ch=0, prec=10000):\n",
    "    p = np.linspace(minp,maxp,prec)\n",
    "    f = 2*np.pi /p\n",
    "    tot = np.concatenate(chunks, axis=0)\n",
    "    #thres = np.mean(tot)\n",
    "    #tot = np.asarray([1 if(el>thres) else 0 for el in tot])\n",
    "    #print(tot.shape, thres)\n",
    "    #print(tot.shape)\n",
    "    tot = [np.argmax(el) for el in tot]\n",
    "    #print(len(tot))\n",
    "    tot = np.asarray(np.asarray(tot)==ch)\n",
    "    #print(tot)\n",
    "    \n",
    "    predp = lombscargle(np.arange(0,len(tot))*29.4/1440, tot,f, normalize=True)\n",
    "    return(p, predp)\n",
    "\n",
    "def color_a_peak_v2(input, el):\n",
    "    req_p = int(el * 24 *60/29.4)\n",
    "    phf = np.asarray([input[i:i+req_p] for i in range(0,len(input)-req_p,req_p)])\n",
    "    phf = np.mean(phf, axis = 0)\n",
    "    md = np.median(phf)\n",
    "    std = np.std(phf)\n",
    "    pts = np.asarray([i for i in range(len(phf)) if(phf[i]<md-0.7*std)])\n",
    "    newpts = pts\n",
    "    x=len(phf)\n",
    "    while(x<68000):\n",
    "        #print(x, len(pts))\n",
    "        #print(len(pts+x*np.ones(len(pts))))\n",
    "        newpts = np.concatenate([newpts,np.asarray(pts+x*np.ones(len(pts)),dtype='int')])\n",
    "        x+=len(phf)\n",
    "    #print(newpts)\n",
    "    newpts = [el for el in newpts if el<68000] \n",
    "    return(newpts)\n",
    "\n",
    "def what_the_peak(per, predp, ip, thres=1):\n",
    "    newp=[]\n",
    "    h = np.median(predp)+thres*np.std(predp)\n",
    "    totip = np.concatenate(ip, axis=0)\n",
    "\n",
    "    plp, _ = find_peaks(np.asarray(predp), height=h, distance=40)\n",
    "    peakf = per[plp]\n",
    "    #print(peakf)\n",
    "    peakinfo = np.asarray([predp[plp],per[plp]])\n",
    "    sortedArr = peakinfo [ :, peakinfo[0].argsort()]\n",
    "    mn = np.mean(totip)\n",
    "    #print(sortedArr)\n",
    "    other = np.arange(0,len(totip),1)\n",
    "    for p in np.flip(np.asarray(sortedArr[1])):\n",
    "        #print(p)\n",
    "        mask = color_a_peak_v2(totip, p)\n",
    "        nother = np.setdiff1d(other,mask)\n",
    "        #tmn = np.mean(totip[other])\n",
    "        #print(p,len(other),len(nother),(len(other)-len(nother))/len(mask))\n",
    "        if((len(other)-len(nother))/len(mask) > 0.15):\n",
    "            newp.append(p)\n",
    "            other = nother\n",
    "        #print(mn, tmn)\n",
    "        #if(tmn>mn): \n",
    "        #    mn = tmn\n",
    "        #    newp.append(p)\n",
    "        #else: continue\n",
    "    return(newp)\n",
    "    #choose good ones.\n",
    "\n",
    "def plot_a_map(input, map):\n",
    "    totip = np.concatenate(input)\n",
    "    totop = np.concatenate(map, axis=0)\n",
    "\n",
    "    counts=np.asarray([np.argmax([el[0],el[1],el[2]]) for el in totop])\n",
    "    pl=np.where(counts==0)[0]\n",
    "    fps=np.where(counts==1)[0]\n",
    "    bkg=np.where(counts==2)[0]\n",
    "\n",
    "    m=min(totip)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Total Segmentation Map')\n",
    "    plt.xlabel('time (days)')\n",
    "    plt.ylabel('flux')\n",
    "    #plt.plot(np.arange(0,len(totip))*29.4/1440,totop[:,0])\n",
    "    #plt.plot(np.arange(0,len(totip))*29.4/1440,totop[:,1])\n",
    "    plt.plot(np.asarray(bkg)*29.4/1440,totip[bkg], color='#a4b3b6', marker='.', ls='none', label='bkg')\n",
    "    plt.plot(np.asarray(fps)*29.4/1440,totip[fps], color='#44318d', marker='.', ls='none', label='fps')\n",
    "    plt.plot(np.asarray(pl)*29.4/1440,totip[pl], color='#d83f87', marker='.', ls='none', label='pl')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for planet signatures, and anomalies and associated stuff\n",
    "# read transit sig... make lomb scargle... right now, predict and redirect NN to the ones which promise best results\n",
    "\n",
    "#start at 570 next time\n",
    "entries = os.listdir('../../processed_directories/full_lcs_best_results/correlation_results')\n",
    "for el in entries[10:800]:\n",
    "    input,mask, pred_mask, counts, tdur, tperiod, tdepth, corr_pl, corr_fps = gc.read_tfr_record('../../processed_directories/full_lcs_best_results/correlation_results/'+el,\n",
    "        ['input','mask','pred_mask','counts','tdur','tperiod','tdepth','corr_pl','corr_fps'],\n",
    "        ['ar','ar','ar','ar','ar','ar','ar','fl','fl'],\n",
    "        ['float32','bool', 'float16','int8','float16','float16','float16','float16','float16'])\n",
    "    tdur = np.array(tdur)\n",
    "    tperiod = np.array(tperiod)\n",
    "    tdepth = np.array(tdepth)\n",
    "    input = np.asarray(input)\n",
    "\n",
    "    ch = int(max(np.asarray(corr_fps))>max(np.asarray(corr_pl)))\n",
    "    #print(max(np.asarray(corr_fps)), max(np.asarray(corr_pl)), ch)\n",
    "\n",
    "    mask=np.asarray(mask).reshape(len(input),4000,3)\n",
    "    pred_mask = np.asarray(pred_mask).reshape(len(input),4000,3)\n",
    "    \n",
    "    p,predp = periodogram_and_freq (pred_mask, 10,70000*29.4*0.5/1440, 1-ch, prec = 3000)\n",
    "    segmapp_sm, segmappredp_sm = periodogram_and_freq (pred_mask, 2*29.4/1440,10, 1-ch, prec = 3000)  \n",
    " \n",
    "    try:\n",
    "        newp2 = what_the_peak(segmapp_sm, segmappredp_sm, np.asarray(input), 0.5)\n",
    "        newp1 = what_the_peak(p, predp, np.asarray(input), 0.2)\n",
    "        newp = np.concatenate(np.asarray([newp1,newp2]))\n",
    "    except: continue\n",
    " \n",
    "    #print(newp, tperiod[0])\n",
    "\n",
    "    net = np.asarray([[input[i],tdur[i],tperiod[i],tdepth[i],newp,p,predp,segmapp_sm, segmappredp_sm] \n",
    "        for i in range(0,len(counts))],dtype='object')\n",
    "    gc.write_tfr_record('../../processed_directories/full_lcs_best_results/lomb_sc_results/'+el,net,\n",
    "            ['input','tdur','tperiod','tdepth','pred_period','p_l','ls_l','p_sm','ls_m'],\n",
    "            ['ar','ar','ar','ar','ar','ar','ar','ar','ar'],\n",
    "            ['float32','float16','float16','float16','float16','float16','float16','float16','float16'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

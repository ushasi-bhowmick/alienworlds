{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#attempt to improve the NN\n",
                "#add the local and the global view construct coz transit false positive mismatch seems to be a major problem\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "import pandas as pd\n",
                "import GetLightcurves as gc\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#network and training parameters:\n",
                "BATCH_SIZE = 128\n",
                "VERBOSE = 1 #no idea what this is but lets see...\n",
                "VAL_SPLIT = 0.12 #how much of sample is reserved for validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#read the data from the official S and V nn\n",
                "# Create a description of the features.\n",
                "TRAIN_MOD = '../../training_data/'\n",
                "Xtrain=[]\n",
                "Ytrain=[]\n",
                "Xtrainl=[]\n",
                "Xval=[]\n",
                "Yval=[]\n",
                "Xvall=[]\n",
                "Xtest=[]\n",
                "Ytest=[]\n",
                "Xtestl=[]\n",
                "\n",
                "feature_description = {\n",
                "    'global_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
                "    'local_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
                "    'av_training_set': tf.io.FixedLenFeature([], tf.string, default_value=''), \n",
                "}\n",
                "\n",
                "def _parse_function(example_proto):\n",
                "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
                "  return tf.io.parse_single_example(example_proto, feature_description)\n",
                "\n",
                "def one_hot(val):\n",
                "    val=val.decode(\"utf-8\") \n",
                "    if(val=='AFP'): return([0,1])\n",
                "    elif(val=='PC'):  return([1,0])\n",
                "    elif(val=='NTP'): return([0,1])\n",
                "\n",
                "raw_data1=tf.data.TFRecordDataset([TRAIN_MOD+'train-00000-of-00008'])\n",
                "raw_data2=tf.data.TFRecordDataset([TRAIN_MOD+'train-00001-of-00008'])\n",
                "raw_data3=tf.data.TFRecordDataset([TRAIN_MOD+'train-00002-of-00008'])\n",
                "raw_data4=tf.data.TFRecordDataset([TRAIN_MOD+'train-00003-of-00008'])\n",
                "raw_data5=tf.data.TFRecordDataset([TRAIN_MOD+'train-00004-of-00008'])\n",
                "raw_data6=tf.data.TFRecordDataset([TRAIN_MOD+'train-00005-of-00008'])\n",
                "raw_data7=tf.data.TFRecordDataset([TRAIN_MOD+'train-00006-of-00008'])\n",
                "raw_data8=tf.data.TFRecordDataset([TRAIN_MOD+'train-00007-of-00008'])\n",
                "raw_val=tf.data.TFRecordDataset([TRAIN_MOD+'val-00000-of-00001'])\n",
                "raw_test=tf.data.TFRecordDataset([TRAIN_MOD+'test-00000-of-00001'])\n",
                "\n",
                "parsed_dataset1 = raw_data1.map(_parse_function)\n",
                "parsed_dataset2 = raw_data2.map(_parse_function)\n",
                "parsed_dataset3 = raw_data3.map(_parse_function)\n",
                "parsed_dataset4 = raw_data4.map(_parse_function)\n",
                "parsed_dataset5 = raw_data5.map(_parse_function)\n",
                "parsed_dataset6 = raw_data6.map(_parse_function)\n",
                "parsed_dataset7 = raw_data7.map(_parse_function)\n",
                "parsed_dataset8 = raw_data8.map(_parse_function)\n",
                "parsed_val = raw_val.map(_parse_function)\n",
                "parsed_test = raw_test.map(_parse_function)\n",
                "\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset1]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset2]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset3]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset4]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset5]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset6]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset7]\n",
                "[Ytrain.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_dataset8]\n",
                "[Yval.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_val]\n",
                "[Ytest.append(one_hot(raw['av_training_set'].numpy())) for raw in parsed_test]\n",
                "\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset1]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset2]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset3]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset4]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset5]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset6]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset7]\n",
                "[Xtrain.append(raw['global_view'].numpy()) for raw in parsed_dataset8]\n",
                "[Xval.append(raw['global_view'].numpy()) for raw in parsed_val]\n",
                "[Xtest.append(raw['global_view'].numpy()) for raw in parsed_test]\n",
                "\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset1]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset2]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset3]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset4]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset5]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset6]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset7]\n",
                "[Xtrainl.append(raw['local_view'].numpy()) for raw in parsed_dataset8]\n",
                "[Xvall.append(raw['local_view'].numpy()) for raw in parsed_val]\n",
                "[Xtestl.append(raw['local_view'].numpy()) for raw in parsed_test]\n",
                "\n",
                "Xtrain=np.array(Xtrain)\n",
                "Xval=np.array(Xval)\n",
                "Xtest=np.array(Xtest)\n",
                "Xtrainl=np.array(Xtrainl)\n",
                "Xvall=np.array(Xvall)\n",
                "Xtestl=np.array(Xtestl)\n",
                "Ytrain=np.array(Ytrain)\n",
                "Ytest=np.array(Ytest)\n",
                "Yval=np.array(Yval)\n",
                "print(Xtrain.shape,Xval.shape,Xtest.shape)\n",
                "print(Xtrainl.shape,Xvall.shape,Xtestl.shape)\n",
                "print(Ytrain.shape,Yval.shape,Ytest.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('training planets:',(Ytrain[:,0]==1).sum())\n",
                "print('training fps:',(Ytrain[:,0]==0).sum())\n",
                "print('test planets:',(Ytest[:,0]==1).sum())\n",
                "print('test fps:',(Ytest[:,0]==0).sum())\n",
                "\n",
                "minind=min((Ytrain[:,0]==1).sum(),(Ytrain[:,0]==0).sum())\n",
                "filtind=[i for i in range(0,len(Ytrain)) if (Ytrain[i]==np.array([1,0])).all()]\n",
                "filtind2=[i for i in range(0,len(Ytrain)) if (Ytrain[i]==np.array([0,1])).all()]\n",
                "print(len(filtind),len(filtind2)) \n",
                "#print(min(len(filtind[0]),len(filtind2[0])))\n",
                "temp1=[]\n",
                "temp2=[] \n",
                "templ=[]  \n",
                "for i in range(0,min(len(filtind),len(filtind2))):\n",
                "    temp1.append(Xtrain[filtind[i]])\n",
                "    temp2.append(Ytrain[filtind[i]])\n",
                "    templ.append(Xtrainl[filtind[i]])\n",
                "    temp1.append(Xtrain[filtind2[i]])\n",
                "    temp2.append(Ytrain[filtind2[i]])\n",
                "    templ.append(Xtrainl[filtind2[i]])\n",
                "\n",
                "Xtrain=np.array(temp1)\n",
                "Ytrain=np.array(temp2)\n",
                "Xtrainl=np.array(templ)\n",
                "\n",
                "temp1=[]\n",
                "temp2=[] \n",
                "templ=[]  \n",
                "filtind=[i for i in range(0,len(Yval)) if (Yval[i]==np.array([1,0])).all()]\n",
                "filtind2=[i for i in range(0,len(Yval)) if (Yval[i]==np.array([0,1])).all()]\n",
                "print(len(filtind),len(filtind2))\n",
                "for i in range(0,min(len(filtind),len(filtind2))):\n",
                "    temp1.append(Xval[filtind[i]])\n",
                "    temp2.append(Yval[filtind[i]])\n",
                "    templ.append(Xvall[filtind[i]])\n",
                "    temp1.append(Xval[filtind2[i]])\n",
                "    temp2.append(Yval[filtind2[i]])\n",
                "    templ.append(Xvall[filtind2[i]])\n",
                "\n",
                "Xval=np.array(temp1)\n",
                "Yval=np.array(temp2)\n",
                "Xvall=np.array(templ)\n",
                "\n",
                "print(Xtrain.shape,Xval.shape)\n",
                "print(Xtrainl.shape,Xvall.shape)\n",
                "print(Ytrain.shape,Yval.shape)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#my training sample\n",
                "Xtrainl,Xtrain,Ytrain,IDtrain = gc.read_tfr_record('../../training_data/cumulative_train',['local','global','label','id'],\n",
                "    ['ar','ar','ar','b'],[tf.float32,tf.float32,tf.bool, tf.string])\n",
                "Xtestl,Xtest,Ytest, IDtest = gc.read_tfr_record('../../training_data/cumulative_test',['local','global','label','id'],\n",
                "    ['ar','ar','ar','b'],[tf.float32,tf.float32,tf.bool, tf.string])\n",
                "Xvall,Xval,Yval, IDval = gc.read_tfr_record('../../training_data/cumulative_val',['local','global','label','id'],\n",
                "    ['ar','ar','ar','b'],[tf.float32,tf.float32,tf.bool, tf.string])\n",
                "\n",
                "\n",
                "Ytrain = np.asarray(Ytrain, dtype='float32')\n",
                "Ytest = np.asarray(Ytest, dtype='float32')\n",
                "Xtrain = np.asarray(Xtrain, dtype='float32')\n",
                "Xtrainl = np.asarray(Xtrainl, dtype='float32')\n",
                "Xtest = np.asarray(Xtest, dtype='float32')\n",
                "Xtestl = np.asarray(Xtestl, dtype='float32')\n",
                "Xval = np.asarray(Xval, dtype='float32')\n",
                "Yval = np.asarray(Yval, dtype='float32')\n",
                "Xvall = np.asarray(Xvall, dtype='float32')\n",
                "\n",
                "\n",
                "print(Xtrain.shape,Xval.shape,Xtest.shape,Xtrainl.shape,Xvall.shape, Xtestl.shape,Ytrain.shape,Yval.shape, Ytest.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#print(IDtest)\n",
                "TestID2 = [str(IDtest[i])[2:11] for i in range(0,len(IDtest))]\n",
                "TrainID2 = [str(IDtrain[i])[2:11] for i in range(0,len(IDtrain))]\n",
                "ValID2 = [str(IDval[i])[2:11] for i in range(0,len(IDval))]\n",
                "#print(TestID2)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Xtrain=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrain])\n",
                "Xtest=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtest])\n",
                "Xtrainl=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtrainl])\n",
                "Xtestl=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xtestl])\n",
                "Xvall=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xvall])\n",
                "Xval=np.asarray([(row-np.median(row))/(-row[np.argmin(row)]+np.median(row)) for row in Xval])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(7,7))\n",
                "\n",
                "i=0\n",
                "ca=[14,15]\n",
                "tab=0\n",
                "xax1=np.linspace(-0.5,0.5,201)\n",
                "xax2=np.linspace(-0.5,0.5,2001)\n",
                "plt.suptitle('Samples', size=17)\n",
                "ax[tab][0].set_title('Global View',size=15)\n",
                "ax[tab][1].set_title('Local View',size=15)\n",
                "ax[1][0].set_xlabel('Phase',size=13)\n",
                "ax[1][1].set_xlabel('Phase',size=13)\n",
                "while tab<2:\n",
                "    \n",
                "    #if(Ytrain[i][0]==0): continue\n",
                "    ax[tab][0].set_ylabel('Flux',size=13)\n",
                "    ax[tab][1].plot(xax1,Xtrainl[ca[i]],label=(Ytrain[ca[i]]),marker='.', ls='None', color=\"#6d4b4b\")\n",
                "    ax[tab][0].plot(xax2,Xtrain[ca[i]],marker='.', ls='None', color=\"#3c4e4b\")\n",
                "    ax[tab][1].legend()\n",
                "    tab+=1\n",
                "    i+=1\n",
                "\n",
                "plt.savefig('fprez_pl_fps.jpg')\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train=np.loadtxt('training_data/Xtrain_av_clean.csv',delimiter=',')\n",
                "X_trainl=np.loadtxt('training_data/Xtrainloc_av_clean.csv',delimiter=',')\n",
                "Y_train=np.loadtxt('training_data/Ytrain_av_clean.csv',delimiter=',')\n",
                "\n",
                "#X_train=[el-np.median(el) for el in X_train]\n",
                "#X_train=[el/np.abs(el[np.argmin(el)]) for el in X_train]\n",
                "#X_train=np.array(X_train)\n",
                "#X_trainl=[el-np.median(el) for el in X_trainl]\n",
                "#X_trainl=[el/np.abs(el[np.argmin(el)]) for el in X_trainl]\n",
                "#X_trainl=np.array(X_trainl)\n",
                "#Ytrain_ref=np.loadtxt('training_data/YtrainR_big.csv',delimiter=',')\n",
                "#Ytest_ref=np.loadtxt('training_data/YtestR_big.csv',delimiter=',')\n",
                "Xtrain, Xtest, Xtrainl, Xtestl, Ytrain, Ytest = train_test_split(X_train, X_trainl, Y_train, test_size=0.1)\n",
                "print(Xtrain.shape,Ytrain.shape,Xtest.shape,Ytest.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#add hidden layers\n",
                "def sandv():\n",
                "     conv_ip = keras.layers.Input(shape=(2001,),name='Input')\n",
                "     conv_ipl = keras.layers.Input(shape=(201,),name='LInput')\n",
                "     x=keras.layers.Reshape((2001, 1), input_shape=(2001,),name='reshape_1')(conv_ip)\n",
                "     x=keras.layers.BatchNormalization()(x)\n",
                "     x=keras.layers.Conv1D(16,kernel_size=5,strides=1,activation='relu',name='conv16_5')(x) \n",
                "     x=keras.layers.Conv1D(16,kernel_size=5,strides=1,activation='relu',name='second_conv16_5')(x)\n",
                "     x=keras.layers.MaxPool1D(5,strides=2,data_format='channels_last',name='maxpool_1')(x)\n",
                "     x=keras.layers.Conv1D(32,kernel_size=5,strides=1,activation='relu',name='first_conv32_5')(x)\n",
                "     x=keras.layers.Conv1D(32,kernel_size=5,strides=1,activation='relu',name='second_conv32_5')(x)\n",
                "     x=keras.layers.MaxPool1D(5,strides=2,data_format='channels_last',name='maxpool_2')(x)\n",
                "     x=keras.layers.Conv1D(64,kernel_size=5,strides=1,activation='relu',name='first_conv64_5')(x)\n",
                "     x=keras.layers.Conv1D(64,kernel_size=5,strides=1,activation='relu',name='second_conv64_5')(x)\n",
                "     x=keras.layers.MaxPool1D(5,strides=2,data_format='channels_last',name='maxpool_3')(x)\n",
                "     x=keras.layers.Conv1D(128,kernel_size=5,strides=1,activation='relu',name='first_conv128_5')(x)\n",
                "     x=keras.layers.Conv1D(128,kernel_size=5,strides=1,activation='relu',name='second_conv128_5')(x)\n",
                "     x=keras.layers.MaxPool1D(5,strides=2,data_format='channels_last',name='maxpool_4')(x)\n",
                "     x=keras.layers.Conv1D(256,kernel_size=5,strides=1,activation='relu',name='first_conv256_5')(x)\n",
                "     x=keras.layers.Conv1D(256,kernel_size=5,strides=1,activation='relu',name='second_conv256_5')(x)\n",
                "     x=keras.layers.MaxPool1D(5,strides=2,data_format='channels_last',name='maxpool_5')(x)\n",
                "     x=keras.layers.Flatten(name='flat_1')(x)\n",
                "     x=keras.Model(conv_ip,x,name='global')\n",
                "\n",
                "     x2=keras.layers.Reshape((201, 1), input_shape=(201,),name='Lreshape_1')(conv_ipl)\n",
                "     x2=keras.layers.BatchNormalization()(x2)\n",
                "     x2=keras.layers.Conv1D(16,kernel_size=5,strides=1,activation='relu',name='Lconv16_5')(x2) \n",
                "     x2=keras.layers.Conv1D(16,kernel_size=5,strides=1,activation='relu',name='Lsecond_conv16_5')(x2)\n",
                "     x2=keras.layers.MaxPool1D(5,strides=2,data_format='channels_last',name='Lmaxpool_1')(x2)\n",
                "     x2=keras.layers.Conv1D(32,kernel_size=5,strides=1,activation='relu',name='Lfirst_conv32_5')(x2)     \n",
                "     x2=keras.layers.Conv1D(32,kernel_size=5,strides=1,activation='relu',name='Lsecond_conv32_5')(x2)\n",
                "     x2=keras.layers.MaxPool1D(5 ,strides=2,data_format='channels_last',name='Lmaxpool_2')(x2)\n",
                "     x2=keras.layers.Flatten(name='flat_2')(x2)\n",
                "     x2=keras.Model(conv_ipl,x2,name='global')\n",
                "\n",
                "     x3=keras.layers.concatenate([x.output,x2.output],name=\"combine\")\n",
                "\n",
                "     x3=keras.layers.Dense(512,name='dense_layer_1',activation='relu')(x3)\n",
                "     x3=keras.layers.Dense(512,name='dense_layer_2',activation='relu')(x3)\n",
                "     x3=keras.layers.Dense(512,name='dense_layer_3',activation='relu')(x3)\n",
                "     x3=keras.layers.Dense(512,name='dense_layer_u',activation='relu')(x3)\n",
                "     #x3=keras.layers.Dense(1,name='dense_layer_4',activation='relu')(x3)\n",
                "     conv_op=keras.layers.Dense(2,name='dense_layer_5',activation='softmax')(x3)\n",
                "\n",
                "\n",
                "     convNN = keras.Model(inputs=[conv_ip,conv_ipl], outputs=conv_op,name='Convolutional_NN')\n",
                "     convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005,beta_1=0.9,beta_2=0.999,epsilon=10**(-8)),\n",
                "         loss='categorical_crossentropy',metrics=['accuracy'])\n",
                "     #convNN.compile(optimizer=keras.optimizers.Adam(learning_rate=0.000005),loss='categorical_crossentropy',metrics=['accuracy'])\n",
                "     return(convNN)\n",
                "\n",
                "\n",
                "#convNN.summary()\n",
                "#"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "convNN = sandv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
                "#convNN = sandv()\n",
                "history=convNN.fit([np.array(Xtrain),np.array(Xtrainl)],np.array(Ytrain), batch_size=64, epochs=30, verbose=VERBOSE, \n",
                "    validation_data=([Xval,Xvall],Yval))\n",
                "#history=convNN.fit([np.asarray(Xtrain),np.asarray(Xtrainl)],np.asarray(Ytrain[:,0:2]), batch_size=64, epochs=20, verbose=VERBOSE, \n",
                "#    validation_split=0.2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(history.history['loss'])\n",
                "plt.plot(history.history['val_loss'])\n",
                "plt.title('model loss')\n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'test'], loc='upper left')\n",
                "plt.savefig('Mloss_SandN_tot_actual_un.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(history.history['accuracy'])\n",
                "plt.plot(history.history['val_accuracy'])\n",
                "plt.title('model accuracy')\n",
                "plt.ylabel('accuracy')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'test'], loc='upper left')\n",
                "plt.savefig('macc_SandN_actual_un.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rKFold = KFold(n_splits=5)\n",
                "kfold = rKFold.split(Xtrain, Ytrain)\n",
                "scores = []\n",
                "#\n",
                "#\n",
                "es_callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
                "\n",
                "for k, (train, test) in enumerate(kfold):\n",
                "    convNN = sandv()\n",
                "    convNN.fit([Xtrain[train, :],Xtrainl[train, :]], Ytrain[train], epochs=30, verbose=1, callbacks=[es_callback])\n",
                "    score = convNN.evaluate([Xtrain[test, :],Xtrainl[test, :]], Ytrain[test])\n",
                "    scores.append(score)\n",
                "    convNN.save_weights('CVm_snv'+str(k+1)+'.h5')\n",
                "    #print('Fold: %2d, Training/Test Split Distribution: %s, Accuracy: %.3f' % (k+1, np.bincount(Ytrain[train]), score))\n",
                " \n",
                "#print('\\n\\nCross-Validation accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(scores)\n",
                "print(np.asarray(scores)[:,1].mean())\n",
                "print(np.asarray(scores)[:,1].std())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#testing the feature map\n",
                "#Y=featuresNN.predict(np.array(Xtrain[:10]))\n",
                "#sep=[[Y[i][j][1] for i in range(10)] for j in range(7)]\n",
                "#plt.imshow(sep)\n",
                "plt.plot(Xtrain[4])\n",
                "#[plt.scatter(i,Y[0][i][2],color='green') for i in range(0,7)]\n",
                "#[plt.scatter(i,Y[0][i][3],color='black') for i in range(0,7)]\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#convNN.load_weights('tempforanalysis.h5')\n",
                "test_loss, test_acc = convNN.evaluate([np.array(Xtest),np.array(Xtestl)], np.array(Ytest[:,0:2]))\n",
                "print('Test accuracy:', test_acc)\n",
                "Ypred_raw=convNN.predict([np.array(Xtest),np.array(Xtestl)])\n",
                "Ytrain_raw=convNN.predict([np.array(Xtrain),np.array(Xtrainl)])\n",
                "Yval_raw=convNN.predict([np.array(Xval),np.array(Xvall)])\n",
                "Ypred=np.argmax(Ypred_raw, axis=1)\n",
                "Ytest_new=np.argmax(Ytest,axis=1)\n",
                "cm = confusion_matrix(Ytest_new, Ypred)\n",
                "print(cm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(Ypred_raw)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#write out the predictions of the test sample here...\n",
                "net=np.asarray([[TestID2[i],Ytest[i],Ypred_raw[i]] for i in range(len(TestID2))], dtype='object')\n",
                "gc.write_tfr_record('../../training_data/jointanalysis_test',net,['id','true_label','pred_label'],['b','ar','ar']\n",
                "    ,['string','bool','float32'])\n",
                "\n",
                "net=np.asarray([[TrainID2[i],Ytrain[i],Ytrain_raw[i]] for i in range(len(TrainID2))], dtype='object')\n",
                "gc.write_tfr_record('../../training_data/jointanalysis_train',net,['id','true_label','pred_label'],['b','ar','ar']\n",
                "    ,['string','bool','float32'])\n",
                "\n",
                "net=np.asarray([[ValID2[i],Yval[i],Yval_raw[i]] for i in range(len(ValID2))], dtype='object')\n",
                "gc.write_tfr_record('../../training_data/jointanalysis_val',net,['id','true_label','pred_label'],['b','ar','ar']\n",
                "    ,['string','bool','float32'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#convNN.save_weights('conv_LocalGlobal_main_un.h5')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#convNN.load_weights('conv_LocalGlobal_main1.h5')\n",
                "#convNN.save('Model_LocalGlobal_main1.h5')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_loss, test_acc = convNN.evaluate([np.array(Xtest),np.array(Xtestl)], np.array(Ytest))\n",
                "print('Test accuracy:', test_acc)\n",
                "Ypred_raw=convNN.predict([np.array(Xtest),np.array(Xtestl)])\n",
                "Ypred=np.argmax(Ypred_raw, axis=1)\n",
                "Ytest_new=np.argmax(Ytest,axis=1)\n",
                "cm = confusion_matrix(Ytest_new, Ypred)\n",
                "print(cm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
                "print(Ypred_raw[:,0].shape,Ytest[:,0].shape)\n",
                "fpr , tpr, thes= roc_curve(Ytest[:,0],Ypred_raw[:,0],pos_label=1)\n",
                "print(auc(fpr,tpr))\n",
                "print(fpr.shape,tpr.shape,thes.shape)\n",
                "plt.style.use('seaborn-bright')\n",
                "plt.figure(figsize=(7,5))\n",
                "plt.plot(thes,tpr)\n",
                "plt.title('ROC Curve')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "#plt.savefig('sandv_ROC')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "convNN.load_weights('conv_LocalGlobal.h5')\n",
                "convNN.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_loss, test_acc = convNN.evaluate([np.asarray(Xtest),np.asarray(Xtestl)], np.asarray(Ytest))\n",
                "print('Test accuracy:', test_acc)\n",
                "Ypred_raw=convNN.predict([np.array(Xtest),np.array(Xtestl)])\n",
                "Ypred=np.argmax(Ypred_raw, axis=1)\n",
                "Ytest_new=np.argmax(Ytest,axis=1)\n",
                "cm = confusion_matrix(Ytest_new, Ypred)\n",
                "print(cm)\n",
                "\n",
                "print(Ypred_raw[:,1].shape,Ytest[:,1].shape)\n",
                "fpr , tpr, thes= roc_curve(Ytest[:,1],Ypred_raw[:,1],pos_label=1)\n",
                "print(auc(fpr,tpr))\n",
                "print(fpr.shape,tpr.shape,thes.shape)\n",
                "plt.style.use('seaborn-bright')\n",
                "plt.figure(figsize=(7,5))\n",
                "plt.plot(fpr, tpr)\n",
                "plt.title('ROC Curve')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "#plt.savefig('ROCmine')"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
        },
        "kernelspec": {
            "display_name": "Python 3.9.7 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

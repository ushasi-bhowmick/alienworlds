{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#long awaited keras tutorial... coz basic idea is a good idea...\n",
                "from keras.models import Sequential\n",
                "import tensorflow as tf\n",
                "from keras.layers import Dense, Activation, Dropout"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#three categories of keras API - model, layer and Core modules\n",
                "#model represents an ANN - each model is a collection of different layers... represent diffrent layers of an ANN\n",
                "#keras model: sequential: linear composition of keras layers\n",
                "\n",
                "model=Sequential()\n",
                "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
                "#like in C++ we need to create a model instance and add layers to it. \n",
                "#We can have different calls of the constructor as well"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#layers: each corresponding layer of the proposed Neural Network\n",
                "#Important keras layers : Core, convolution, pooling, recurrent\n",
                "#we can create customised layers as well\n",
                "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
                "model.add(Dropout(0.2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#core modules: activation, loss, optimizer, regularizers, initializers, constraints, metrics etc\n",
                "#backend: keras runs on tensorflow backend\n",
                "from keras import backend as k \n",
                "k.get_uid(prefix='') #initializer for default graph\n",
                "k.reset_uids() #reset the uid value\n",
                "data = k.placeholder(shape=(1,3,3),dtype='float32') #create a placeholder tensor of a desired shape\n",
                "k.int_shape(data)   #show the shape of data\n",
                "a=k.placeholder(shape=(3,3)) \n",
                "b=k.placeholder(shape=(3,3))\n",
                "c=k.dot(a,b) #dot product of two tensors\n",
                "res = k.ones(shape=(2,2)) #initialize a tensor with all values 1\n",
                "d=k.batch_dot(a,b) #execute a batch dot ... no idea what that is\n",
                "var = k.variable([[1,2,3],[4,5,6]])  #create a variable... unlike placeholders, their value can be altered at runtime\n",
                "res = k.transpose(var) #transpose of a variable\n",
                "print(k.eval(res)) #display only array from the tensor specs\n",
                "\n",
                "e=k.placeholder((2,2), sparse=True) #ig it means the row and column is dynamic to change\n",
                "print(k.is_sparse(e)) #check if a tensor is sparse. prints true in this case\n",
                "f=k.to_dense(e) #convert a sparse tensor to a dense tensor, where rows and cols are fixed in dimensionality\n",
                "g=k.random_uniform_variable(shape=(2,2), low=1, high=10) #declare a uniform random variable of a particular shape and range"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#utilities of keras\n",
                "labels=[0,1,2,3,4,5,6,7,8,9]\n",
                "cat_label=tf.keras.utils.to_categorical(labels) #convert neural net to binary format for classification\n",
                "n=tf.keras.utils.normalize([1,2,3,4,5]) #normalize the input vector\n",
                "tf.keras.utils.plot_model(model, to_file='model_im.png') #generate an image version of the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#for a keras layer to work the following things must be specified: \n",
                "#shape of input data, no. of neurons, initializer, regularizer, constraint, activation\n",
                "from keras import initializers,constraints,regularizers\n",
                "\n",
                "model.add(Dense(32, input_shape=(16,), kernel_initializer = 'he_uniform', \n",
                "   kernel_regularizer = None, kernel_constraint = 'MaxNorm', activation = 'relu')) \n",
                "\n",
                "my_init=initializers.Zeros()\n",
                "my_constrain=constraints.MaxNorm(max_value=2,axis=0)\n",
                "my_reg=regularizers.l1(0.)\n",
                "model.add(Dense(32, input_shape=(16,), kernel_initializer = my_init, \n",
                "   kernel_regularizer = my_reg, kernel_constraint = 'MaxNorm', activation = 'relu')) \n",
                "#initializers: set initial vales for weights and biases ig\n",
                "initializers.Ones()\n",
                "initializers.Constant(value=0)\n",
                "initializers.RandomNormal(mean=0.0,stddev=0.05,seed=None)\n",
                "initializers.RandomUniform(minval=-0.05,maxval=0.05,seed=None)\n",
                "initializers.TruncatedNormal(mean = 0.0, stddev = 0.05, seed = None)\n",
                "initializers.VarianceScaling(scale = 1.0, mode = 'fan_in', distribution = 'normal', seed = None) \n",
                "#varaince scale calculates stddev using sqrt(scale/n) and calculates weights using normal distribution. \n",
                "#now whether n is input unit, output unit or an average of the two depends on the mode fan_in, fan_out or fan_avg\n",
                "#can also calculate limit as sqrt(3*scale/n)\n",
                "initializers.lecun_uniform(seed = None) \n",
                "initializers.glorot_normal(seed=None) \n",
                "initializers.glorot_uniform(seed = None) \n",
                "initializers.he_normal(seed = None) \n",
                "initializers.Orthogonal(gain = 1.0, seed = None) \n",
                "initializers.Identity(gain = 1.0)\n",
                "\n",
                "#Constraints: constraints on weights and biases help to limit the NN and prevent overfitting\n",
                "constraints.NonNeg\n",
                "constraints.UnitNorm(axis=0)\n",
                "constraints.MinMaxNorm(min_value=0.0,max_value=1.0,rate=1.0,axis=0)\n",
                "#rate is rate at which constraint is applied\n",
                "\n",
                "#regularizers: penalties appllied for overfitting\n",
                "regularizers.l2(0.)\n",
                "regularizers.l1_l2(0.)\n",
                "\n",
                "#activations: non-linear tranformation of a layer, enable it to learn better\n",
                "#options: linear, elu, selu, relu, softmax, softplus, softsign, tanh, sigmoid, hard_sigmoid, exponential\n",
                "#elu means exponential linear unit\n",
                "\n",
                "#Layer varieties:\n",
                "layer_1 = Dense(16, input_shape = (8,))\n",
                "#arguments: units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer,\n",
                "#kernel_constraint, bias_constraint\n",
                "#activity_regularizer applies a regularization constraint on the output\n",
                "#input_shape is a special argument given iff the layer specified is the first layer of the NN\n",
                "layer_2 = Dropout(0.3, noise_shape=None, seed=None)\n",
                "#rate is the fraction of nodes to dropout\n",
                "#noise shape represents the dimension of shape in which dropout has to be applied.\n",
                "layer_3 = tf.keras.layers.Flatten(data_format=None)\n",
                "#flatten the input, get a 2x2 input to 4 array, data_format is an optional argu ment\n",
                "layer_4= tf.keras.layers.Reshape((16,8))\n",
                "#alter shape of layers\n",
                "layer_5= tf.keras.layers.Permute((2,1))\n",
                "#if input is say (8,16), output will be a (16,8) matrix... need to know more on this\n",
                "layer_6= tf.keras.layers.RepeatVector(16)\n",
                "#repeat an input vector a set number of times... if input shape is (x,32), output shape will be (x,16,32)\n",
                "#lambda ... transform a layer before input, need to find more about it.\n",
                "layer_7=tf.keras.layers.Conv1D(filters=None, kernel_size=4, strides=1, padding='valid',data_format='channels_last')\n",
                "layer_8=tf.keras.layers.Conv2D(filters=None, kernel_size=(4,4), strides=(1,1), padding='valid')\n",
                "#filters: number of filters to apply in each convolution, affects dimension of output shape\n",
                "#kernel size: length of convolution window\n",
                "#padding: valid, causal, same(o/p same length as i/p)\n",
                "layer_9=tf.keras.layers.MaxPool1D(pool_size=2,strides=None,padding='valid')\n",
                "#max pooling operations on temporal data\n",
                "layer_10=tf.keras.layers.LocallyConnected1D(16,3,input_shape=(10,8))\n",
                "# apply a unshared weight convolution 1-dimension of length 3 to a sequence with 10 timesteps, with 16 output filters\n",
                "#just like convolution but with unshared weights\n",
                "layer_11=tf.keras.layers.subtract([layer_1,layer_2])\n",
                "#other options include add,multiply, average, maximum, minimum, concatenate, dot\n",
                "layer_12=tf.keras.layers.Embedding(input_dim=(10,10),output_dim=(5,5))\n",
                "#embedding operations: convert positive into dense vectors of fixed size\n",
                "\n",
                "#common functions associated with layers:\n",
                "layer_1.get_weights() #get the weight matrix\n",
                "config=layer_1.get_config()   #get configuration of a layer as an object\n",
                "reload_layer=Dense.from_config(config) #this configuration can be used to reset layers anytime\n",
                "layer_1.input_shape #shape of a single node\n",
                "layer_1.input #get input data of single node\n",
                "layer_1.get_input_at(8) #get input data at a specified index\n",
                "layer_1.get_input_shape_at(8)\n",
                "layer_1.output_shape #get output shape\n",
                "layer_1.output\n",
                "layer_1.get_output_at(7)\n",
                "layer_1.get_output_shape_at(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#creating custom layers\n",
                "from keras import backend as k \n",
                "from keras.layers import Layer\n",
                "\n",
                "#need to define a layer class\n",
                "class custom_ly(Layer):\n",
                "    #constructor call\n",
                "    def __init__(self, output_dim, **kwargs):    \n",
                "        self.output_dim = output_dim \n",
                "        super(custom_ly, self).__init__(**kwargs)\n",
                "    \n",
                "    #main method needed to build the layer properly\n",
                "    #kernel function that creates the appropriate weight matrix according to our input shape\n",
                "    def build(self,input_shape):\n",
                "        self.kernel = self.add_weight(name = 'kernel', shape = (input_shape[1], self.output_dim), initializer = 'normal', trainable = True) \n",
                "        super(custom_ly, self).build(input_shape)\n",
                "\n",
                "    #call method:how the layer works during training\n",
                "    def call(self, input_data):\n",
                "        return(k.dot(input_data,self.kernel))\n",
                "    \n",
                "    #compute output shape\n",
                "    def compute_output_shape(self, input_shape): \n",
                "        return (input_shape[0], self.output_dim)\n",
                "\n",
                "    #these functions are important for creating a custom layer. Now we can use it as an ordinary TF layer\n",
                "model = Sequential() \n",
                "model.add(custom_ly(32, input_shape = (16,))) \n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
        },
        "kernelspec": {
            "display_name": "Python 3.9.5 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
